{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use Nick's PPO trained policy to perform inference on whether there is a pedestrian or not\"\"\"\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PyQt5\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.core.util import emission_to_csv\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import get_flow_params\n",
    "from flow.utils.rllib import get_rllib_config\n",
    "from flow.utils.rllib import get_rllib_pkl\n",
    "\n",
    "from examples.rllib.multiagent_exps.test_predictor.pedestrian_policy_1 import create_env, create_agent\n",
    "from examples.rllib.multiagent_exps.bayesian_1_env import make_flow_params as bayesian_0_flow_params\n",
    "\n",
    "EXAMPLE_USAGE = \"\"\"\n",
    "example usage:\n",
    "    python ./visualizer_rllib.py /ray_results/experiment_dir/result_dir 1\n",
    "Here the arguments are:\n",
    "1 - the path to the simulation results\n",
    "2 - the number of the checkpoint\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy takes in an observation of length 22 (10 + max_num_objects * 3)\n",
    "\n",
    "observation[4:10] = ped_param\n",
    "\n",
    "I want the set to posterior probabilities: $$Pr(\\text{ped_in_grid_i} | action), \\forall i = 1, \\ldots, 6$$\n",
    "\n",
    "$Pr(\\text{ped_in_grid_i} | \\text{action}) = \\frac{Pr(\\text{action} | \\text{ped_in_i}) Pr(\\text{ped_in_i})}{Pr(\\text{action})}$\n",
    "\n",
    "$\\Pr(action) = \\sum_{i=1}^6 \\Pr(action | ped\\text{_}in\\text{_}i) \\Pr(ped\\text{_}in\\text{_}i)$\n",
    "\n",
    "\n",
    "Compute Pr(action | ped_in_i) by taking\n",
    "\n",
    "1. actual state e.g. s = [a, b, c, d, [0], e, _, _, _], where I only care about the ith grid (suppose there is no ped in the ith grid)\n",
    "\n",
    "2. actual state with flipped pedestrian at grid i, i.e\n",
    "s_flipped = [a, b, c, d, [1], e, _, _, _]\n",
    "\n",
    "Input these two into the policy to get PDFs:\n",
    "\n",
    "If $s[i] == 0,$ ped_PDF = $\\pi(s\\text{_}flipped)$ and no_ped_PDF = $\\pi(s)$. Pr(action | ped_in_i) = $\\frac{ped\\text{_}PDF}{ped\\text{_}PDF + ped\\text{_}no\\text{_}PDF}$\n",
    "\n",
    "If $s[i] == 1,$ ped_PDF = $\\pi(s)$ and no_ped_PDF = $\\pi(s\\text{_}flipped)$. Pr(action | ped_in_i) = $\\frac{ped\\text{_}PDF}{ped\\text{_}PDF + ped\\text{_}no\\text{_}PDF}$\n",
    "\n",
    "and do the usual normalization thing to get Pr(action | ped_in_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Math\n",
    "\n",
    "Now, suppose we have access to some PDF $f(a| o_1, o_2, \\ldots, o_6)$, where $o_i$ for $i = 1, \\ldots, 6$ are indicator variables for whether a pedestrian is or isn't on grid $i$ (as defined in the figure $<insert figure>)$.\n",
    "\n",
    "Our goal: $p(o_i = b_i| a), \\forall i = 1, \\ldots, 6$, $b_i \\in \\{0, 1\\}$. We get these 'single' conditional probabilities we want by marginalizing:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(o_1 = b_1 | a) = \\sum_{b_2, \\ldots, b_6} p(o_1 = b_1, \\ldots, o_6 = b_6 |a )\n",
    "\\end{equation}\n",
    "\n",
    "Thus, to get these 'single' conditional probabilities, we need to calculate these joint conditional probabilities: $p(o_1 = b_1, \\ldots, o_6 = b_6 |a), \\forall b_i \\in \\{0, 1\\}, i \\in 1, \\ldots, 6$. Via Bayes' rule again:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(o_1 = b_1, \\ldots, o_6 = b_6 |a ) = \\frac{p(a | o_1 = b_1, \\ldots, o_6 = b_6) p(o_1, \\ldots, o_6)}{p(a)}    \n",
    "\\end{equation}\n",
    "\n",
    "Since we only have access to the PDF $f(a | o_1 = b_1, \\ldots, o_6 = b_6)$, we'd end up with the PDF $f(o_1 = b_1, \\ldots, o_6 = b_6 |a )$ from the equation above. That is, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(o_1 = b_1, \\ldots, o_6 = b_6 |a ) = \\frac{f(a | o_1 = b_1, \\ldots, o_6 = b_6) p(o_1, \\ldots, o_6)}{p(a)}    \n",
    "\\end{equation}\n",
    "\n",
    "To get the probability $p(o_1 = b_1, \\ldots, o_6 = b_6 |a)$, we normalize:\n",
    "\n",
    "\\begin{equation}\n",
    "p(o_1 = b_1, \\ldots, o_6 = b_6 |a) = \\frac{f(o_1 = b_1, \\ldots, o_6 = b_6 |a)}{c}    \n",
    "\\end{equation}\n",
    "\n",
    "where $c = \\sum_{b_1, \\ldots, b_6} f(o_1 = b_1, \\ldots, o_6 = b_6 |a)$\n",
    "\n",
    "Note that we can drop the denominator $p(a)$ in equation 6 since we're normalizing these PDFs into probabilities. Thus, we can just let\n",
    "\n",
    "\\begin{equation}\n",
    "f(o_1 = b_1, \\ldots, o_6 = b_6 |a) = f(a | o_1 = b_1, \\ldots, o_6 = b_6) p(o_1 = b_1, \\ldots, o_b = b_6) \n",
    "\\end{equation}\n",
    "\n",
    "Notice that we'll need $2^6$ of these conditional densities / probabilities.\n",
    "Explicitly, we need $2^6$ of $f(a | \\cap_i o_i = b_i), f(\\cap_i o_i = b_i | a), p(\\cap_i o_i = b_i | a)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env, agent, config, flow_params):\n",
    "    \"\"\"Run the simulation and control the rl car using the trained policy. \n",
    "    \n",
    "    observation[4:10] = ped_param\n",
    "    \n",
    "    The six binary grids are at indices 4 to 9 inclusive\n",
    "    \"\"\"\n",
    "    # set up relevant policy and env\n",
    "    if config.get('multiagent', {}).get('policies', None):\n",
    "        multiagent = True\n",
    "        rets = {}\n",
    "        # map the agent id to its policy\n",
    "        policy_map_fn = config['multiagent']['policy_mapping_fn']\n",
    "        \n",
    "#         policy_map_fn = config['multiagent']['policy_mapping_fn'].func\n",
    "        for key in config['multiagent']['policies'].keys():\n",
    "            rets[key] = []\n",
    "    else:\n",
    "        multiagent = False\n",
    "        rets = []\n",
    "\n",
    "    if config['model']['use_lstm']:\n",
    "        use_lstm = True\n",
    "        if multiagent:\n",
    "            state_init = {}\n",
    "            # map the agent id to its policy\n",
    "#             policy_map_fn = config['multiagent']['policy_mapping_fn'].func\n",
    "\n",
    "            policy_map_fn = config['multiagent']['policy_mapping_fn']\n",
    "            size = config['model']['lstm_cell_size']\n",
    "            for key in config['multiagent']['policies'].keys():\n",
    "                state_init[key] = [np.zeros(size, np.float32),\n",
    "                                   np.zeros(size, np.float32)]\n",
    "        else:\n",
    "            state_init = [\n",
    "                np.zeros(config['model']['lstm_cell_size'], np.float32),\n",
    "                np.zeros(config['model']['lstm_cell_size'], np.float32)\n",
    "            ]\n",
    "    else:\n",
    "        use_lstm = False\n",
    "\n",
    "    env.restart_simulation(\n",
    "        sim_params=flow_params['sim'], render=flow_params['sim'].render)\n",
    "\n",
    "    \n",
    "    # Define variables to collect probability data. Each variable is dict\n",
    "    # 2^6 keys (or, 2^5 values for p(o_i = b_i)) and values will be a list of numbers\n",
    "    # TODO(KL) HARD CODED is_ped_visible is the 5th item in the state vector\n",
    "\n",
    "    ped_idx_lst = [4, 5, 6, 7, 8, 9]\n",
    "    ped_front = ped_idx_lst[0]\n",
    "    ped_back = ped_idx_lst[-1]\n",
    "    \n",
    "    binary_observations = True\n",
    "    \n",
    "    \n",
    "    # dict for f(a | o_i = b_i), where the key is bitstring s e.g.s = \"010011\", \n",
    "    # where s[i] corresponds to the value of b_i. cond_pdf_a_on_joint_ped[\"010011\"] = list()\n",
    "    \n",
    "    # 1\n",
    "    # IF binary_observations == True:\n",
    "    cond_pdf_action_on_joint_ped_dct = make_dct_of_lsts(num_digits=len(ped_idx_lst), vals_per_dig=2)\n",
    "    \n",
    "    # 2\n",
    "    # updated Pr(o_1 = b_1, ..., o_6 = b_6) for i = 1, ..., 6 and b_i = 0, 1\n",
    "    updated_joint_prior_prob_ped_lst = initial_prior_probs(num_digits=6, vals_per_dig=2)\n",
    "    # fixed Pr(o_1 = b_1, ..., o_6 = b_6) for i = 1, ..., 6 and b_i = 0, 1\n",
    "    fixed_joint_prior_prob_ped_lst = initial_prior_probs(num_digits=6, vals_per_dig=2)\n",
    "    \n",
    "    # 3\n",
    "    # updated joint cond_densities lst f(o_1 = b_1, ..., o_6 = b_6 | a)\n",
    "    joint_cond_densities_updated_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\n",
    "    # fixed joint cond_densities lst f(o_1 = b_1, ..., o_6 = b_6 | a)\n",
    "    joint_cond_densities_fixed_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\n",
    "\n",
    "    # 4\n",
    "    # updated joint cond_probs lst p(o_1 = b_1, ..., o_6 = b_6 | a)\n",
    "    joint_cond_probs_updated_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\n",
    "    # fixed joint cond_probs lst p(o_1 = b_1, ..., o_6 = b_6 | a)\n",
    "    joint_cond_probs_fixed_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\n",
    "    \n",
    "    # 5\n",
    "    # updated single cond_probs lst p(o_i = b_i | a)\n",
    "    single_cond_probs_updated_priors_lst = make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2)\n",
    "    # fixed single cond_probs lst p(o_i = b_i | a)\n",
    "    single_cond_probs_fixed_priors_lst = make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2)\n",
    "    \n",
    "    # 6\n",
    "    # this will be the only variable that's a dict of lists\n",
    "    visible_pedestrian_dct = {i : [] for i in range(1,7)}\n",
    "\n",
    "    # each element in this list corresponds to a key \n",
    "    len_6_bitstring_lst = make_permutations(num_digits=6, vals_per_dig=2)\n",
    "    for i in range(args.num_rollouts):\n",
    "        state = env.reset()\n",
    "        # divide by 5 to get \"time\" in the simulation\n",
    "        for _ in range(1000):\n",
    "            vehicles = env.unwrapped.k.vehicle\n",
    "            pedestrian = env.unwrapped.k.pedestrian\n",
    "\n",
    "            if multiagent:\n",
    "                action, logits = {}, {}\n",
    "                for agent_id in state.keys():\n",
    "                    if use_lstm:\n",
    "                        action[agent_id], state_init[agent_id], logits = \\\n",
    "                            agent.compute_action(state[agent_id], state=state_init[agent_id], policy_id=policy_map_fn(agent_id))\n",
    "                    else:\n",
    "                        s_all = state[agent_id]\n",
    "                        # get ped visibility state array of length 6 from the rl car's POV\n",
    "                        s_ped = s_all[ped_idx_lst]\n",
    "                        \n",
    "                        # update the visible_pedestrian dict\n",
    "                        for idx, val in enumerate(s_ped, 1):\n",
    "                            visible_pedestrian_dct[idx] = visible_pedestrian_dct[idx] + [val]\n",
    "                        print(len(state[agent_id]))\n",
    "                        # compute the actual action taken by the rl car\n",
    "                        import ipdb;ipdb.set_trace()\n",
    "                        action[agent_id], _, logit_actual = agent.compute_action(state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)    \n",
    "                        action_ = action[agent_id][0]\n",
    "                        \n",
    "                        # store variable for the sum of densities conditioned on the actual action\n",
    "                        cond_density_sum_updated = 0\n",
    "                        cond_density_sum_fixed = 0\n",
    "\n",
    "                        # compute joint conditional densities for all possible permutations of ped visibility (ignore which permutation corresponds to the actual permutation for now)\n",
    "                        for obs_comb in len_6_bitstring_lst:\n",
    "                            s_all_modified = np.copy(s_all)\n",
    "                            s_all_modified[ped_front : ped_back + 1] = list(obs_comb)\n",
    "                            print(f'{s_all_modified[ped_front : ped_back + 1]} is list of length 6, {list(obs_comb)} has length 6')\n",
    "                            _, _, logit = agent.compute_action(state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)\n",
    "                            # where in the docs did we figure out the logits were ln of sigma??\n",
    "                            import ipdb;ipdb.set_trace()\n",
    "                            mu, ln_sigma = logit['behaviour_logits']\n",
    "                            sigma = np.exp(ln_sigma)\n",
    "                            \n",
    "                            # 1\n",
    "                            # compute the conditional a_given_obs pdfs, i.e. f(a | o_1 = b_1, ..., o_6 = b_6)\n",
    "                            cond_a_given_joint_o_pdf = accel_pdf(mu, sigma, action_)\n",
    "                            # append new result to lists\n",
    "                            cond_pdf_action_on_joint_ped_dct[obs_comb] = cond_pdf_action_on_joint_ped_dct[obs_comb] + [cond_a_given_joint_o_pdf]\n",
    "                            \n",
    "                            # 2\n",
    "                            # Get the updated and fixed priors\n",
    "                            updated_prior = updated_joint_prior_prob_ped_lst[obs_comb][-1]\n",
    "                            fixed_prior = fixed_joint_prior_prob_ped_lst[obs_comb][-1]\n",
    "\n",
    "                            # 3\n",
    "                            # compute the joint conditional obs_given_a pdfs, i.e. f(o_1 = b_1, ..., o_6 = b_6 | a)\n",
    "                            cond_joint_o_given_a_pdf_updated = cond_a_given_joint_o_pdf * updated_prior\n",
    "                            cond_joint_o_given_a_pdf_fixed = cond_a_given_joint_o_pdf * fixed_prior\n",
    "                            # append new result to lists\n",
    "                            joint_cond_densities_updated_priors_dct[obs_comb] = joint_cond_densities_updated_priors_dct[obs_comb] + [cond_joint_o_given_a_pdf_updated]\n",
    "                            joint_cond_densities_fixed_priors_dct[obs_comb] = joint_cond_densities_updated_priors_dct[obs_comb] + [cond_joint_o_given_a_pdf_fixed]\n",
    "\n",
    "                            cond_density_sum_updated += cond_joint_o_given_a_pdf_updated\n",
    "                            cond_density_sum_fixed += cond_joint_o_given_a_pdf_fixed\n",
    "                            \n",
    "                        # 4\n",
    "                        # compute Pr(o_1 = b_1, ... o_6 = b_6 | a)\n",
    "                        for obs_comb_ in len_6_bitstring_lst:\n",
    "                            cond_joint_o_given_a_prob_updated = joint_cond_densities_updated_priors_dct[obs_comb_][-1] / cond_density_sum_updated\n",
    "                            cond_joint_o_given_a_prob_fixed = joint_cond_densities_fixed_priors_dct[obs_comb_][-1] / cond_density_sum_fixed\n",
    "                            # append to lists\n",
    "                            joint_cond_probs_updated_priors_dct[obs_comb_] = joint_cond_probs_updated_priors_dct[obs_comb_] + [cond_joint_o_given_a_prob_updated]\n",
    "                            joint_cond_probs_fixed_priors_dct[obs_comb_] = joint_cond_probs_fixed_priors_dct[obs_comb_] + [cond_joint_o_given_a_prob_fixed]\n",
    "                        \n",
    "                            # 2\n",
    "                            # Update the updated and fixed priors lists\n",
    "                            updated_joint_prior_prob_ped_lst[obs_comb_] = updated_joint_prior_prob_ped_lst[obs_comb_] + [cond_joint_o_given_a_prob_updated]\n",
    "                            fixed_joint_prior_prob_ped_lst[obs_comb_] = fixed_joint_prior_prob_ped_lst[obs_comb_] + [cond_joint_o_given_a_prob_fixed]\n",
    "                        \n",
    "                        # 5\n",
    "                        # compute Pr(o_i = b_i | a) for all i = 1, ..., 6\n",
    "                        for grid_idx in range(1, 7):\n",
    "                            for val in range(0, 2):\n",
    "                                # compute the marginalization (4) - need to get the relevant ped observation combinations\n",
    "                                single_cond_o_given_a_prob_updated = 0\n",
    "                                single_cond_o_given_a_prob_fixed = 0\n",
    "\n",
    "                                for key in get_ped_possiblities(grid_idx, val):\n",
    "                                    single_cond_o_given_a_prob_updated += joint_cond_probs_updated_priors_dct[key]\n",
    "                                    single_cond_o_given_a_prob_fixed += joint_cond_probs_fixed_priors_dct[key]\n",
    "                                \n",
    "                                single_cond_prob_str = single_cond_prob_to_str(grid_idx, val)\n",
    "                                # append to list\n",
    "                                single_cond_probs_updated_priors_lst[single_cond_prob_str] = single_cond_probs_updated_priors_lst[single_cond_prob_str] + [single_cond_o_given_a_prob_updated]\n",
    "                                single_cond_probs_fixed_priors_lst[single_cond_prob_str] = single_cond_probs_fixed_priors_lst[single_cond_prob_str] + [single_cond_o_given_a_prob_fixed]\n",
    "                                \n",
    "\n",
    "            else:\n",
    "                action = agent.compute_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if multiagent and done['__all__']:\n",
    "                print(_,1)\n",
    "                break\n",
    "            if not multiagent and done:\n",
    "                print(_,1)\n",
    "\n",
    "                break    \n",
    "            state, reward, done, _ = env.step(action)   \n",
    "\n",
    "        visible_ped_lsts = [visible_pedestrian_dct[i] for i in range(1, 7)]\n",
    "        legends = [f'ped in grid {i}' for i in range(1, 7)]\n",
    "\n",
    "        for grid_idx in range(1, 7):\n",
    "            for val in range(0, 2):\n",
    "                single_cond_prob_str = single_cond_prob_to_str(grid_idx, val)\n",
    "                a = single_cond_probs_updated_priors_lst[single_cond_prob_str]\n",
    "                b = single_cond_probs_fixed_priors_lst[single_cond_prob_str]\n",
    "\n",
    "                plot_2_lines(a, b)\n",
    "#         plot_lines(visible_ped_lsts, legends)\n",
    "#         plot_2_lines(probs_ped_given_action_updated_priors, probs_no_ped_given_action_updated_priors, ['Pr(ped | action) using updated priors', 'Pr(no_ped | action) using updated priors'], viewable_ped=visible_pedestrian[4])\n",
    "#         plot_2_lines(probs_ped_given_action_fixed_priors, probs_no_ped_given_action_fixed_priors, ['Pr(ped | action) using fixed priors of Pr(ped) = 0.5', 'Pr(no_ped | action) using fixed priors of Pr(ped) = 0.5'], viewable_ped=visible_pedestrian[5])\n",
    "#         plot_2_lines(probs_action_given_ped, probs_action_given_no_ped, ['Pr(action | ped)', 'Pr(action | no_ped)'], viewable_ped=visible_pedestrian[6])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_cond_prob_to_str(grid_idx, val, num_indices = 6):\n",
    "    \"\"\"Generate the string representing the probability:\n",
    "    \n",
    "    Pr(o_i = val)\n",
    "    \n",
    "    ex:\n",
    "    For Pr(o_2 = 1), we'd have the string '21'\n",
    "    NB we're 1-indexing here\n",
    "    \"\"\"\n",
    "    assert grid_idx >= 1 and grid_idx <= num_indices\n",
    "    return str(grid_idx) + str(val)\n",
    "\n",
    "def get_ped_possiblities(grid_idx, val, output_len=6):\n",
    "    \"\"\"Give the single probability we want, compute the list of all bitstrings we need to perform the marginalization.\n",
    "    \n",
    "    ex:\n",
    "    3:0 means we want p(o_3 = 0 | a)\n",
    "    Therefore, we can get the list of all possible length 5 bitstrings, and stitch '0' in the correct place.\n",
    "    \n",
    "    \n",
    "    @Return\n",
    "    NB 1-indexing here\n",
    "    bitstring of length 6\n",
    "    \"\"\"\n",
    "    assert grid_idx >= 1 and grid_idx <= output_len\n",
    "    res_lst = make_permutations(output_len - 1, 2)\n",
    "    for perm in res_lst:\n",
    "\n",
    "        print(str(perm[:grid_idx - 1:] + str(val) + perm[grid_idx - 1:]))\n",
    "\n",
    "def initial_prior_probs(num_digits=6, vals_per_dig=2):\n",
    "    \"\"\"Returns a dict with values of all permutations of bitstrings of length num_digits. \n",
    "    Each digit can take a value from 0 to (vals_per_dig - 1)\"\"\"\n",
    "    uniform_prob = 1 / (vals_per_dig ** num_digits)\n",
    "    res = make_dct_of_lsts(num_digits, vals_per_dig)\n",
    "    for key in res.keys():\n",
    "        res[key] = res[key] + [uniform_prob]\n",
    "    return res\n",
    "\n",
    "def make_dct_of_lsts(num_digits=6, vals_per_dig=2):\n",
    "    \"\"\"Return a dict with keys of bitstrings and values as empty lists. \n",
    "    Hardcoded for binary vals per var.\"\"\"\n",
    "    res = {}\n",
    "    lst_of_bitstrings = make_permutations(num_digits, vals_per_dig)\n",
    "        \n",
    "    return {str_ : [] for str_ in lst_of_bitstrings}\n",
    "\n",
    "def make_permutations(num_digits, vals_per_dig=2):\n",
    "    \"\"\"Make all permutations for a bit string of length num_digits\n",
    "    and vals_per_dig values per digit. Hardcoded for work for binary vals per var\"\"\"\n",
    "    if num_digits == 1:\n",
    "        return [str(i) for i in range(vals_per_dig)]\n",
    "    else:\n",
    "        small_perms = make_permutations(num_digits - 1, vals_per_dig)\n",
    "        # hardcoded for work for binary vals per var\n",
    "        return ['0' + bit_str for bit_str in small_perms] + ['1' + bit_str for bit_str in small_perms]\n",
    "    \n",
    "def make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2):\n",
    "    \"\"\"@Params\n",
    "    num_variables = number of states that we care about\n",
    "    \n",
    "    @Returns\n",
    "    dict of lists. Keys have the format: 'o_{i}={val}', where val is either '0' or '1'\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for i in range(1, num_variables + 1):\n",
    "        for val in range(vals_per_var):\n",
    "            res[f'o_{i} = {val}'] = []\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function testing suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050000\n",
      "050001\n",
      "050010\n",
      "050011\n",
      "050100\n",
      "050101\n",
      "050110\n",
      "050111\n",
      "051000\n",
      "051001\n",
      "051010\n",
      "051011\n",
      "051100\n",
      "051101\n",
      "051110\n",
      "051111\n",
      "150000\n",
      "150001\n",
      "150010\n",
      "150011\n",
      "150100\n",
      "150101\n",
      "150110\n",
      "150111\n",
      "151000\n",
      "151001\n",
      "151010\n",
      "151011\n",
      "151100\n",
      "151101\n",
      "151110\n",
      "151111\n"
     ]
    }
   ],
   "source": [
    "get_ped_possiblities(2, 5, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define indicator RVs $G_i$, where $G_i = 1$ if there's a pedestrian in grid cell $i$ and $0$ if not.\n",
    "\n",
    "Want $Pr(G_i = 1 | action) = \\frac{Pr(action | G_i = 1) Pr(G_i = 1)}{Pr(G_i = 1)}$\n",
    "\n",
    "Consider grid 1:\n",
    "$\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "Pr(action | G_1 = 1) & = \\frac{Pr(action, G_1 = 1)}{Pr(G_1 = 1)} \\\\\n",
    " & = \\frac{\\sum_{g2 \\in \\{0, 1\\}} \\sum_{g3 \\in \\{0, 1\\}} \\sum_{g4 \\in \\{0, 1\\}} \\sum_{g5 \\in \\{0, 1\\}} \\sum_{g6 \\in \\{0, 1\\}} Pr(action, G_1 = 1, G_2 = g2, G_3 = g3, G_4 = g4, G_5 = g5, G_6 = g6)}{Pr(G_1 = 1)} \\\\\n",
    " & = \\frac{\\sum_{g2 \\in \\{0, 1\\}} \\sum_{g3 \\in \\{0, 1\\}} \\sum_{g4 \\in \\{0, 1\\}} \\sum_{g5 \\in \\{0, 1\\}} \\sum_{g6 \\in \\{0, 1\\}} Pr(action | G_1 = 1, G_2 = g2, G_3 = g3, G_4 = g4, G_5 = g5, G_6 = g6) Pr(G_1 = 1, G_2 = g2, G_3 = g3, G_4 = g4, G_5 = g5, G_6 = g6))}{Pr(G_1 = 1)}\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "\n",
    "The policy doesn't actually output a probability; the policy outputs a probability density.\n",
    "i.e. instead of outputting $Pr(action | G_1 = 1, G_2 = g2, G_3 = g3, G_4 = g4, G_5 = g5, G_6 = g6)$, \n",
    "the policy outputs $f(action | G_1 = 1, G_2 = g2, G_3 = g3, G_4 = g4, G_5 = g5, G_6 = g6)$\n",
    "\n",
    "Perhaps after some handwaving and assumptions about $Pr(G_i = 0) = 0.5$, I could say, I have $f(action | G_1 = 1) \\sim \\sum \\sum \\sum \\ldots \\ldots = k_1$. Note, I don't have equality here. Under the assumption of $Pr(G_i = 0) = 0.5$, the numerator joint probability / density term (assuming independence) is $0.5^6$.\n",
    "\n",
    "How do I convert these $k_i$'s into probabilities?\n",
    "\n",
    "Is it feasible to normalize just the 6 $f(action | G_i = 1)$ values, i.e assert\n",
    "\n",
    "$Pr(action | G_1 = 1) = \\frac{f(action | G_1 = 1)}{\\sum_{i=1}^6 f(action | G_i = 1)}$?\n",
    "\n",
    "I don't think this normalization is accurate. It seems more accurate to do:\n",
    "\n",
    "$Pr(action | G_1 = 1) = \\frac{f(action | G_1 = 1)}{f(action | G_1 = 1) + f(action | G_1 = 0)}$?\n",
    "\n",
    "My doubts are: \n",
    "\n",
    "1. the conversion from pdfs to probabilities - is this how we normalize pdfs to get probabilities?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_ped_given_action(action, mu_ped, s_ped, mu_no_ped, s_no_ped, prior, fixed_prior=True):\n",
    "    \"\"\"\n",
    "    @Params\n",
    "    mu_ped, s_ped: mean, sd pair from the policy receiving an input state where there is a visible pedestrian \n",
    "    mu_no_ped, s_no_ped: mean, sd pair from the policy receiving an input state where there is no visible pedestrian \n",
    "\n",
    "    action: the vehicle's acceleration as dictated by the policy\n",
    "    prior: Pr(ped)\n",
    "    fixed_prior: Boolean telling us whether to 'update' the prior Pr(ped) using Pr(ped | action) or not\n",
    "\n",
    "    @Returns\n",
    "    \n",
    "    probs, a dict containing:\n",
    "    1. Pr(action | ped)\n",
    "    2. Pr(action | no_ped)\n",
    "    3. Pr(ped | action)\n",
    "    4. Pr(no_ped | action)\n",
    "    5. Pr(ped) for the next computation of Pr(ped|action)\n",
    "    \n",
    "    3, 4, 5 are calculated subject to the fixed_prior parameter\n",
    "    \"\"\"\n",
    "    probs = {}\n",
    "    \n",
    "    # Compute 1, 2: Pr(action | ped), Pr(action | no_ped)\n",
    "    unnormed_pr_action_given_ped = accel_pdf(mu_ped, s_ped, action)\n",
    "    unnormed_pr_action_given_no_ped = accel_pdf(mu_no_ped, s_no_ped, action)\n",
    "    \n",
    "    pr_a_given_ped = unnormed_pr_action_given_ped / (unnormed_pr_action_given_ped + unnormed_pr_action_given_no_ped)\n",
    "    pr_a_given_no_ped = 1 - pr_a_given_ped\n",
    "    \n",
    "    probs[\"pr_a_given_ped\"] = pr_a_given_ped\n",
    "    probs[\"pr_a_given_no_ped\"] = pr_a_given_no_ped\n",
    "    \n",
    "    # Compute 3, 4: Pr(ped | action), Pr(no_ped | action)\n",
    "    # Apply Bayes' rule\n",
    "    pr_ped_given_action = (pr_a_given_ped * prior) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\n",
    "    pr_no_ped_given_action = (pr_a_given_no_ped * (1 - prior)) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\n",
    "    probs[\"pr_ped_given_action\"] = pr_ped_given_action\n",
    "    probs[\"pr_no_ped_given_action\"] = pr_no_ped_given_action\n",
    "                    \n",
    "    if fixed_prior:\n",
    "        probs[\"prior\"] = prior\n",
    "    else:\n",
    "        probs[\"prior\"] = probs[\"pr_ped_given_action\"]\n",
    "    return probs\n",
    "    \n",
    "\n",
    "def accel_pdf(mu, sigma, actual):\n",
    "    \"\"\"Return pdf evaluated at actual acceleration\"\"\"\n",
    "    coeff = 1 / np.sqrt(2 * np.pi * (sigma**2))\n",
    "    exp = -0.5 * ((actual - mu) / sigma)**2\n",
    "    return coeff * np.exp(exp)\n",
    "\n",
    "def run_transfer(args):\n",
    "    # run transfer on the bayesian 1 env first\n",
    "    bayesian_0_params = bayesian_0_flow_params(args, pedestrians=True, render=True)\n",
    "#     import ipdb; ipdb.set_trace()\n",
    "    env, env_name = create_env(args, bayesian_0_params)\n",
    "    agent, config = create_agent(args, flow_params=bayesian_0_params)\n",
    "    run_env(env, agent, config, bayesian_0_params)\n",
    "\n",
    "def plot_2_lines(y1, y2, legend, viewable_ped=False):\n",
    "    x = np.arange(len(y1))\n",
    "    plt.plot(x, y1)\n",
    "    plt.plot(x, y2)\n",
    "    if viewable_ped:\n",
    "        plt.plot(x, viewable_ped)\n",
    "    plt.legend(legend, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\n",
    "   \n",
    "    plt.draw()\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "def plot_lines(y_val_lsts, legends):\n",
    "    assert len(y_val_lsts) == len(legends)\n",
    "    x = np.arange(len(y_val_lsts[0]))\n",
    "    for y_vals in y_val_lsts:\n",
    "        plt.plot(x, y_vals)\n",
    "    plt.legend(legends, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\n",
    "    plt.draw()\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser():\n",
    "    \"\"\"Create the parser to capture CLI arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        description='[Flow] Evaluates a reinforcement learning agent '\n",
    "                    'given a checkpoint.',\n",
    "        epilog=EXAMPLE_USAGE)\n",
    "\n",
    "    # required input parameters\n",
    "    parser.add_argument(\n",
    "        'result_dir', type=str, help='Directory containing results')\n",
    "    parser.add_argument('checkpoint_num', type=str, help='Checkpoint number.')\n",
    "\n",
    "    # optional input parameters\n",
    "    parser.add_argument(\n",
    "        '--run',\n",
    "        type=str,\n",
    "        help='The algorithm or model to train. This may refer to '\n",
    "             'the name of a built-on algorithm (e.g. RLLib\\'s DQN '\n",
    "             'or PPO), or a user-defined trainable function or '\n",
    "             'class registered in the tune registry. '\n",
    "             'Required for results trained with flow-0.2.0 and before.')\n",
    "    parser.add_argument(\n",
    "        '--num_rollouts',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='The number of rollouts to visualize.')\n",
    "    parser.add_argument(\n",
    "        '--gen_emission',\n",
    "        action='store_true',\n",
    "        help='Specifies whether to generate an emission file from the '\n",
    "             'simulation')\n",
    "    parser.add_argument(\n",
    "        '--evaluate',\n",
    "        action='store_true',\n",
    "        help='Specifies whether to use the \\'evaluate\\' reward '\n",
    "             'for the environment.')\n",
    "    parser.add_argument(\n",
    "        '--render_mode',\n",
    "        type=str,\n",
    "        default='sumo_gui',\n",
    "        help='Pick the render mode. Options include sumo_web3d, '\n",
    "             'rgbd and sumo_gui')\n",
    "    parser.add_argument(\n",
    "        '--save_render',\n",
    "        action='store_true',\n",
    "        help='Saves a rendered video to a file. NOTE: Overrides render_mode '\n",
    "             'with pyglet rendering.')\n",
    "    parser.add_argument(\n",
    "        '--horizon',\n",
    "        type=int,\n",
    "        help='Specifies the horizon.')\n",
    "    \n",
    "    parser.add_argument('--grid_search', action='store_true', default=False,\n",
    "                        help='If true, a grid search is run')\n",
    "    parser.add_argument('--run_mode', type=str, default='local',\n",
    "                        help=\"Experiment run mode (local | cluster)\")\n",
    "    parser.add_argument('--algo', type=str, default='QMIX',\n",
    "                        help=\"RL method to use (PPO, TD3, QMIX)\")\n",
    "    parser.add_argument(\"--pedestrians\",\n",
    "                        help=\"use pedestrians, sidewalks, and crossings in the simulation\",\n",
    "                        action=\"store_true\")\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 22:39:16,646\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-04-07 22:39:16,647\tINFO resource_spec.py:216 -- Starting Ray with 4.69 GiB memory available for workers and up to 2.35 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: With render mode sumo_gui, an extra instance of the SUMO GUI will display before the GUI for visualizing the result. Click the green Play arrow to continue.\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "Error making env  Cannot re-register id: Bayesian1Env-v0\n",
      "True\n",
      "NOTE: With render mode sumo_gui, an extra instance of the SUMO GUI will display before the GUI for visualizing the result. Click the green Play arrow to continue.\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "Error making env  Cannot re-register id: Bayesian1Env-v0\n",
      "True\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "Error making env  Cannot re-register id: Bayesian1Env-v0\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 22:39:22,852\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "2020-04-07 22:39:23,006\tINFO trainable.py:346 -- Restored from checkpoint: ./MADDPG_Bayesian1Env-v0_29b010d8_2020-04-07_21-19-39bm1c4hbt/checkpoint_500/checkpoint-500\n",
      "2020-04-07 22:39:23,006\tINFO trainable.py:353 -- Current state after restoring: {'_iteration': 500, '_timesteps_total': 50000, '_time_total': 420.01589179039, '_episodes_total': 104}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "> \u001b[0;32m<ipython-input-44-7e2db927dee7>\u001b[0m(118)\u001b[0;36mrun_env\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    117 \u001b[0;31m                        \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 118 \u001b[0;31m                        \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_fetch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    119 \u001b[0;31m                        \u001b[0maction_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> state[agent_id]\n",
      "array([ 0.00000000e+00,  1.24477067e+00,  1.00000000e+00,  7.89646705e-01,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  9.00000000e+01,  2.52004054e-01,\n",
      "        1.36096073e+01, -9.88779482e+00,  0.00000000e+00,  2.70000000e+02,\n",
      "        2.48711144e+00,  1.04096073e+01,  1.42298902e+01,  1.00000000e+00,\n",
      "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00])\n",
      "ipdb> policy_map_fn(agent_id)\n",
      "*** TypeError: list indices must be integers or slices, not str\n",
      "ipdb> policy_map_fn\n",
      "<function setup_exps_MADDPG.<locals>.<lambda> at 0x7f2ee00d70d0>\n",
      "ipdb> agent_id\n",
      "'rl_0'\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-3caafd22175c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrun_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-6e35f1bb690b>\u001b[0m in \u001b[0;36mrun_transfer\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbayesian_0_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbayesian_0_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mrun_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbayesian_0_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_2_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviewable_ped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-7e2db927dee7>\u001b[0m in \u001b[0;36mrun_env\u001b[0;34m(env, agent, config, flow_params)\u001b[0m\n\u001b[1;32m    116\u001b[0m                         \u001b[0;31m# compute the actual action taken by the rl car\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                         \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_fetch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                         \u001b[0maction_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-7e2db927dee7>\u001b[0m in \u001b[0;36mrun_env\u001b[0;34m(env, agent, config, flow_params)\u001b[0m\n\u001b[1;32m    116\u001b[0m                         \u001b[0;31m# compute the actual action taken by the rl car\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                         \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_actual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_fetch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                         \u001b[0maction_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = create_parser()\n",
    "args = parser.parse_args([\"./MADDPG_Bayesian1Env-v0_29b010d8_2020-04-07_21-19-39bm1c4hbt\", \"500\"])\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=1)\n",
    "run_transfer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.rllib.agents.trainer_template.MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_ped_given_action(.6538469, 0.14252673, 2.4491935, 0.14200129, 2.5765653, 0.5100277175584501, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([11,22, 3, 4, 5, 6, 7])\n",
    "c = [1, 2, 3]\n",
    "b[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(b, 1):\n",
    "    print(idx, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flow] *",
   "language": "python",
   "name": "conda-env-flow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
