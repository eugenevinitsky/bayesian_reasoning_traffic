{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use Nick's PPO trained policy to perform inference on whether there is a pedestrian or not\"\"\"\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PyQt5\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.core.util import emission_to_csv\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import get_flow_params\n",
    "from flow.utils.rllib import get_rllib_config\n",
    "from flow.utils.rllib import get_rllib_pkl\n",
    "\n",
    "from examples.rllib.multiagent_exps.test_predictor.pedestrian_policy_1 import create_env, create_agent\n",
    "from examples.rllib.multiagent_exps.bayesian_1_env import make_flow_params as bayesian_0_flow_params\n",
    "\n",
    "EXAMPLE_USAGE = \"\"\"\n",
    "example usage:\n",
    "    python ./visualizer_rllib.py /ray_results/experiment_dir/result_dir 1\n",
    "Here the arguments are:\n",
    "1 - the path to the simulation results\n",
    "2 - the number of the checkpoint\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy takes in an observation of length 22 (10 + max_num_objects * 3)\n",
    "\n",
    "observation[4:10] = ped_param\n",
    "\n",
    "I want the set to posterior probabilities: $$Pr(\\text{ped_in_grid_i} | action), \\forall i = 1, \\ldots, 6$$\n",
    "\n",
    "$Pr(\\text{ped_in_grid_i} | \\text{action}) = \\frac{Pr(\\text{action} | \\text{ped_in_i}) Pr(\\text{ped_in_i})}{Pr(\\text{action})}$\n",
    "\n",
    "$\\Pr(action) = \\sum_{i=1}^6 \\Pr(action | ped\\text{_}in\\text{_}i) \\Pr(ped\\text{_}in\\text{_}i)$\n",
    "\n",
    "\n",
    "Compute Pr(action | ped_in_i) by taking\n",
    "\n",
    "1. actual state e.g. s = [a, b, c, d, [0], e, _, _, _], where I only care about the ith grid (suppose there is no ped in the ith grid)\n",
    "\n",
    "2. actual state with flipped pedestrian at grid i, i.e\n",
    "s_flipped = [a, b, c, d, [1], e, _, _, _]\n",
    "\n",
    "Input these two into the policy to get PDFs:\n",
    "\n",
    "If $s[i] == 0,$ ped_PDF = $\\pi(s\\text{_}flipped)$ and no_ped_PDF = $\\pi(s)$. Pr(action | ped_in_i) = $\\frac{ped\\text{_}PDF}{ped\\text{_}PDF + ped\\text{_}no\\text{_}PDF}$\n",
    "\n",
    "If $s[i] == 1,$ ped_PDF = $\\pi(s)$ and no_ped_PDF = $\\pi(s\\text{_}flipped)$. Pr(action | ped_in_i) = $\\frac{ped\\text{_}PDF}{ped\\text{_}PDF + ped\\text{_}no\\text{_}PDF}$\n",
    "\n",
    "and do the usual normalization thing to get Pr(action | ped_in_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Math\n",
    "\n",
    "Now, suppose we have access to some PDF $f(a| o_1, o_2, \\ldots, o_6)$, where $o_i$ for $i = 1, \\ldots, 6$ are indicator variables for whether a pedestrian is or isn't on grid $i$ (as defined in the figure $<insert figure>)$.\n",
    "\n",
    "Our goal: $p(o_i = b_i| a), \\forall i = 1, \\ldots, 6$, $b_i \\in \\{0, 1\\}$. We get these 'single' conditional probabilities we want by marginalizing:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(o_1 = b_1 | a) = \\sum_{b_2, \\ldots, b_6} p(o_1 = b_1, \\ldots, o_6 = b_6 |a )\n",
    "\\end{equation}\n",
    "\n",
    "Thus, to get these 'single' conditional probabilities, we need to calculate these joint conditional probabilities: $p(o_1 = b_1, \\ldots, o_6 = b_6 |a), \\forall b_i \\in \\{0, 1\\}, i \\in 1, \\ldots, 6$. Via Bayes' rule again:\n",
    "\n",
    "\\begin{equation}\n",
    "    p(o_1 = b_1, \\ldots, o_6 = b_6 |a ) = \\frac{p(a | o_1 = b_1, \\ldots, o_6 = b_6) p(o_1, \\ldots, o_6)}{p(a)}    \n",
    "\\end{equation}\n",
    "\n",
    "Since we only have access to the PDF $f(a | o_1 = b_1, \\ldots, o_6 = b_6)$, we'd end up with the PDF $f(o_1 = b_1, \\ldots, o_6 = b_6 |a )$ from the equation above. That is, we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(o_1 = b_1, \\ldots, o_6 = b_6 |a ) = \\frac{f(a | o_1 = b_1, \\ldots, o_6 = b_6) p(o_1, \\ldots, o_6)}{p(a)}    \n",
    "\\end{equation}\n",
    "\n",
    "To get the probability $p(o_1 = b_1, \\ldots, o_6 = b_6 |a)$, we normalize:\n",
    "\n",
    "\\begin{equation}\n",
    "p(o_1 = b_1, \\ldots, o_6 = b_6 |a) = \\frac{f(o_1 = b_1, \\ldots, o_6 = b_6 |a)}{c}    \n",
    "\\end{equation}\n",
    "\n",
    "where $c = \\sum_{b_1, \\ldots, b_6} f(o_1 = b_1, \\ldots, o_6 = b_6 |a)$\n",
    "\n",
    "Note that we can drop the denominator $p(a)$ in equation 6 since we're normalizing these PDFs into probabilities. Thus, we can just let\n",
    "\n",
    "\\begin{equation}\n",
    "f(o_1 = b_1, \\ldots, o_6 = b_6 |a) = f(a | o_1 = b_1, \\ldots, o_6 = b_6) p(o_1 = b_1, \\ldots, o_b = b_6) \n",
    "\\end{equation}\n",
    "\n",
    "Notice that we'll need $2^6$ of these conditional densities / probabilities.\n",
    "Explicitly, we need $2^6$ of $f(a | \\cap_i o_i = b_i), f(\\cap_i o_i = b_i | a), p(\\cap_i o_i = b_i | a)$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env, agent, config, flow_params):\n",
    "    \"\"\"Run the simulation and control the rl car using the trained policy. \n",
    "    \n",
    "    observation[4:10] = ped_param\n",
    "    \n",
    "    The six binary grids are at indices 4 to 9 inclusive\n",
    "    \"\"\"\n",
    "    # set up relevant policy and env\n",
    "    if config.get('multiagent', {}).get('policies', None):\n",
    "        multiagent = True\n",
    "        rets = {}\n",
    "        # map the agent id to its policy\n",
    "        policy_map_fn = config['multiagent']['policy_mapping_fn']\n",
    "        \n",
    "#         policy_map_fn = config['multiagent']['policy_mapping_fn'].func\n",
    "        for key in config['multiagent']['policies'].keys():\n",
    "            rets[key] = []\n",
    "    else:\n",
    "        multiagent = False\n",
    "        rets = []\n",
    "\n",
    "    if config['model']['use_lstm']:\n",
    "        use_lstm = True\n",
    "        if multiagent:\n",
    "            state_init = {}\n",
    "            # map the agent id to its policy\n",
    "#             policy_map_fn = config['multiagent']['policy_mapping_fn'].func\n",
    "\n",
    "            policy_map_fn = config['multiagent']['policy_mapping_fn']\n",
    "            size = config['model']['lstm_cell_size']\n",
    "            for key in config['multiagent']['policies'].keys():\n",
    "                state_init[key] = [np.zeros(size, np.float32),\n",
    "                                   np.zeros(size, np.float32)]\n",
    "        else:\n",
    "            state_init = [\n",
    "                np.zeros(config['model']['lstm_cell_size'], np.float32),\n",
    "                np.zeros(config['model']['lstm_cell_size'], np.float32)\n",
    "            ]\n",
    "    else:\n",
    "        use_lstm = False\n",
    "\n",
    "    env.restart_simulation(\n",
    "        sim_params=flow_params['sim'], render=flow_params['sim'].render)\n",
    "\n",
    "    \n",
    "    # Define variables to collect probability data. Each variable is dict\n",
    "    # 2^6 keys (or, 2^5 values for p(o_i = b_i)) and values will be a list of numbers\n",
    "    # TODO(KL) HARD CODED is_ped_visible is the 5th item in the state vector\n",
    "\n",
    "    ped_idx_lst = [4, 5, 6, 7, 8, 9]\n",
    "    ped_front = ped_idx_lst[0]\n",
    "    ped_back = ped_idx_lst[-1]\n",
    "    \n",
    "    binary_observations = True\n",
    "    \n",
    "    \n",
    "    # dict for f(a | o_i = b_i), where the key is bitstring s e.g.s = \"010011\", \n",
    "    # where s[i] corresponds to the value of b_i. cond_pdf_a_on_joint_ped[\"010011\"] = list()\n",
    "    \n",
    "    # 1\n",
    "    # IF binary_observations == True:\n",
    "    cond_pdf_action_on_joint_ped_dct = make_dct_of_lsts(num_digits=len(ped_idx_lst), vals_per_dig=2)\n",
    "    \n",
    "    # 2\n",
    "    # updated Pr(o_1 = b_1, ..., o_6 = b_6) for i = 1, ..., 6 and b_i = 0, 1\n",
    "    updated_joint_prior_prob_ped_lst = initial_prior_probs(num_digits=6, vals_per_dig=2)\n",
    "    # fixed Pr(o_1 = b_1, ..., o_6 = b_6) for i = 1, ..., 6 and b_i = 0, 1\n",
    "    fixed_joint_prior_prob_ped_lst = initial_prior_probs(num_digits=6, vals_per_dig=2)\n",
    "    \n",
    "    # 3\n",
    "    # updated joint cond_densities lst f(o_1 = b_1, ..., o_6 = b_6 | a)\n",
    "    joint_cond_densities_updated_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\n",
    "    # fixed joint cond_densities lst f(o_1 = b_1, ..., o_6 = b_6 | a)\n",
    "    joint_cond_densities_fixed_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\n",
    "\n",
    "    # 4\n",
    "    # updated joint cond_probs lst p(o_1 = b_1, ..., o_6 = b_6 | a)\n",
    "    joint_cond_probs_updated_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\n",
    "    # fixed joint cond_probs lst p(o_1 = b_1, ..., o_6 = b_6 | a)\n",
    "    joint_cond_probs_fixed_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\n",
    "    \n",
    "    # 5\n",
    "    # updated single cond_probs lst p(o_i = b_i | a)\n",
    "    single_cond_probs_updated_priors_lst = make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2)\n",
    "    # fixed single cond_probs lst p(o_i = b_i | a)\n",
    "    single_cond_probs_fixed_priors_lst = make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2)\n",
    "    \n",
    "    # 6\n",
    "    # this will be the only variable that's a dict of lists\n",
    "    visible_pedestrian_dct = {i : [] for i in range(1,7)}\n",
    "\n",
    "    # each element in this list corresponds to a key \n",
    "    len_6_bitstring_lst = make_permutations(num_digits=6, vals_per_dig=2)\n",
    "    for i in range(args.num_rollouts):\n",
    "        state = env.reset()\n",
    "        # divide by 5 to get \"time\" in the simulation\n",
    "        for _ in range(1000):\n",
    "            vehicles = env.unwrapped.k.vehicle\n",
    "            pedestrian = env.unwrapped.k.pedestrian\n",
    "\n",
    "            if multiagent:\n",
    "                action, logits = {}, {}\n",
    "                for agent_id in state.keys():\n",
    "                    if 'QMIX' == 'QMIX':\n",
    "                        agent_id += 1\n",
    "                    if use_lstm:\n",
    "                        action[agent_id], state_init[agent_id], logits = \\\n",
    "                            agent.compute_action(state[agent_id], state=state_init[agent_id], policy_id=policy_map_fn(agent_id))\n",
    "                    else:\n",
    "                        s_all = state[agent_id]\n",
    "                        # get ped visibility state array of length 6 from the rl car's POV\n",
    "                        s_ped = s_all[ped_idx_lst]\n",
    "                        \n",
    "                        # update the visible_pedestrian dict\n",
    "                        for idx, val in enumerate(s_ped, 1):\n",
    "                            visible_pedestrian_dct[idx] = visible_pedestrian_dct[idx] + [val]\n",
    "                        print(len(state[agent_id]))\n",
    "                        # compute the actual action taken by the rl car\n",
    "                        action[agent_id], _, logit_actual = agent.compute_action(state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)    \n",
    "                        action_ = action[agent_id][0]\n",
    "                        \n",
    "                        # store variable for the sum of densities conditioned on the actual action\n",
    "                        cond_density_sum_updated = 0\n",
    "                        cond_density_sum_fixed = 0\n",
    "\n",
    "                        # compute joint conditional densities for all possible permutations of ped visibility (ignore which permutation corresponds to the actual permutation for now)\n",
    "                        for obs_comb in len_6_bitstring_lst:\n",
    "                            s_all_modified = np.copy(s_all)\n",
    "                            s_all_modified[ped_front : ped_back + 1] = list(obs_comb)\n",
    "                            print(f'{s_all_modified[ped_front : ped_back + 1]} is list of length 6, {list(obs_comb)} has length 6')\n",
    "                            _, _, logit = agent.compute_action(state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)\n",
    "                            # where in the docs did we figure out the logits were ln of sigma??\n",
    "                            import ipdb;ipdb.set_trace()\n",
    "                            mu, ln_sigma = logit['behaviour_logits']\n",
    "                            sigma = np.exp(ln_sigma)\n",
    "                            \n",
    "                            # 1\n",
    "                            # compute the conditional a_given_obs pdfs, i.e. f(a | o_1 = b_1, ..., o_6 = b_6)\n",
    "                            cond_a_given_joint_o_pdf = accel_pdf(mu, sigma, action_)\n",
    "                            # append new result to lists\n",
    "                            cond_pdf_action_on_joint_ped_dct[obs_comb] = cond_pdf_action_on_joint_ped_dct[obs_comb] + [cond_a_given_joint_o_pdf]\n",
    "                            \n",
    "                            # 2\n",
    "                            # Get the updated and fixed priors\n",
    "                            updated_prior = updated_joint_prior_prob_ped_lst[obs_comb][-1]\n",
    "                            fixed_prior = fixed_joint_prior_prob_ped_lst[obs_comb][-1]\n",
    "\n",
    "                            # 3\n",
    "                            # compute the joint conditional obs_given_a pdfs, i.e. f(o_1 = b_1, ..., o_6 = b_6 | a)\n",
    "                            cond_joint_o_given_a_pdf_updated = cond_a_given_joint_o_pdf * updated_prior\n",
    "                            cond_joint_o_given_a_pdf_fixed = cond_a_given_joint_o_pdf * fixed_prior\n",
    "                            # append new result to lists\n",
    "                            joint_cond_densities_updated_priors_dct[obs_comb] = joint_cond_densities_updated_priors_dct[obs_comb] + [cond_joint_o_given_a_pdf_updated]\n",
    "                            joint_cond_densities_fixed_priors_dct[obs_comb] = joint_cond_densities_updated_priors_dct[obs_comb] + [cond_joint_o_given_a_pdf_fixed]\n",
    "\n",
    "                            cond_density_sum_updated += cond_joint_o_given_a_pdf_updated\n",
    "                            cond_density_sum_fixed += cond_joint_o_given_a_pdf_fixed\n",
    "                            \n",
    "                        # 4\n",
    "                        # compute Pr(o_1 = b_1, ... o_6 = b_6 | a)\n",
    "                        for obs_comb_ in len_6_bitstring_lst:\n",
    "                            cond_joint_o_given_a_prob_updated = joint_cond_densities_updated_priors_dct[obs_comb_][-1] / cond_density_sum_updated\n",
    "                            cond_joint_o_given_a_prob_fixed = joint_cond_densities_fixed_priors_dct[obs_comb_][-1] / cond_density_sum_fixed\n",
    "                            # append to lists\n",
    "                            joint_cond_probs_updated_priors_dct[obs_comb_] = joint_cond_probs_updated_priors_dct[obs_comb_] + [cond_joint_o_given_a_prob_updated]\n",
    "                            joint_cond_probs_fixed_priors_dct[obs_comb_] = joint_cond_probs_fixed_priors_dct[obs_comb_] + [cond_joint_o_given_a_prob_fixed]\n",
    "                        \n",
    "                            # 2\n",
    "                            # Update the updated and fixed priors lists\n",
    "                            updated_joint_prior_prob_ped_lst[obs_comb_] = updated_joint_prior_prob_ped_lst[obs_comb_] + [cond_joint_o_given_a_prob_updated]\n",
    "                            fixed_joint_prior_prob_ped_lst[obs_comb_] = fixed_joint_prior_prob_ped_lst[obs_comb_] + [cond_joint_o_given_a_prob_fixed]\n",
    "                        \n",
    "                        # 5\n",
    "                        # compute Pr(o_i = b_i | a) for all i = 1, ..., 6\n",
    "                        for grid_idx in range(1, 7):\n",
    "                            for val in range(0, 2):\n",
    "                                # compute the marginalization (4) - need to get the relevant ped observation combinations\n",
    "                                single_cond_o_given_a_prob_updated = 0\n",
    "                                single_cond_o_given_a_prob_fixed = 0\n",
    "\n",
    "                                for key in get_ped_possiblities(grid_idx, val):\n",
    "                                    single_cond_o_given_a_prob_updated += joint_cond_probs_updated_priors_dct[key]\n",
    "                                    single_cond_o_given_a_prob_fixed += joint_cond_probs_fixed_priors_dct[key]\n",
    "                                \n",
    "                                single_cond_prob_str = single_cond_prob_to_str(grid_idx, val)\n",
    "                                # append to list\n",
    "                                single_cond_probs_updated_priors_lst[single_cond_prob_str] = single_cond_probs_updated_priors_lst[single_cond_prob_str] + [single_cond_o_given_a_prob_updated]\n",
    "                                single_cond_probs_fixed_priors_lst[single_cond_prob_str] = single_cond_probs_fixed_priors_lst[single_cond_prob_str] + [single_cond_o_given_a_prob_fixed]\n",
    "                                \n",
    "\n",
    "            else:\n",
    "                action = agent.compute_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if multiagent and done['__all__']:\n",
    "                print(_,1)\n",
    "                break\n",
    "            if not multiagent and done:\n",
    "                print(_,1)\n",
    "\n",
    "                break    \n",
    "            state, reward, done, _ = env.step(action)   \n",
    "\n",
    "        visible_ped_lsts = [visible_pedestrian_dct[i] for i in range(1, 7)]\n",
    "        legends = [f'ped in grid {i}' for i in range(1, 7)]\n",
    "\n",
    "        for grid_idx in range(1, 7):\n",
    "            for val in range(0, 2):\n",
    "                single_cond_prob_str = single_cond_prob_to_str(grid_idx, val)\n",
    "                a = single_cond_probs_updated_priors_lst[single_cond_prob_str]\n",
    "                b = single_cond_probs_fixed_priors_lst[single_cond_prob_str]\n",
    "\n",
    "                plot_2_lines(a, b)\n",
    "#         plot_lines(visible_ped_lsts, legends)\n",
    "#         plot_2_lines(probs_ped_given_action_updated_priors, probs_no_ped_given_action_updated_priors, ['Pr(ped | action) using updated priors', 'Pr(no_ped | action) using updated priors'], viewable_ped=visible_pedestrian[4])\n",
    "#         plot_2_lines(probs_ped_given_action_fixed_priors, probs_no_ped_given_action_fixed_priors, ['Pr(ped | action) using fixed priors of Pr(ped) = 0.5', 'Pr(no_ped | action) using fixed priors of Pr(ped) = 0.5'], viewable_ped=visible_pedestrian[5])\n",
    "#         plot_2_lines(probs_action_given_ped, probs_action_given_no_ped, ['Pr(action | ped)', 'Pr(action | no_ped)'], viewable_ped=visible_pedestrian[6])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_cond_prob_to_str(grid_idx, val, num_indices = 6):\n",
    "    \"\"\"Generate the string representing the probability:\n",
    "    \n",
    "    Pr(o_i = val)\n",
    "    \n",
    "    ex:\n",
    "    For Pr(o_2 = 1), we'd have the string '21'\n",
    "    NB we're 1-indexing here\n",
    "    \"\"\"\n",
    "    assert grid_idx >= 1 and grid_idx <= num_indices\n",
    "    return str(grid_idx) + str(val)\n",
    "\n",
    "def get_ped_possiblities(grid_idx, val, output_len=6):\n",
    "    \"\"\"Give the single probability we want, compute the list of all bitstrings we need to perform the marginalization.\n",
    "    \n",
    "    ex:\n",
    "    3:0 means we want p(o_3 = 0 | a)\n",
    "    Therefore, we can get the list of all possible length 5 bitstrings, and stitch '0' in the correct place.\n",
    "    \n",
    "    \n",
    "    @Return\n",
    "    NB 1-indexing here\n",
    "    bitstring of length 6\n",
    "    \"\"\"\n",
    "    assert grid_idx >= 1 and grid_idx <= output_len\n",
    "    res_lst = make_permutations(output_len - 1, 2)\n",
    "    for perm in res_lst:\n",
    "\n",
    "        print(str(perm[:grid_idx - 1:] + str(val) + perm[grid_idx - 1:]))\n",
    "\n",
    "def initial_prior_probs(num_digits=6, vals_per_dig=2):\n",
    "    \"\"\"Returns a dict with values of all permutations of bitstrings of length num_digits. \n",
    "    Each digit can take a value from 0 to (vals_per_dig - 1)\"\"\"\n",
    "    uniform_prob = 1 / (vals_per_dig ** num_digits)\n",
    "    res = make_dct_of_lsts(num_digits, vals_per_dig)\n",
    "    for key in res.keys():\n",
    "        res[key] = res[key] + [uniform_prob]\n",
    "    return res\n",
    "\n",
    "def make_dct_of_lsts(num_digits=6, vals_per_dig=2):\n",
    "    \"\"\"Return a dict with keys of bitstrings and values as empty lists. \n",
    "    Hardcoded for binary vals per var.\"\"\"\n",
    "    res = {}\n",
    "    lst_of_bitstrings = make_permutations(num_digits, vals_per_dig)\n",
    "        \n",
    "    return {str_ : [] for str_ in lst_of_bitstrings}\n",
    "\n",
    "def make_permutations(num_digits, vals_per_dig=2):\n",
    "    \"\"\"Make all permutations for a bit string of length num_digits\n",
    "    and vals_per_dig values per digit. Hardcoded for work for binary vals per var\"\"\"\n",
    "    if num_digits == 1:\n",
    "        return [str(i) for i in range(vals_per_dig)]\n",
    "    else:\n",
    "        small_perms = make_permutations(num_digits - 1, vals_per_dig)\n",
    "        # hardcoded for work for binary vals per var\n",
    "        return ['0' + bit_str for bit_str in small_perms] + ['1' + bit_str for bit_str in small_perms]\n",
    "    \n",
    "def make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2):\n",
    "    \"\"\"@Params\n",
    "    num_variables = number of states that we care about\n",
    "    \n",
    "    @Returns\n",
    "    dict of lists. Keys have the format: 'o_{i}={val}', where val is either '0' or '1'\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for i in range(1, num_variables + 1):\n",
    "        for val in range(vals_per_var):\n",
    "            res[f'o_{i} = {val}'] = []\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function testing suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050000\n",
      "050001\n",
      "050010\n",
      "050011\n",
      "050100\n",
      "050101\n",
      "050110\n",
      "050111\n",
      "051000\n",
      "051001\n",
      "051010\n",
      "051011\n",
      "051100\n",
      "051101\n",
      "051110\n",
      "051111\n",
      "150000\n",
      "150001\n",
      "150010\n",
      "150011\n",
      "150100\n",
      "150101\n",
      "150110\n",
      "150111\n",
      "151000\n",
      "151001\n",
      "151010\n",
      "151011\n",
      "151100\n",
      "151101\n",
      "151110\n",
      "151111\n"
     ]
    }
   ],
   "source": [
    "get_ped_possiblities(2, 5, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define indicator RVs $G_i$, where $G_i = 1$ if there's a pedestrian in grid cell $i$ and $0$ if not.\n",
    "\n",
    "Want $Pr(G_i = 1 | action) = \\frac{Pr(action | G_i = 1) Pr(G_i = 1)}{Pr(G_i = 1)}$\n",
    "\n",
    "Consider grid 1:\n",
    "$\\begin{equation} \\label{eq1}\n",
    "\\begin{split}\n",
    "Pr(action | G_1 = 1) & = \\frac{Pr(action, G_1 = 1)}{Pr(G_1 = 1)} \\\\\n",
    " & = \\frac{\\sum_{g2 \\in \\{0, 1\\}} \\sum_{g3 \\in \\{0, 1\\}} \\sum_{g4 \\in \\{0, 1\\}} \\sum_{g5 \\in \\{0, 1\\}} \\sum_{g6 \\in \\{0, 1\\}} Pr(action, G_1 = 1, G_2 = g2, G_3 = g3, G_4 = g4, G_5 = g5, G_6 = g6)}{Pr(G_1 = 1)} \\\\\n",
    " & = \\frac{\\sum_{g2 \\in \\{0, 1\\}} \\sum_{g3 \\in \\{0, 1\\}} \\sum_{g4 \\in \\{0, 1\\}} \\sum_{g5 \\in \\{0, 1\\}} \\sum_{g6 \\in \\{0, 1\\}} Pr(action | G_1 = 1, G_2 = g2, G_3 = g3, G_4 = g4, G_5 = g5, G_6 = g6) Pr(G_1 = 1, G_2 = g2, G_3 = g3, G_4 = g4, G_5 = g5, G_6 = g6))}{Pr(G_1 = 1)}\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "\n",
    "The policy doesn't actually output a probability; the policy outputs a probability density.\n",
    "i.e. instead of outputting $Pr(action | G_1 = 1, G_2 = g2, G_3 = g3, G_4 = g4, G_5 = g5, G_6 = g6)$, \n",
    "the policy outputs $f(action | G_1 = 1, G_2 = g2, G_3 = g3, G_4 = g4, G_5 = g5, G_6 = g6)$\n",
    "\n",
    "Perhaps after some handwaving and assumptions about $Pr(G_i = 0) = 0.5$, I could say, I have $f(action | G_1 = 1) \\sim \\sum \\sum \\sum \\ldots \\ldots = k_1$. Note, I don't have equality here. Under the assumption of $Pr(G_i = 0) = 0.5$, the numerator joint probability / density term (assuming independence) is $0.5^6$.\n",
    "\n",
    "How do I convert these $k_i$'s into probabilities?\n",
    "\n",
    "Is it feasible to normalize just the 6 $f(action | G_i = 1)$ values, i.e assert\n",
    "\n",
    "$Pr(action | G_1 = 1) = \\frac{f(action | G_1 = 1)}{\\sum_{i=1}^6 f(action | G_i = 1)}$?\n",
    "\n",
    "I don't think this normalization is accurate. It seems more accurate to do:\n",
    "\n",
    "$Pr(action | G_1 = 1) = \\frac{f(action | G_1 = 1)}{f(action | G_1 = 1) + f(action | G_1 = 0)}$?\n",
    "\n",
    "My doubts are: \n",
    "\n",
    "1. the conversion from pdfs to probabilities - is this how we normalize pdfs to get probabilities?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_ped_given_action(action, mu_ped, s_ped, mu_no_ped, s_no_ped, prior, fixed_prior=True):\n",
    "    \"\"\"\n",
    "    @Params\n",
    "    mu_ped, s_ped: mean, sd pair from the policy receiving an input state where there is a visible pedestrian \n",
    "    mu_no_ped, s_no_ped: mean, sd pair from the policy receiving an input state where there is no visible pedestrian \n",
    "\n",
    "    action: the vehicle's acceleration as dictated by the policy\n",
    "    prior: Pr(ped)\n",
    "    fixed_prior: Boolean telling us whether to 'update' the prior Pr(ped) using Pr(ped | action) or not\n",
    "\n",
    "    @Returns\n",
    "    \n",
    "    probs, a dict containing:\n",
    "    1. Pr(action | ped)\n",
    "    2. Pr(action | no_ped)\n",
    "    3. Pr(ped | action)\n",
    "    4. Pr(no_ped | action)\n",
    "    5. Pr(ped) for the next computation of Pr(ped|action)\n",
    "    \n",
    "    3, 4, 5 are calculated subject to the fixed_prior parameter\n",
    "    \"\"\"\n",
    "    probs = {}\n",
    "    \n",
    "    # Compute 1, 2: Pr(action | ped), Pr(action | no_ped)\n",
    "    unnormed_pr_action_given_ped = accel_pdf(mu_ped, s_ped, action)\n",
    "    unnormed_pr_action_given_no_ped = accel_pdf(mu_no_ped, s_no_ped, action)\n",
    "    \n",
    "    pr_a_given_ped = unnormed_pr_action_given_ped / (unnormed_pr_action_given_ped + unnormed_pr_action_given_no_ped)\n",
    "    pr_a_given_no_ped = 1 - pr_a_given_ped\n",
    "    \n",
    "    probs[\"pr_a_given_ped\"] = pr_a_given_ped\n",
    "    probs[\"pr_a_given_no_ped\"] = pr_a_given_no_ped\n",
    "    \n",
    "    # Compute 3, 4: Pr(ped | action), Pr(no_ped | action)\n",
    "    # Apply Bayes' rule\n",
    "    pr_ped_given_action = (pr_a_given_ped * prior) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\n",
    "    pr_no_ped_given_action = (pr_a_given_no_ped * (1 - prior)) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\n",
    "    probs[\"pr_ped_given_action\"] = pr_ped_given_action\n",
    "    probs[\"pr_no_ped_given_action\"] = pr_no_ped_given_action\n",
    "                    \n",
    "    if fixed_prior:\n",
    "        probs[\"prior\"] = prior\n",
    "    else:\n",
    "        probs[\"prior\"] = probs[\"pr_ped_given_action\"]\n",
    "    return probs\n",
    "    \n",
    "\n",
    "def accel_pdf(mu, sigma, actual):\n",
    "    \"\"\"Return pdf evaluated at actual acceleration\"\"\"\n",
    "    coeff = 1 / np.sqrt(2 * np.pi * (sigma**2))\n",
    "    exp = -0.5 * ((actual - mu) / sigma)**2\n",
    "    return coeff * np.exp(exp)\n",
    "\n",
    "def run_transfer(args):\n",
    "    # run transfer on the bayesian 1 env first\n",
    "    bayesian_0_params = bayesian_0_flow_params(args, pedestrians=True, render=True)\n",
    "#     import ipdb; ipdb.set_trace()\n",
    "    env, env_name = create_env(args, bayesian_0_params)\n",
    "    agent, config = create_agent(args, flow_params=bayesian_0_params)\n",
    "    run_env(env, agent, config, bayesian_0_params)\n",
    "\n",
    "def plot_2_lines(y1, y2, legend, viewable_ped=False):\n",
    "    x = np.arange(len(y1))\n",
    "    plt.plot(x, y1)\n",
    "    plt.plot(x, y2)\n",
    "    if viewable_ped:\n",
    "        plt.plot(x, viewable_ped)\n",
    "    plt.legend(legend, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\n",
    "   \n",
    "    plt.draw()\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "def plot_lines(y_val_lsts, legends):\n",
    "    assert len(y_val_lsts) == len(legends)\n",
    "    x = np.arange(len(y_val_lsts[0]))\n",
    "    for y_vals in y_val_lsts:\n",
    "        plt.plot(x, y_vals)\n",
    "    plt.legend(legends, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\n",
    "    plt.draw()\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser():\n",
    "    \"\"\"Create the parser to capture CLI arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        description='[Flow] Evaluates a reinforcement learning agent '\n",
    "                    'given a checkpoint.',\n",
    "        epilog=EXAMPLE_USAGE)\n",
    "\n",
    "    # required input parameters\n",
    "    parser.add_argument(\n",
    "        'result_dir', type=str, help='Directory containing results')\n",
    "    parser.add_argument('checkpoint_num', type=str, help='Checkpoint number.')\n",
    "\n",
    "    # optional input parameters\n",
    "    parser.add_argument(\n",
    "        '--run',\n",
    "        type=str,\n",
    "        help='The algorithm or model to train. This may refer to '\n",
    "             'the name of a built-on algorithm (e.g. RLLib\\'s DQN '\n",
    "             'or PPO), or a user-defined trainable function or '\n",
    "             'class registered in the tune registry. '\n",
    "             'Required for results trained with flow-0.2.0 and before.')\n",
    "    parser.add_argument(\n",
    "        '--num_rollouts',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='The number of rollouts to visualize.')\n",
    "    parser.add_argument(\n",
    "        '--gen_emission',\n",
    "        action='store_true',\n",
    "        help='Specifies whether to generate an emission file from the '\n",
    "             'simulation')\n",
    "    parser.add_argument(\n",
    "        '--evaluate',\n",
    "        action='store_true',\n",
    "        help='Specifies whether to use the \\'evaluate\\' reward '\n",
    "             'for the environment.')\n",
    "    parser.add_argument(\n",
    "        '--render_mode',\n",
    "        type=str,\n",
    "        default='sumo_gui',\n",
    "        help='Pick the render mode. Options include sumo_web3d, '\n",
    "             'rgbd and sumo_gui')\n",
    "    parser.add_argument(\n",
    "        '--save_render',\n",
    "        action='store_true',\n",
    "        help='Saves a rendered video to a file. NOTE: Overrides render_mode '\n",
    "             'with pyglet rendering.')\n",
    "    parser.add_argument(\n",
    "        '--horizon',\n",
    "        type=int,\n",
    "        help='Specifies the horizon.')\n",
    "    \n",
    "    parser.add_argument('--grid_search', action='store_true', default=False,\n",
    "                        help='If true, a grid search is run')\n",
    "    parser.add_argument('--run_mode', type=str, default='local',\n",
    "                        help=\"Experiment run mode (local | cluster)\")\n",
    "    parser.add_argument('--algo', type=str, default='QMIX',\n",
    "                        help=\"RL method to use (PPO, TD3, QMIX)\")\n",
    "    parser.add_argument(\"--pedestrians\",\n",
    "                        help=\"use pedestrians, sidewalks, and crossings in the simulation\",\n",
    "                        action=\"store_true\")\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 20:38:10,886\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-04-07 20:38:10,889\tINFO resource_spec.py:216 -- Starting Ray with 6.74 GiB memory available for workers and up to 3.38 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: With render mode sumo_gui, an extra instance of the SUMO GUI will display before the GUI for visualizing the result. Click the green Play arrow to continue.\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "True\n",
      "NOTE: With render mode sumo_gui, an extra instance of the SUMO GUI will display before the GUI for visualizing the result. Click the green Play arrow to continue.\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "Error making env  Cannot re-register id: Bayesian1Env-v0\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 20:38:13,615\tINFO trainer.py:371 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2020-04-07 20:38:13,626\tINFO trainer.py:512 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use traffic lights: False\n",
      "allway_stop\n",
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "use traffic lights: False\n",
      "allway_stop\n",
      "Error making env  Cannot re-register id: Bayesian1Env-v0\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-07 20:38:17,533\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "2020-04-07 20:38:17,720\tINFO trainable.py:346 -- Restored from checkpoint: ./contrib_MADDPG_Bayesian1Env-v0_76a54d9c_2020-04-02_14-39-42gtq5jssz/checkpoint_100/checkpoint-100\n",
      "2020-04-07 20:38:17,721\tINFO trainable.py:353 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': 10000, '_time_total': 98.60757613182068, '_episodes_total': 22}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "[0. 0. 0. 0. 0. 0.] is list of length 6, ['0', '0', '0', '0', '0', '0'] has length 6\n",
      "> \u001b[0;32m<ipython-input-2-695afcacbaa9>\u001b[0m(134)\u001b[0;36mrun_env\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    133 \u001b[0;31m                            \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 134 \u001b[0;31m                            \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mln_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'behaviour_logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    135 \u001b[0;31m                            \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mln_sigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> l\n",
      "\u001b[1;32m    129 \u001b[0m                            \u001b[0ms_all_modified\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mped_front\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mped_back\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_comb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    130 \u001b[0m                            \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{s_all_modified[ped_front : ped_back + 1]} is list of length 6, {list(obs_comb)} has length 6'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    131 \u001b[0m                            \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_fetch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    132 \u001b[0m                            \u001b[0;31m# where in the docs did we figure out the logits were ln of sigma??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    133 \u001b[0m                            \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 134 \u001b[0;31m                            \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mln_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'behaviour_logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    135 \u001b[0m                            \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mln_sigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    136 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    137 \u001b[0m                            \u001b[0;31m# 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    138 \u001b[0m                            \u001b[0;31m# compute the conditional a_given_obs pdfs, i.e. f(a | o_1 = b_1, ..., o_6 = b_6)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    139 \u001b[0m                            \u001b[0mcond_a_given_joint_o_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccel_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> agent\n",
      "<ray.rllib.agents.trainer_template.MADDPG object at 0x7f3b48de1400>\n",
      "ipdb> policy_map_fn\n",
      "<function setup_exps_QMIX.<locals>.<lambda> at 0x7f3b2c1bb510>\n",
      "ipdb> import inspect\n",
      "ipdb> inspect.signature(policy_map_fn)\n",
      "<Signature (i)>\n",
      "ipdb> policy_map_fn(1)\n",
      "'policy_1'\n",
      "ipdb> config\n",
      "{'actor_feature_reg': 0.001, 'actor_hidden_activation': 'relu', 'actor_hiddens': [64, 64], 'actor_lr': 0.01, 'adv_policy': 'maddpg', 'agent_id': None, 'batch_mode': 'truncate_episodes', 'buffer_size': 1000000, 'callbacks': {'on_episode_end': '<function on_episode_end at 0x7f72ad2deae8>', 'on_episode_start': '<function on_episode_start at 0x7f72ad2de9d8>', 'on_episode_step': '<function on_episode_step at 0x7f72ad2dea60>'}, 'clip_actions': True, 'clip_rewards': None, 'collect_metrics_timeout': 180, 'compress_observations': False, 'critic_hidden_activation': 'relu', 'critic_hiddens': [64, 64], 'critic_lr': 0.01, 'custom_resources_per_worker': {}, 'eager': False, 'eager_tracing': False, 'env': 'Bayesian1Env-v0', 'env_config': {'flow_params': '{\\n    \"env\": {\\n        \"additional_params\": {\\n            \"max_accel\": 2.6,\\n            \"max_decel\": 4.5,\\n            \"max_num_objects\": 3,\\n            \"qmix\": true,\\n            \"search_radius\": 50,\\n            \"target_velocity\": 25\\n        },\\n        \"clip_actions\": true,\\n        \"evaluate\": false,\\n        \"horizon\": 500,\\n        \"sims_per_step\": 1,\\n        \"warmup_steps\": 0\\n    },\\n    \"env_name\": \"flow.envs.multiagent.bayesian_1_env.Bayesian1Env\",\\n    \"exp_tag\": \"bayesian_1_env\",\\n    \"initial\": {\\n        \"additional_params\": {},\\n        \"bunching\": 0,\\n        \"edges_distribution\": \"all\",\\n        \"lanes_distribution\": Infinity,\\n        \"min_gap\": 0,\\n        \"perturbation\": 0.0,\\n        \"shuffle\": false,\\n        \"sidewalks\": true,\\n        \"spacing\": \"custom\",\\n        \"x0\": 0\\n    },\\n    \"net\": {\\n        \"additional_params\": {\\n            \"grid_array\": {\\n                \"cars_bot\": 1,\\n                \"cars_left\": 0,\\n                \"cars_right\": 1,\\n                \"cars_top\": 1,\\n                \"col_num\": 1,\\n                \"inner_length\": 50,\\n                \"row_num\": 1\\n            },\\n            \"horizontal_lanes\": 1,\\n            \"randomize_routes\": true,\\n            \"speed_limit\": 35,\\n            \"vertical_lanes\": 1\\n        },\\n        \"inflows\": {\\n            \"_InFlows__flows\": []\\n        },\\n        \"osm_path\": null,\\n        \"template\": null\\n    },\\n    \"network\": \"flow.networks.bayesian_1.Bayesian1Network\",\\n    \"ped\": {\\n        \"_PedestrianParams__pedestrians\": {},\\n        \"ids\": [\\n            \"ped_0\"\\n        ],\\n        \"num_pedestrians\": 1,\\n        \"params\": {\\n            \"ped_0\": {\\n                \"arrivalPos\": \"43\",\\n                \"depart\": \"0.00\",\\n                \"departPos\": \"5\",\\n                \"from\": \"(1.1)--(2.1)\",\\n                \"id\": \"ped_0\",\\n                \"to\": \"(2.1)--(1.1)\"\\n            }\\n        }\\n    },\\n    \"sim\": {\\n        \"color_vehicles\": true,\\n        \"emission_path\": null,\\n        \"lateral_resolution\": null,\\n        \"no_step_log\": true,\\n        \"num_clients\": 1,\\n        \"overtake_right\": false,\\n        \"port\": null,\\n        \"print_warnings\": true,\\n        \"pxpm\": 2,\\n        \"render\": false,\\n        \"restart_instance\": true,\\n        \"rllib_training\": false,\\n        \"save_render\": false,\\n        \"seed\": null,\\n        \"show_radius\": false,\\n        \"sight_radius\": 25,\\n        \"sim_step\": 0.1,\\n        \"teleport_time\": -1\\n    },\\n    \"simulator\": \"traci\",\\n    \"veh\": [\\n        {\\n            \"acceleration_controller\": [\\n                \"SimCarFollowingController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 2.6,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 7.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 25\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 2,\\n            \"routing_controller\": [\\n                \"GridRouter\",\\n                {}\\n            ],\\n            \"veh_id\": \"human\"\\n        },\\n        {\\n            \"acceleration_controller\": [\\n                \"RLController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 2.6,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 4.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 25\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 1,\\n            \"routing_controller\": [\\n                \"GridRouter\",\\n                {}\\n            ],\\n            \"veh_id\": \"rl\"\\n        }\\n    ]\\n}', 'run': 'contrib/MADDPG'}, 'evaluation_config': {}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'gamma': 0.999, 'good_policy': 'maddpg', 'grad_norm_clipping': 0.5, 'horizon': 500, 'ignore_worker_failures': False, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'learning_starts': 25600, 'local_tf_session_args': {'inter_op_parallelism_threads': 8, 'intra_op_parallelism_threads': 8}, 'log_level': 'WARN', 'log_sys_usage': True, 'lr': 0.0001, 'memory': 0, 'memory_per_worker': 0, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'model': {'conv_activation': 'relu', 'conv_filters': None, 'custom_action_dist': None, 'custom_model': None, 'custom_options': {}, 'custom_preprocessor': None, 'dim': 84, 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'framestack': True, 'free_log_std': False, 'grayscale': False, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'max_seq_len': 20, 'no_final_linear': False, 'state_shape': None, 'use_lstm': False, 'vf_share_layers': True, 'zero_mean': True}, 'monitor': False, 'multiagent': {'policies': {'policy_0': (None, Box(25,), Box(1,), {'agent_id': 0, 'use_local_critic': True, 'obs_space_dict': {0: Box(25,), 1: Box(25,), 2: Box(25,)}, 'act_space_dict': {0: Box(1,), 1: Box(1,), 2: Box(1,)}}), 'policy_1': (None, Box(25,), Box(1,), {'agent_id': 1, 'use_local_critic': True, 'obs_space_dict': {0: Box(25,), 1: Box(25,), 2: Box(25,)}, 'act_space_dict': {0: Box(1,), 1: Box(1,), 2: Box(1,)}}), 'policy_2': (None, Box(25,), Box(1,), {'agent_id': 2, 'use_local_critic': True, 'obs_space_dict': {0: Box(25,), 1: Box(25,), 2: Box(25,)}, 'act_space_dict': {0: Box(1,), 1: Box(1,), 2: Box(1,)}})}, 'policy_mapping_fn': <function setup_exps_QMIX.<locals>.<lambda> at 0x7f3b2c1bb510>}, 'n_step': 1, 'no_done_at_end': True, 'no_eager_on_workers': False, 'num_cpus_for_driver': 1, 'num_cpus_per_worker': 1, 'num_envs_per_worker': 1, 'num_gpus': 0, 'num_gpus_per_worker': 0, 'num_workers': 0, 'object_store_memory': 0, 'object_store_memory_per_worker': 0, 'observation_filter': 'NoFilter', 'optimizer': {}, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'postprocess_inputs': False, 'preprocessor_pref': 'deepmind', 'remote_env_batch_wait_ms': 0, 'remote_worker_envs': False, 'sample_async': False, 'sample_batch_size': 100, 'seed': None, 'shuffle_buffer_size': 0, 'soft_horizon': False, 'synchronize_filters': True, 'target_network_update_freq': 0, 'tau': 0.01, 'tf_session_args': {'allow_soft_placement': True, 'device_count': {'CPU': 1}, 'gpu_options': {'allow_growth': True}, 'inter_op_parallelism_threads': 2, 'intra_op_parallelism_threads': 2, 'log_device_placement': False}, 'timesteps_per_iteration': 0, 'train_batch_size': 1024, 'use_local_critic': False, 'use_state_preprocessor': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> policy_mapping_fn.get_weights()\n",
      "*** NameError: name 'policy_mapping_fn' is not defined\n",
      "ipdb> agent\n",
      "<ray.rllib.agents.trainer_template.MADDPG object at 0x7f3b48de1400>\n",
      "ipdb> dir(agent)\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_allow_unknown_configs', '_allow_unknown_subkeys', '_before_evaluate', '_default_config', '_env_id', '_episodes_total', '_evaluate', '_experiment_id', '_export_model', '_has_policy_optimizer', '_init', '_iteration', '_iterations_since_restore', '_local_ip', '_log_result', '_logdir', '_make_workers', '_monitor', '_name', '_policy', '_register_if_needed', '_restore', '_restored', '_result_logger', '_save', '_setup', '_stop', '_time_since_restore', '_time_total', '_timesteps_since_restore', '_timesteps_total', '_train', '_try_recover', '_validate_config', 'collect_metrics', 'compute_action', 'config', 'current_ip', 'default_resource_request', 'env_creator', 'export_model', 'export_policy_checkpoint', 'export_policy_model', 'get_config', 'get_policy', 'get_weights', 'global_vars', 'iteration', 'logdir', 'optimizer', 'raw_user_config', 'reset_config', 'resource_help', 'restore', 'restore_from_object', 'save', 'save_to_object', 'set_weights', 'state', 'stop', 'train', 'with_updates', 'workers']\n",
      "ipdb> agent.get_weigths\n",
      "*** AttributeError: 'MADDPG' object has no attribute 'get_weigths'\n",
      "ipdb> agent.get_weights\n",
      "<bound method Trainer.get_weights of <ray.rllib.agents.trainer_template.MADDPG object at 0x7f3b48de1400>>\n",
      "ipdb> agent.get_weights()\n",
      "{'policy_0': [array([[ 0.23747978, -0.1481274 , -0.19575025, ..., -0.03279233,\n",
      "        -0.14341465,  0.22023335],\n",
      "       [-0.17459184,  0.18790334, -0.22791141, ...,  0.07554972,\n",
      "         0.18341258, -0.18313888],\n",
      "       [ 0.18088838, -0.0099757 ,  0.2372458 , ..., -0.23764943,\n",
      "         0.01065353,  0.07818785],\n",
      "       ...,\n",
      "       [-0.02291285,  0.03670314,  0.01039818, ...,  0.19822234,\n",
      "         0.14868507, -0.01169278],\n",
      "       [-0.2054506 , -0.2463583 ,  0.11976114, ...,  0.08959141,\n",
      "         0.22685036,  0.21834964],\n",
      "       [ 0.2065011 ,  0.0607813 , -0.18414593, ...,  0.15406734,\n",
      "         0.14386347,  0.12077218]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.19289632,  0.07341258,  0.0090829 , ...,  0.1542113 ,\n",
      "        -0.16960421, -0.06412701],\n",
      "       [ 0.09312277, -0.09149259, -0.06797719, ...,  0.0999649 ,\n",
      "         0.05966724, -0.12060098],\n",
      "       [-0.20793545, -0.11003377, -0.07183485, ...,  0.0633565 ,\n",
      "         0.1302679 , -0.21579131],\n",
      "       ...,\n",
      "       [-0.05086488, -0.13342023,  0.01959896, ...,  0.15876435,\n",
      "        -0.1841814 , -0.03214742],\n",
      "       [ 0.11273371, -0.03038235,  0.09601562, ...,  0.11186837,\n",
      "        -0.18395248,  0.0453759 ],\n",
      "       [ 0.18111734, -0.13734165, -0.06908876, ...,  0.06760754,\n",
      "         0.01063964, -0.02812211]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.02163258],\n",
      "       [-0.19924209],\n",
      "       [ 0.10486734],\n",
      "       [ 0.01455262],\n",
      "       [ 0.25584   ],\n",
      "       [-0.24736024],\n",
      "       [-0.28293547],\n",
      "       [-0.22738059],\n",
      "       [ 0.22795188],\n",
      "       [ 0.21416384],\n",
      "       [-0.14130406],\n",
      "       [ 0.23100555],\n",
      "       [ 0.26423663],\n",
      "       [-0.22010526],\n",
      "       [ 0.27997988],\n",
      "       [-0.2558909 ],\n",
      "       [-0.11711048],\n",
      "       [-0.1264073 ],\n",
      "       [ 0.10690755],\n",
      "       [-0.23237973],\n",
      "       [ 0.2768982 ],\n",
      "       [ 0.2090472 ],\n",
      "       [-0.18413801],\n",
      "       [ 0.07215101],\n",
      "       [-0.02204707],\n",
      "       [ 0.08979332],\n",
      "       [-0.26847702],\n",
      "       [ 0.09181476],\n",
      "       [-0.09625709],\n",
      "       [ 0.14760351],\n",
      "       [-0.00679195],\n",
      "       [-0.28816625],\n",
      "       [-0.25642535],\n",
      "       [ 0.26223016],\n",
      "       [ 0.11010337],\n",
      "       [ 0.2398727 ],\n",
      "       [ 0.16194731],\n",
      "       [ 0.02524465],\n",
      "       [ 0.14059448],\n",
      "       [-0.02485234],\n",
      "       [ 0.02828118],\n",
      "       [-0.06542337],\n",
      "       [-0.19845992],\n",
      "       [ 0.1231519 ],\n",
      "       [ 0.03267884],\n",
      "       [-0.20659602],\n",
      "       [-0.11460061],\n",
      "       [-0.11863041],\n",
      "       [ 0.05308601],\n",
      "       [ 0.20910162],\n",
      "       [-0.04665956],\n",
      "       [-0.22557662],\n",
      "       [-0.16571394],\n",
      "       [-0.29068285],\n",
      "       [-0.2240895 ],\n",
      "       [ 0.24631757],\n",
      "       [-0.02215153],\n",
      "       [-0.0050106 ],\n",
      "       [-0.14833108],\n",
      "       [ 0.16382957],\n",
      "       [ 0.26505733],\n",
      "       [ 0.06283781],\n",
      "       [ 0.15828243],\n",
      "       [-0.2157124 ]], dtype=float32), array([0.], dtype=float32), array([[ 0.03027987,  0.15722865,  0.19925046, ..., -0.05717561,\n",
      "        -0.08668692, -0.03836668],\n",
      "       [ 0.00578478, -0.25960034,  0.03856346, ...,  0.01821238,\n",
      "         0.24681813, -0.25734264],\n",
      "       [-0.22895053, -0.13653597,  0.23323628, ...,  0.12357172,\n",
      "        -0.03220299, -0.17298546],\n",
      "       ...,\n",
      "       [-0.07301958, -0.18783271, -0.09233508, ..., -0.07546529,\n",
      "        -0.06560263, -0.04852931],\n",
      "       [-0.08846055,  0.20874628, -0.08465126, ...,  0.1638664 ,\n",
      "        -0.12475818, -0.16664925],\n",
      "       [-0.09841488,  0.02027965, -0.09908722, ...,  0.25849742,\n",
      "         0.11434713, -0.148833  ]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.18923312, -0.12732056, -0.19749744, ...,  0.11115144,\n",
      "         0.13151117, -0.14362963],\n",
      "       [-0.1453727 ,  0.06393056,  0.18076397, ..., -0.1463149 ,\n",
      "         0.11321934,  0.16501792],\n",
      "       [ 0.15466483,  0.08738552, -0.10073361, ..., -0.16574682,\n",
      "        -0.18239781, -0.20223004],\n",
      "       ...,\n",
      "       [-0.16597313,  0.20488228,  0.01340477, ...,  0.05260743,\n",
      "        -0.19732943,  0.05440818],\n",
      "       [ 0.14059283,  0.05855952, -0.14945278, ...,  0.01552714,\n",
      "         0.17151134, -0.17456281],\n",
      "       [-0.10093632, -0.17366835,  0.14401673, ...,  0.111192  ,\n",
      "         0.064936  ,  0.08037026]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.18237808],\n",
      "       [-0.09053335],\n",
      "       [-0.01393843],\n",
      "       [ 0.08440179],\n",
      "       [-0.26558483],\n",
      "       [ 0.28245074],\n",
      "       [-0.3021511 ],\n",
      "       [ 0.2903046 ],\n",
      "       [ 0.11281815],\n",
      "       [ 0.1528405 ],\n",
      "       [-0.00788698],\n",
      "       [-0.18376264],\n",
      "       [ 0.11227858],\n",
      "       [ 0.1849772 ],\n",
      "       [-0.03085372],\n",
      "       [-0.18477523],\n",
      "       [-0.15909678],\n",
      "       [-0.20842418],\n",
      "       [ 0.27526826],\n",
      "       [ 0.11822873],\n",
      "       [-0.10379456],\n",
      "       [-0.05000055],\n",
      "       [-0.0731912 ],\n",
      "       [ 0.20764554],\n",
      "       [-0.24245512],\n",
      "       [-0.14102843],\n",
      "       [ 0.04995254],\n",
      "       [-0.06273378],\n",
      "       [-0.17087245],\n",
      "       [ 0.16134891],\n",
      "       [-0.11676386],\n",
      "       [-0.30046153],\n",
      "       [-0.10987222],\n",
      "       [-0.2630315 ],\n",
      "       [-0.28047124],\n",
      "       [ 0.18953049],\n",
      "       [ 0.10706544],\n",
      "       [-0.23798692],\n",
      "       [ 0.2522725 ],\n",
      "       [-0.1381895 ],\n",
      "       [ 0.1289128 ],\n",
      "       [-0.09331949],\n",
      "       [ 0.1469211 ],\n",
      "       [-0.1266819 ],\n",
      "       [-0.01032528],\n",
      "       [ 0.26729035],\n",
      "       [-0.01471582],\n",
      "       [-0.05610438],\n",
      "       [ 0.09637973],\n",
      "       [-0.1219943 ],\n",
      "       [ 0.15770209],\n",
      "       [ 0.12227419],\n",
      "       [ 0.15738589],\n",
      "       [-0.1519356 ],\n",
      "       [ 0.20348585],\n",
      "       [ 0.20078391],\n",
      "       [-0.29463965],\n",
      "       [-0.14643265],\n",
      "       [-0.07129966],\n",
      "       [-0.27495888],\n",
      "       [ 0.16803953],\n",
      "       [ 0.10285869],\n",
      "       [ 0.23051274],\n",
      "       [ 0.25193143]], dtype=float32), array([0.], dtype=float32), array([[ 0.19302565, -0.04350103, -0.03064988, ..., -0.10481738,\n",
      "        -0.09779254,  0.0990052 ],\n",
      "       [-0.14363123,  0.14710696, -0.18869926, ...,  0.07200281,\n",
      "         0.18309318, -0.05576954],\n",
      "       [ 0.14290054,  0.05924881,  0.08036356, ..., -0.12337548,\n",
      "         0.05377639,  0.04112445],\n",
      "       ...,\n",
      "       [ 0.0262734 ,  0.02691524, -0.02328755, ...,  0.10522425,\n",
      "         0.00625565,  0.00801087],\n",
      "       [-0.04704669, -0.19114806,  0.06421689, ...,  0.06751885,\n",
      "         0.1327044 ,  0.08247846],\n",
      "       [ 0.07519738, -0.02399348, -0.15389666, ...,  0.13874753,\n",
      "         0.10361321,  0.00238043]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.14000247,  0.05728384,  0.02260126, ...,  0.03094952,\n",
      "        -0.15506837, -0.0787214 ],\n",
      "       [ 0.10117751, -0.07208082,  0.02070291, ...,  0.04255088,\n",
      "        -0.03075805, -0.02801283],\n",
      "       [-0.19760169, -0.02850804,  0.0100636 , ..., -0.01975782,\n",
      "         0.09672596, -0.11861073],\n",
      "       ...,\n",
      "       [-0.06143669, -0.0194242 , -0.01668161, ...,  0.10502842,\n",
      "        -0.07451562, -0.00181015],\n",
      "       [-0.00236688, -0.01531994,  0.01215395, ...,  0.06659223,\n",
      "        -0.07243435,  0.0240213 ],\n",
      "       [ 0.1449928 , -0.13533004, -0.06895835, ...,  0.05884318,\n",
      "         0.00466524,  0.06037128]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.09541249],\n",
      "       [-0.02954411],\n",
      "       [ 0.13561386],\n",
      "       [ 0.02015249],\n",
      "       [ 0.13715887],\n",
      "       [-0.05477099],\n",
      "       [-0.08742139],\n",
      "       [-0.04871905],\n",
      "       [ 0.23835579],\n",
      "       [ 0.17125405],\n",
      "       [-0.13892804],\n",
      "       [ 0.11531758],\n",
      "       [ 0.12195177],\n",
      "       [-0.02972565],\n",
      "       [ 0.23801793],\n",
      "       [-0.1392073 ],\n",
      "       [ 0.01317077],\n",
      "       [-0.0958262 ],\n",
      "       [-0.02753092],\n",
      "       [-0.222963  ],\n",
      "       [ 0.12788515],\n",
      "       [ 0.16543333],\n",
      "       [-0.11614738],\n",
      "       [-0.00489846],\n",
      "       [-0.06726422],\n",
      "       [ 0.1490296 ],\n",
      "       [-0.25450364],\n",
      "       [ 0.11882211],\n",
      "       [-0.14603482],\n",
      "       [ 0.01624682],\n",
      "       [ 0.03977152],\n",
      "       [-0.16340694],\n",
      "       [-0.22062635],\n",
      "       [ 0.09373963],\n",
      "       [ 0.01807388],\n",
      "       [ 0.17654228],\n",
      "       [ 0.04155057],\n",
      "       [ 0.06172853],\n",
      "       [ 0.09768475],\n",
      "       [ 0.0212617 ],\n",
      "       [-0.0868177 ],\n",
      "       [-0.00797685],\n",
      "       [-0.16797872],\n",
      "       [ 0.11190908],\n",
      "       [-0.07387119],\n",
      "       [-0.04615045],\n",
      "       [ 0.02810569],\n",
      "       [-0.18434818],\n",
      "       [-0.02116042],\n",
      "       [ 0.08847339],\n",
      "       [ 0.05716554],\n",
      "       [-0.15142763],\n",
      "       [-0.12066615],\n",
      "       [-0.09517504],\n",
      "       [-0.07351341],\n",
      "       [ 0.2326007 ],\n",
      "       [-0.06192832],\n",
      "       [-0.03487399],\n",
      "       [-0.02440623],\n",
      "       [ 0.21356665],\n",
      "       [ 0.12367394],\n",
      "       [-0.00341986],\n",
      "       [ 0.14116925],\n",
      "       [-0.08828515]], dtype=float32), array([0.], dtype=float32), array([[ 2.70900805e-03,  3.57247330e-02,  2.10737228e-01, ...,\n",
      "        -3.60663533e-02,  3.83888208e-03, -6.11109771e-02],\n",
      "       [ 5.53471334e-02, -1.19602062e-01, -4.40260172e-02, ...,\n",
      "        -1.57979783e-02,  1.97133407e-01, -9.25665572e-02],\n",
      "       [-1.13688670e-01, -2.04418153e-02,  1.80020034e-01, ...,\n",
      "        -7.81212375e-03,  7.37485662e-02, -1.37866408e-01],\n",
      "       ...,\n",
      "       [-1.54180918e-04, -7.59278908e-02, -6.96215183e-02, ...,\n",
      "        -1.06806867e-01, -9.24065243e-03,  1.82346664e-02],\n",
      "       [-2.93403883e-02,  1.12469763e-01, -1.09121449e-01, ...,\n",
      "         1.13760784e-01, -4.13675308e-02, -1.34684876e-01],\n",
      "       [-1.10493183e-01,  1.07615499e-03, -4.77418453e-02, ...,\n",
      "         1.98075578e-01,  4.94904630e-02, -1.58272862e-01]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.11171559, -0.10044737, -0.13256325, ...,  0.13268961,\n",
      "         0.05166677, -0.01264494],\n",
      "       [-0.10630101,  0.07205412,  0.15673594, ..., -0.08284548,\n",
      "         0.06934015,  0.14721313],\n",
      "       [ 0.08349611,  0.10880944, -0.0561184 , ..., -0.15288231,\n",
      "        -0.09058063, -0.19581428],\n",
      "       ...,\n",
      "       [-0.17085268,  0.12784906,  0.0008951 , ..., -0.04148605,\n",
      "        -0.09803876,  0.01663004],\n",
      "       [ 0.12683375,  0.11202123, -0.01721468, ...,  0.01124871,\n",
      "         0.07991313, -0.12645619],\n",
      "       [-0.01543864, -0.14120606,  0.11258987, ...,  0.03633181,\n",
      "         0.04963503,  0.08574238]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.01952256],\n",
      "       [-0.0199746 ],\n",
      "       [ 0.06016624],\n",
      "       [ 0.11135215],\n",
      "       [-0.08533115],\n",
      "       [ 0.06810454],\n",
      "       [-0.26569358],\n",
      "       [ 0.15831138],\n",
      "       [ 0.00876182],\n",
      "       [ 0.09790666],\n",
      "       [ 0.04508695],\n",
      "       [-0.08125157],\n",
      "       [ 0.04343552],\n",
      "       [ 0.17594938],\n",
      "       [-0.09078889],\n",
      "       [-0.22765818],\n",
      "       [ 0.00967178],\n",
      "       [-0.20552959],\n",
      "       [ 0.11237598],\n",
      "       [-0.00869555],\n",
      "       [-0.12516145],\n",
      "       [ 0.01447444],\n",
      "       [-0.03161126],\n",
      "       [ 0.17525752],\n",
      "       [-0.17473559],\n",
      "       [-0.06338243],\n",
      "       [ 0.05274795],\n",
      "       [-0.03237149],\n",
      "       [-0.21209134],\n",
      "       [ 0.05929645],\n",
      "       [-0.00960364],\n",
      "       [-0.16093123],\n",
      "       [-0.0344578 ],\n",
      "       [-0.16923119],\n",
      "       [-0.18737747],\n",
      "       [ 0.19657475],\n",
      "       [ 0.15474048],\n",
      "       [-0.16680254],\n",
      "       [ 0.13944466],\n",
      "       [-0.11645361],\n",
      "       [ 0.06288432],\n",
      "       [-0.09997573],\n",
      "       [ 0.03049853],\n",
      "       [ 0.01479232],\n",
      "       [ 0.02718754],\n",
      "       [ 0.12587105],\n",
      "       [-0.01277632],\n",
      "       [-0.09427024],\n",
      "       [ 0.01257813],\n",
      "       [-0.11524522],\n",
      "       [ 0.10619271],\n",
      "       [-0.00838416],\n",
      "       [ 0.00207858],\n",
      "       [-0.02653386],\n",
      "       [ 0.10874003],\n",
      "       [ 0.08236536],\n",
      "       [-0.22455981],\n",
      "       [-0.13054033],\n",
      "       [ 0.0323014 ],\n",
      "       [-0.28352663],\n",
      "       [ 0.09016009],\n",
      "       [ 0.14254971],\n",
      "       [ 0.08888873],\n",
      "       [ 0.24530797]], dtype=float32), array([0.], dtype=float32)], 'policy_1': [array([[-0.01323712, -0.02873157,  0.13538578, ...,  0.20113319,\n",
      "         0.23093608, -0.15423907],\n",
      "       [-0.14832143,  0.11529055,  0.04271245, ..., -0.02319011,\n",
      "         0.10418043, -0.25413886],\n",
      "       [-0.15348658, -0.1556912 , -0.03280576, ..., -0.02650508,\n",
      "         0.07698375,  0.10561398],\n",
      "       ...,\n",
      "       [-0.22171152,  0.0034613 , -0.14853978, ..., -0.16972189,\n",
      "         0.06467849, -0.05331427],\n",
      "       [ 0.00318667, -0.17094648,  0.23202851, ...,  0.09671795,\n",
      "        -0.12864119,  0.17634511],\n",
      "       [-0.07880116, -0.24101076, -0.01076464, ..., -0.13202862,\n",
      "         0.2053582 , -0.2304945 ]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.08177598, -0.21048652, -0.13764666, ..., -0.19250175,\n",
      "        -0.19609898, -0.204798  ],\n",
      "       [ 0.01591784,  0.09379904,  0.01264679, ...,  0.15165852,\n",
      "        -0.03809084,  0.10036395],\n",
      "       [ 0.1204185 , -0.06507583, -0.12284791, ...,  0.06861763,\n",
      "        -0.05181296,  0.12665354],\n",
      "       ...,\n",
      "       [-0.12559845, -0.03125992, -0.11897942, ..., -0.1250326 ,\n",
      "         0.10023616, -0.08794264],\n",
      "       [-0.1294213 , -0.08374085, -0.02007711, ..., -0.16342494,\n",
      "        -0.16570434, -0.14173165],\n",
      "       [-0.10573886, -0.15745978,  0.07894181, ...,  0.10397138,\n",
      "        -0.04062472,  0.04182528]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.23174882],\n",
      "       [-0.04534367],\n",
      "       [ 0.22494072],\n",
      "       [-0.07778883],\n",
      "       [-0.05459754],\n",
      "       [ 0.21480507],\n",
      "       [ 0.13851285],\n",
      "       [ 0.13382801],\n",
      "       [ 0.0667057 ],\n",
      "       [ 0.02741462],\n",
      "       [-0.09022579],\n",
      "       [ 0.029053  ],\n",
      "       [ 0.1980201 ],\n",
      "       [ 0.24904895],\n",
      "       [-0.21905826],\n",
      "       [-0.19575587],\n",
      "       [ 0.20882273],\n",
      "       [ 0.18243358],\n",
      "       [-0.12884994],\n",
      "       [ 0.14391643],\n",
      "       [-0.2699346 ],\n",
      "       [-0.15074185],\n",
      "       [-0.04663202],\n",
      "       [-0.00923032],\n",
      "       [ 0.10834199],\n",
      "       [ 0.13675171],\n",
      "       [-0.09033096],\n",
      "       [ 0.17553073],\n",
      "       [-0.02422589],\n",
      "       [ 0.03201705],\n",
      "       [-0.1019638 ],\n",
      "       [-0.1817021 ],\n",
      "       [ 0.20751762],\n",
      "       [ 0.17160341],\n",
      "       [ 0.05799454],\n",
      "       [-0.14657281],\n",
      "       [ 0.04214138],\n",
      "       [ 0.2086814 ],\n",
      "       [ 0.19885457],\n",
      "       [-0.10687885],\n",
      "       [-0.01214394],\n",
      "       [-0.10739663],\n",
      "       [ 0.23343253],\n",
      "       [-0.05886625],\n",
      "       [-0.20586361],\n",
      "       [-0.01891086],\n",
      "       [-0.05049929],\n",
      "       [-0.00183925],\n",
      "       [-0.11560959],\n",
      "       [-0.19540823],\n",
      "       [ 0.2717747 ],\n",
      "       [ 0.03776476],\n",
      "       [ 0.2545504 ],\n",
      "       [-0.00626493],\n",
      "       [-0.11420184],\n",
      "       [ 0.07780099],\n",
      "       [-0.05000019],\n",
      "       [ 0.05213737],\n",
      "       [-0.0492354 ],\n",
      "       [-0.24160029],\n",
      "       [ 0.0774039 ],\n",
      "       [ 0.10705906],\n",
      "       [ 0.25133717],\n",
      "       [ 0.10182971]], dtype=float32), array([0.], dtype=float32), array([[ 0.02999404,  0.00468051, -0.12812918, ..., -0.15667678,\n",
      "        -0.13226531, -0.07064252],\n",
      "       [-0.02107501, -0.10558234, -0.02471121, ..., -0.24525495,\n",
      "        -0.10785323, -0.03416151],\n",
      "       [ 0.23373628, -0.12093572,  0.10203856, ...,  0.2458545 ,\n",
      "         0.17043272,  0.12949702],\n",
      "       ...,\n",
      "       [ 0.05821702,  0.24088609, -0.01547359, ...,  0.0843443 ,\n",
      "         0.1287598 ,  0.14774549],\n",
      "       [ 0.21457717, -0.05927534, -0.03990684, ...,  0.12101218,\n",
      "        -0.21087529,  0.21975398],\n",
      "       [-0.15892532, -0.17022112, -0.12178214, ..., -0.23927097,\n",
      "        -0.0328232 ,  0.02907395]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.08059131,  0.12366532,  0.13603614, ..., -0.031991  ,\n",
      "        -0.1540553 , -0.17826751],\n",
      "       [-0.13599046,  0.21045695,  0.1323605 , ...,  0.19129778,\n",
      "         0.00340582, -0.1682297 ],\n",
      "       [ 0.09435467, -0.08543308, -0.07947618, ..., -0.18897766,\n",
      "         0.15153126,  0.11376064],\n",
      "       ...,\n",
      "       [ 0.00379705,  0.00699249,  0.01593384, ...,  0.02779396,\n",
      "         0.19777746, -0.18379882],\n",
      "       [ 0.05059229, -0.07714252,  0.01323582, ...,  0.0098732 ,\n",
      "        -0.18766855, -0.1850368 ],\n",
      "       [-0.21106157, -0.08943841,  0.04531805, ...,  0.08732219,\n",
      "         0.14137517, -0.14272857]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.19670905],\n",
      "       [ 0.17226642],\n",
      "       [-0.247009  ],\n",
      "       [ 0.10253105],\n",
      "       [-0.28326455],\n",
      "       [ 0.14844167],\n",
      "       [-0.03912947],\n",
      "       [-0.27123404],\n",
      "       [ 0.03958336],\n",
      "       [ 0.16773477],\n",
      "       [-0.05198619],\n",
      "       [-0.02286908],\n",
      "       [ 0.06105521],\n",
      "       [ 0.0439986 ],\n",
      "       [-0.07507969],\n",
      "       [-0.10234453],\n",
      "       [-0.25054935],\n",
      "       [-0.28984693],\n",
      "       [-0.05282858],\n",
      "       [-0.07535183],\n",
      "       [ 0.03549591],\n",
      "       [ 0.21739608],\n",
      "       [-0.1315172 ],\n",
      "       [ 0.01777446],\n",
      "       [-0.27713138],\n",
      "       [-0.06216088],\n",
      "       [ 0.17060858],\n",
      "       [ 0.12198612],\n",
      "       [-0.16525897],\n",
      "       [-0.20788641],\n",
      "       [-0.04017603],\n",
      "       [-0.1598341 ],\n",
      "       [ 0.18936673],\n",
      "       [ 0.16651937],\n",
      "       [-0.0304603 ],\n",
      "       [-0.14538355],\n",
      "       [ 0.24830389],\n",
      "       [ 0.12433025],\n",
      "       [ 0.14326862],\n",
      "       [ 0.28739482],\n",
      "       [ 0.03374279],\n",
      "       [-0.1181674 ],\n",
      "       [ 0.2193073 ],\n",
      "       [ 0.0313499 ],\n",
      "       [-0.07863365],\n",
      "       [ 0.11708665],\n",
      "       [-0.15191568],\n",
      "       [-0.276719  ],\n",
      "       [-0.03808102],\n",
      "       [ 0.20448273],\n",
      "       [-0.00190669],\n",
      "       [ 0.15994263],\n",
      "       [ 0.27708715],\n",
      "       [ 0.27501166],\n",
      "       [-0.06844926],\n",
      "       [-0.04651982],\n",
      "       [ 0.12849826],\n",
      "       [ 0.2588889 ],\n",
      "       [ 0.11165467],\n",
      "       [-0.18549955],\n",
      "       [-0.17397694],\n",
      "       [ 0.16371316],\n",
      "       [ 0.20697004],\n",
      "       [ 0.02055052]], dtype=float32), array([0.], dtype=float32), array([[-0.03058783,  0.06113428,  0.12282456, ...,  0.10795117,\n",
      "         0.13224864, -0.06593235],\n",
      "       [-0.01144452, -0.00232614, -0.04858343, ..., -0.00871529,\n",
      "        -0.01675834, -0.11904513],\n",
      "       [-0.08648125, -0.09588548, -0.08189563, ..., -0.05703888,\n",
      "        -0.01847622,  0.11263499],\n",
      "       ...,\n",
      "       [-0.15715791,  0.04775542, -0.14206246, ..., -0.15378736,\n",
      "         0.0543839 ,  0.04160823],\n",
      "       [-0.0345225 , -0.1017748 ,  0.12178257, ...,  0.03233021,\n",
      "        -0.00330423,  0.15807517],\n",
      "       [ 0.02258741, -0.12725313, -0.01762457, ..., -0.08534915,\n",
      "         0.09990437, -0.19120814]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.0234    , -0.06475577, -0.03337555, ..., -0.09508833,\n",
      "        -0.15536489, -0.14849569],\n",
      "       [-0.06171826,  0.09977493,  0.08469467, ...,  0.03138552,\n",
      "        -0.02993266,  0.04557983],\n",
      "       [ 0.1348785 , -0.04515353, -0.13185069, ...,  0.04407418,\n",
      "        -0.00547357,  0.06820188],\n",
      "       ...,\n",
      "       [-0.11869711, -0.08345293, -0.070467  , ..., -0.10908982,\n",
      "         0.12259987,  0.0001906 ],\n",
      "       [-0.04616589, -0.0507342 ,  0.01106118, ..., -0.08973139,\n",
      "        -0.0468115 , -0.0246893 ],\n",
      "       [-0.1348025 , -0.13317768,  0.08966345, ...,  0.11867336,\n",
      "         0.03299351,  0.05156981]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.07448417],\n",
      "       [ 0.04563561],\n",
      "       [ 0.1716961 ],\n",
      "       [-0.14468463],\n",
      "       [-0.13791913],\n",
      "       [ 0.0884769 ],\n",
      "       [ 0.11663181],\n",
      "       [ 0.18148643],\n",
      "       [ 0.0299877 ],\n",
      "       [ 0.04319117],\n",
      "       [-0.13038933],\n",
      "       [-0.02888078],\n",
      "       [ 0.18563178],\n",
      "       [ 0.11948813],\n",
      "       [-0.23321247],\n",
      "       [-0.20174766],\n",
      "       [ 0.21053986],\n",
      "       [ 0.1958574 ],\n",
      "       [-0.07603669],\n",
      "       [ 0.0362312 ],\n",
      "       [-0.17244492],\n",
      "       [-0.08120698],\n",
      "       [-0.03655339],\n",
      "       [ 0.00188894],\n",
      "       [ 0.05916039],\n",
      "       [ 0.13850063],\n",
      "       [-0.04093556],\n",
      "       [ 0.0450119 ],\n",
      "       [-0.04566501],\n",
      "       [ 0.11705984],\n",
      "       [ 0.00745661],\n",
      "       [-0.17210343],\n",
      "       [ 0.10617235],\n",
      "       [ 0.2130438 ],\n",
      "       [ 0.06068464],\n",
      "       [-0.10347037],\n",
      "       [-0.00443077],\n",
      "       [ 0.04565045],\n",
      "       [ 0.21224432],\n",
      "       [ 0.02012143],\n",
      "       [ 0.03155404],\n",
      "       [-0.101413  ],\n",
      "       [ 0.10142848],\n",
      "       [ 0.01552797],\n",
      "       [-0.04847256],\n",
      "       [-0.07923225],\n",
      "       [-0.11243592],\n",
      "       [-0.08306053],\n",
      "       [-0.01737088],\n",
      "       [-0.17275397],\n",
      "       [ 0.10981732],\n",
      "       [ 0.05898905],\n",
      "       [ 0.15308528],\n",
      "       [-0.0148131 ],\n",
      "       [-0.15330139],\n",
      "       [ 0.01270584],\n",
      "       [ 0.02468374],\n",
      "       [-0.00606991],\n",
      "       [-0.09555326],\n",
      "       [-0.14404696],\n",
      "       [ 0.09024497],\n",
      "       [-0.03522275],\n",
      "       [ 0.12498786],\n",
      "       [ 0.17247051]], dtype=float32), array([0.], dtype=float32), array([[-0.02836804,  0.01693427, -0.10821537, ..., -0.07789561,\n",
      "        -0.13752119, -0.00897708],\n",
      "       [ 0.02672532, -0.12536503,  0.03486844, ..., -0.24656591,\n",
      "        -0.0591887 , -0.04499862],\n",
      "       [ 0.17867479, -0.06494498,  0.10147917, ...,  0.08230247,\n",
      "         0.05794229,  0.03820629],\n",
      "       ...,\n",
      "       [ 0.12941521,  0.23334073,  0.08127576, ...,  0.04828782,\n",
      "         0.02101099,  0.17997038],\n",
      "       [ 0.07332186,  0.0559225 ,  0.05269364, ...,  0.02028332,\n",
      "        -0.22732615,  0.21667843],\n",
      "       [-0.05303073, -0.0909111 , -0.02452828, ..., -0.10107087,\n",
      "        -0.03888191,  0.08460667]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.01763238,  0.05019489,  0.0721144 , ..., -0.01117073,\n",
      "        -0.15662697, -0.07524027],\n",
      "       [-0.01810573,  0.18112756,  0.1305617 , ...,  0.19732502,\n",
      "        -0.03439654, -0.12669076],\n",
      "       [ 0.03303374, -0.10925967, -0.11215632, ..., -0.1867352 ,\n",
      "         0.12795459,  0.11199141],\n",
      "       ...,\n",
      "       [ 0.06587641,  0.01187017, -0.03457369, ...,  0.03265594,\n",
      "         0.2038025 , -0.05601645],\n",
      "       [-0.00398676, -0.10320336,  0.064     , ...,  0.04473253,\n",
      "        -0.07434552, -0.1394854 ],\n",
      "       [-0.13486902, -0.02661765,  0.02061009, ...,  0.06348789,\n",
      "         0.10255968, -0.11896788]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.23512201],\n",
      "       [ 0.21690227],\n",
      "       [-0.18611321],\n",
      "       [-0.03029819],\n",
      "       [-0.15702237],\n",
      "       [ 0.09552769],\n",
      "       [-0.11261102],\n",
      "       [-0.19247904],\n",
      "       [-0.0485283 ],\n",
      "       [ 0.06208819],\n",
      "       [ 0.00817272],\n",
      "       [ 0.00126614],\n",
      "       [ 0.13721588],\n",
      "       [ 0.04450156],\n",
      "       [-0.15489578],\n",
      "       [-0.11867489],\n",
      "       [-0.21333389],\n",
      "       [-0.24683364],\n",
      "       [ 0.04756181],\n",
      "       [-0.11692391],\n",
      "       [ 0.10487717],\n",
      "       [ 0.10959804],\n",
      "       [-0.08774275],\n",
      "       [-0.02872825],\n",
      "       [-0.10249705],\n",
      "       [-0.02682585],\n",
      "       [ 0.0597748 ],\n",
      "       [-0.00377504],\n",
      "       [-0.00377211],\n",
      "       [-0.18012051],\n",
      "       [ 0.04875248],\n",
      "       [-0.15491077],\n",
      "       [ 0.16370583],\n",
      "       [ 0.12760003],\n",
      "       [-0.11590993],\n",
      "       [-0.05285713],\n",
      "       [ 0.09992619],\n",
      "       [ 0.00920157],\n",
      "       [ 0.17846987],\n",
      "       [ 0.15198438],\n",
      "       [ 0.12549083],\n",
      "       [-0.14678386],\n",
      "       [ 0.041001  ],\n",
      "       [-0.00667745],\n",
      "       [-0.08986613],\n",
      "       [ 0.11488502],\n",
      "       [-0.08430228],\n",
      "       [-0.24209462],\n",
      "       [-0.01251272],\n",
      "       [ 0.038845  ],\n",
      "       [-0.04424874],\n",
      "       [ 0.20042583],\n",
      "       [ 0.13930699],\n",
      "       [ 0.10180089],\n",
      "       [-0.01085353],\n",
      "       [-0.01423122],\n",
      "       [ 0.01613439],\n",
      "       [ 0.09105963],\n",
      "       [ 0.16696258],\n",
      "       [-0.05426235],\n",
      "       [-0.15545617],\n",
      "       [ 0.08602409],\n",
      "       [ 0.02849146],\n",
      "       [-0.0325283 ]], dtype=float32), array([0.], dtype=float32)], 'policy_2': [array([[ 0.04447156,  0.11744589,  0.12431288, ...,  0.1330728 ,\n",
      "         0.07554194, -0.20110746],\n",
      "       [-0.1924653 , -0.22636214,  0.1293942 , ..., -0.11128667,\n",
      "         0.19876155,  0.08320123],\n",
      "       [ 0.14875099, -0.16041915,  0.05819771, ..., -0.02866717,\n",
      "         0.06263208, -0.09986076],\n",
      "       ...,\n",
      "       [-0.00594363, -0.01355778,  0.15238336, ...,  0.05314755,\n",
      "         0.00371617, -0.14843395],\n",
      "       [ 0.23611125, -0.10653158,  0.1107226 , ...,  0.15184274,\n",
      "         0.13836095,  0.19891232],\n",
      "       [-0.10956191,  0.18798071, -0.03138952, ..., -0.02517909,\n",
      "        -0.23125729, -0.11179349]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.02672085,  0.21586524, -0.06209983, ...,  0.0464869 ,\n",
      "        -0.06432261,  0.1413636 ],\n",
      "       [-0.15763038,  0.10138975, -0.14993258, ...,  0.16633932,\n",
      "        -0.1587723 , -0.03418699],\n",
      "       [ 0.19268732, -0.0902175 , -0.06768209, ..., -0.09443107,\n",
      "         0.20267133, -0.03742841],\n",
      "       ...,\n",
      "       [-0.10779108, -0.10494047,  0.0183073 , ...,  0.10553439,\n",
      "        -0.02273518,  0.03088786],\n",
      "       [ 0.10636579,  0.1993063 ,  0.13655962, ..., -0.06319813,\n",
      "         0.0852582 , -0.15316725],\n",
      "       [ 0.182265  ,  0.09762944, -0.07775736, ..., -0.15201877,\n",
      "        -0.03803371, -0.00164603]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.00633344],\n",
      "       [ 0.2972998 ],\n",
      "       [-0.02942953],\n",
      "       [ 0.15607521],\n",
      "       [ 0.18575323],\n",
      "       [ 0.08203855],\n",
      "       [-0.14503115],\n",
      "       [ 0.08513194],\n",
      "       [ 0.03492755],\n",
      "       [-0.04612881],\n",
      "       [-0.20309153],\n",
      "       [-0.28925586],\n",
      "       [ 0.29244083],\n",
      "       [ 0.03835592],\n",
      "       [-0.28176147],\n",
      "       [-0.16693066],\n",
      "       [ 0.02787447],\n",
      "       [ 0.19299936],\n",
      "       [-0.29037037],\n",
      "       [-0.23610125],\n",
      "       [ 0.13152719],\n",
      "       [-0.19971779],\n",
      "       [ 0.14148566],\n",
      "       [ 0.06467676],\n",
      "       [-0.18815985],\n",
      "       [-0.12211476],\n",
      "       [-0.08685103],\n",
      "       [-0.07623637],\n",
      "       [ 0.10657635],\n",
      "       [ 0.20541269],\n",
      "       [-0.14306276],\n",
      "       [-0.04763356],\n",
      "       [ 0.29993594],\n",
      "       [ 0.01430011],\n",
      "       [ 0.22936946],\n",
      "       [-0.17615955],\n",
      "       [-0.17174228],\n",
      "       [-0.01320761],\n",
      "       [-0.28626674],\n",
      "       [-0.08227338],\n",
      "       [ 0.2636358 ],\n",
      "       [ 0.14569038],\n",
      "       [-0.05982704],\n",
      "       [-0.29374897],\n",
      "       [-0.2726953 ],\n",
      "       [-0.21433365],\n",
      "       [ 0.18530917],\n",
      "       [-0.13152336],\n",
      "       [-0.2583652 ],\n",
      "       [-0.1852108 ],\n",
      "       [-0.18317613],\n",
      "       [ 0.00158396],\n",
      "       [-0.20510903],\n",
      "       [-0.279059  ],\n",
      "       [-0.19446468],\n",
      "       [-0.04302317],\n",
      "       [ 0.25075245],\n",
      "       [-0.2447901 ],\n",
      "       [ 0.02601936],\n",
      "       [ 0.24487239],\n",
      "       [ 0.26174313],\n",
      "       [-0.02158782],\n",
      "       [ 0.19848758],\n",
      "       [-0.21490958]], dtype=float32), array([0.], dtype=float32), array([[ 0.0514746 ,  0.1610718 ,  0.03740469, ...,  0.03407577,\n",
      "         0.15805632, -0.04376504],\n",
      "       [-0.00770256,  0.20065892,  0.16722539, ...,  0.17737558,\n",
      "         0.1390912 , -0.13790783],\n",
      "       [ 0.23027325, -0.03104661, -0.23680383, ...,  0.06226557,\n",
      "         0.13549003, -0.18198562],\n",
      "       ...,\n",
      "       [-0.13432606, -0.23303826, -0.10636327, ...,  0.10076216,\n",
      "        -0.16801022, -0.014411  ],\n",
      "       [ 0.18909302,  0.06613699, -0.14462334, ..., -0.1841842 ,\n",
      "        -0.19542739, -0.2097633 ],\n",
      "       [ 0.0816876 , -0.23918115, -0.06574748, ..., -0.16306773,\n",
      "         0.19328952, -0.18386328]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.03169347, -0.0885469 ,  0.00651242, ..., -0.07763946,\n",
      "        -0.13406357,  0.1469783 ],\n",
      "       [-0.17446283, -0.20991355, -0.16611105, ..., -0.07458515,\n",
      "         0.10948981, -0.207629  ],\n",
      "       [ 0.142527  ,  0.0647112 ,  0.20330156, ..., -0.10810802,\n",
      "        -0.12913714, -0.07824196],\n",
      "       ...,\n",
      "       [-0.10410387, -0.07691246,  0.07369603, ..., -0.1036887 ,\n",
      "         0.17744352, -0.18066275],\n",
      "       [ 0.08822887,  0.18168621,  0.12062483, ..., -0.10032442,\n",
      "         0.06903212,  0.13301419],\n",
      "       [-0.14337602,  0.09714963, -0.17851937, ..., -0.07853092,\n",
      "         0.12278701,  0.05177797]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.1869724 ],\n",
      "       [ 0.26433778],\n",
      "       [-0.07862011],\n",
      "       [-0.20332152],\n",
      "       [-0.05609053],\n",
      "       [ 0.1460906 ],\n",
      "       [-0.12487532],\n",
      "       [-0.12354046],\n",
      "       [-0.26437476],\n",
      "       [ 0.07192412],\n",
      "       [-0.21522534],\n",
      "       [ 0.21002555],\n",
      "       [ 0.11924729],\n",
      "       [-0.2646128 ],\n",
      "       [ 0.05427578],\n",
      "       [ 0.15381598],\n",
      "       [ 0.26782882],\n",
      "       [ 0.16757056],\n",
      "       [-0.04214638],\n",
      "       [ 0.16458249],\n",
      "       [ 0.1863156 ],\n",
      "       [ 0.24611807],\n",
      "       [-0.09698696],\n",
      "       [-0.18347108],\n",
      "       [-0.05273679],\n",
      "       [ 0.2709158 ],\n",
      "       [-0.05839251],\n",
      "       [ 0.16839635],\n",
      "       [ 0.11051053],\n",
      "       [-0.1632287 ],\n",
      "       [ 0.10682815],\n",
      "       [-0.20902453],\n",
      "       [ 0.09251595],\n",
      "       [-0.05562079],\n",
      "       [-0.05525295],\n",
      "       [-0.0342544 ],\n",
      "       [ 0.05468136],\n",
      "       [-0.08859408],\n",
      "       [ 0.22065753],\n",
      "       [-0.06190844],\n",
      "       [ 0.08035672],\n",
      "       [ 0.04450521],\n",
      "       [ 0.08012208],\n",
      "       [-0.10362209],\n",
      "       [ 0.24494338],\n",
      "       [-0.15826389],\n",
      "       [-0.26393333],\n",
      "       [ 0.13500735],\n",
      "       [ 0.05239555],\n",
      "       [-0.24605668],\n",
      "       [-0.16207378],\n",
      "       [-0.14939988],\n",
      "       [-0.12221466],\n",
      "       [-0.17035033],\n",
      "       [-0.07494061],\n",
      "       [ 0.1743918 ],\n",
      "       [-0.1317046 ],\n",
      "       [-0.03255075],\n",
      "       [-0.13724673],\n",
      "       [ 0.00999448],\n",
      "       [ 0.06628957],\n",
      "       [-0.04272074],\n",
      "       [ 0.11550063],\n",
      "       [-0.0155842 ]], dtype=float32), array([0.], dtype=float32), array([[ 0.04447156,  0.11744589,  0.12431288, ...,  0.1330728 ,\n",
      "         0.07554194, -0.20110746],\n",
      "       [-0.1924653 , -0.22636214,  0.1293942 , ..., -0.11128667,\n",
      "         0.19876157,  0.08320123],\n",
      "       [ 0.14875099, -0.16041915,  0.05819771, ..., -0.02866717,\n",
      "         0.06263208, -0.09986076],\n",
      "       ...,\n",
      "       [-0.00594363, -0.01355779,  0.15238336, ...,  0.05314755,\n",
      "         0.00371617, -0.14843395],\n",
      "       [ 0.23611137, -0.10653158,  0.1107226 , ...,  0.15184274,\n",
      "         0.13836095,  0.19891232],\n",
      "       [-0.10956191,  0.18798071, -0.03138952, ..., -0.02517909,\n",
      "        -0.23125729, -0.11179349]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.02672085,  0.21586524, -0.06209983, ...,  0.0464869 ,\n",
      "        -0.06432261,  0.14136374],\n",
      "       [-0.15763038,  0.10138975, -0.14993258, ...,  0.16633932,\n",
      "        -0.1587723 , -0.03418699],\n",
      "       [ 0.19268732, -0.0902175 , -0.06768209, ..., -0.09443107,\n",
      "         0.20267133, -0.03742841],\n",
      "       ...,\n",
      "       [-0.10779108, -0.10494047,  0.01830731, ...,  0.10553439,\n",
      "        -0.02273519,  0.03088786],\n",
      "       [ 0.10636579,  0.1993063 ,  0.13655962, ..., -0.06319813,\n",
      "         0.0852582 , -0.15316725],\n",
      "       [ 0.182265  ,  0.09762944, -0.07775736, ..., -0.15201877,\n",
      "        -0.03803371, -0.00164603]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.00633344],\n",
      "       [ 0.2972998 ],\n",
      "       [-0.02942954],\n",
      "       [ 0.15607521],\n",
      "       [ 0.18575323],\n",
      "       [ 0.08203855],\n",
      "       [-0.14503115],\n",
      "       [ 0.08513194],\n",
      "       [ 0.03492755],\n",
      "       [-0.04612881],\n",
      "       [-0.20309153],\n",
      "       [-0.28925586],\n",
      "       [ 0.29244083],\n",
      "       [ 0.03835592],\n",
      "       [-0.28176147],\n",
      "       [-0.16693066],\n",
      "       [ 0.02787447],\n",
      "       [ 0.19299936],\n",
      "       [-0.29037037],\n",
      "       [-0.23610125],\n",
      "       [ 0.13152719],\n",
      "       [-0.19971779],\n",
      "       [ 0.14148566],\n",
      "       [ 0.06467676],\n",
      "       [-0.18815985],\n",
      "       [-0.12211476],\n",
      "       [-0.08685103],\n",
      "       [-0.07623637],\n",
      "       [ 0.10657635],\n",
      "       [ 0.20541269],\n",
      "       [-0.14306276],\n",
      "       [-0.04763356],\n",
      "       [ 0.29993594],\n",
      "       [ 0.01430011],\n",
      "       [ 0.22936946],\n",
      "       [-0.17615955],\n",
      "       [-0.17174228],\n",
      "       [-0.01320761],\n",
      "       [-0.28626674],\n",
      "       [-0.08227338],\n",
      "       [ 0.2636358 ],\n",
      "       [ 0.14569038],\n",
      "       [-0.05982704],\n",
      "       [-0.29374897],\n",
      "       [-0.2726953 ],\n",
      "       [-0.21433365],\n",
      "       [ 0.18530917],\n",
      "       [-0.13152336],\n",
      "       [-0.2583652 ],\n",
      "       [-0.1852108 ],\n",
      "       [-0.18317613],\n",
      "       [ 0.00158396],\n",
      "       [-0.20510903],\n",
      "       [-0.279059  ],\n",
      "       [-0.19446468],\n",
      "       [-0.04302317],\n",
      "       [ 0.25075245],\n",
      "       [-0.2447901 ],\n",
      "       [ 0.02601936],\n",
      "       [ 0.24487239],\n",
      "       [ 0.26174313],\n",
      "       [-0.02158782],\n",
      "       [ 0.19848758],\n",
      "       [-0.21490958]], dtype=float32), array([0.], dtype=float32), array([[ 0.0514746 ,  0.1610718 ,  0.03740469, ...,  0.03407577,\n",
      "         0.15805632, -0.04376504],\n",
      "       [-0.00770256,  0.20065892,  0.16722539, ...,  0.17737558,\n",
      "         0.1390913 , -0.13790783],\n",
      "       [ 0.23027325, -0.03104661, -0.23680383, ...,  0.06226557,\n",
      "         0.13549003, -0.18198562],\n",
      "       ...,\n",
      "       [-0.13432606, -0.23303826, -0.10636327, ...,  0.10076216,\n",
      "        -0.16801022, -0.014411  ],\n",
      "       [ 0.18909302,  0.06613701, -0.14462334, ..., -0.1841842 ,\n",
      "        -0.19542739, -0.2097633 ],\n",
      "       [ 0.0816876 , -0.23918115, -0.06574748, ..., -0.16306773,\n",
      "         0.19328952, -0.18386337]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.0316935 , -0.0885469 ,  0.00651242, ..., -0.07763946,\n",
      "        -0.13406365,  0.1469783 ],\n",
      "       [-0.17446283, -0.20991361, -0.16611105, ..., -0.07458515,\n",
      "         0.10948981, -0.207629  ],\n",
      "       [ 0.142527  ,  0.0647112 ,  0.20330156, ..., -0.10810802,\n",
      "        -0.12913714, -0.07824198],\n",
      "       ...,\n",
      "       [-0.10410387, -0.07691246,  0.07369603, ..., -0.10368872,\n",
      "         0.17744352, -0.18066275],\n",
      "       [ 0.08822887,  0.18168631,  0.12062483, ..., -0.10032442,\n",
      "         0.06903212,  0.13301419],\n",
      "       [-0.14337602,  0.09714963, -0.17851937, ..., -0.07853092,\n",
      "         0.12278701,  0.05177797]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.1869724 ],\n",
      "       [ 0.26433778],\n",
      "       [-0.07862011],\n",
      "       [-0.20332152],\n",
      "       [-0.05609053],\n",
      "       [ 0.1460906 ],\n",
      "       [-0.12487532],\n",
      "       [-0.12354046],\n",
      "       [-0.26437476],\n",
      "       [ 0.07192412],\n",
      "       [-0.21522534],\n",
      "       [ 0.21002555],\n",
      "       [ 0.11924729],\n",
      "       [-0.2646128 ],\n",
      "       [ 0.05427578],\n",
      "       [ 0.15381598],\n",
      "       [ 0.26782882],\n",
      "       [ 0.16757056],\n",
      "       [-0.04214638],\n",
      "       [ 0.16458249],\n",
      "       [ 0.1863156 ],\n",
      "       [ 0.24611807],\n",
      "       [-0.09698696],\n",
      "       [-0.18347108],\n",
      "       [-0.05273679],\n",
      "       [ 0.2709158 ],\n",
      "       [-0.05839251],\n",
      "       [ 0.16839635],\n",
      "       [ 0.11051053],\n",
      "       [-0.1632287 ],\n",
      "       [ 0.10682815],\n",
      "       [-0.20902453],\n",
      "       [ 0.09251595],\n",
      "       [-0.05562079],\n",
      "       [-0.055253  ],\n",
      "       [-0.0342544 ],\n",
      "       [ 0.05468136],\n",
      "       [-0.08859408],\n",
      "       [ 0.22065753],\n",
      "       [-0.06190844],\n",
      "       [ 0.08035672],\n",
      "       [ 0.04450521],\n",
      "       [ 0.08012208],\n",
      "       [-0.10362209],\n",
      "       [ 0.24494338],\n",
      "       [-0.15826389],\n",
      "       [-0.26393333],\n",
      "       [ 0.13500735],\n",
      "       [ 0.05239555],\n",
      "       [-0.24605668],\n",
      "       [-0.16207378],\n",
      "       [-0.14939994],\n",
      "       [-0.12221466],\n",
      "       [-0.17035033],\n",
      "       [-0.07494062],\n",
      "       [ 0.1743918 ],\n",
      "       [-0.1317046 ],\n",
      "       [-0.03255075],\n",
      "       [-0.13724673],\n",
      "       [ 0.00999448],\n",
      "       [ 0.06628957],\n",
      "       [-0.04272074],\n",
      "       [ 0.11550063],\n",
      "       [-0.0155842 ]], dtype=float32), array([0.], dtype=float32)]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> dir(agent)\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_allow_unknown_configs', '_allow_unknown_subkeys', '_before_evaluate', '_default_config', '_env_id', '_episodes_total', '_evaluate', '_experiment_id', '_export_model', '_has_policy_optimizer', '_init', '_iteration', '_iterations_since_restore', '_local_ip', '_log_result', '_logdir', '_make_workers', '_monitor', '_name', '_policy', '_register_if_needed', '_restore', '_restored', '_result_logger', '_save', '_setup', '_stop', '_time_since_restore', '_time_total', '_timesteps_since_restore', '_timesteps_total', '_train', '_try_recover', '_validate_config', 'collect_metrics', 'compute_action', 'config', 'current_ip', 'default_resource_request', 'env_creator', 'export_model', 'export_policy_checkpoint', 'export_policy_model', 'get_config', 'get_policy', 'get_weights', 'global_vars', 'iteration', 'logdir', 'optimizer', 'raw_user_config', 'reset_config', 'resource_help', 'restore', 'restore_from_object', 'save', 'save_to_object', 'set_weights', 'state', 'stop', 'train', 'with_updates', 'workers']\n",
      "ipdb> agent\n",
      "<ray.rllib.agents.trainer_template.MADDPG object at 0x7f3b48de1400>\n",
      "ipdb> agent.__dict__\n",
      "{'global_vars': {'timestep': 0}, '_env_id': 'Bayesian1Env-v0', '_experiment_id': 'b0ae3c125bb04d3a920089f2a3191a27', 'config': {'num_workers': 0, 'num_envs_per_worker': 1, 'sample_batch_size': 100, 'batch_mode': 'truncate_episodes', 'num_gpus': 0, 'train_batch_size': 1024, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_action_dist': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.999, 'horizon': 500, 'soft_horizon': False, 'no_done_at_end': True, 'env_config': {'flow_params': '{\\n    \"env\": {\\n        \"additional_params\": {\\n            \"max_accel\": 2.6,\\n            \"max_decel\": 4.5,\\n            \"max_num_objects\": 3,\\n            \"qmix\": true,\\n            \"search_radius\": 50,\\n            \"target_velocity\": 25\\n        },\\n        \"clip_actions\": true,\\n        \"evaluate\": false,\\n        \"horizon\": 500,\\n        \"sims_per_step\": 1,\\n        \"warmup_steps\": 0\\n    },\\n    \"env_name\": \"flow.envs.multiagent.bayesian_1_env.Bayesian1Env\",\\n    \"exp_tag\": \"bayesian_1_env\",\\n    \"initial\": {\\n        \"additional_params\": {},\\n        \"bunching\": 0,\\n        \"edges_distribution\": \"all\",\\n        \"lanes_distribution\": Infinity,\\n        \"min_gap\": 0,\\n        \"perturbation\": 0.0,\\n        \"shuffle\": false,\\n        \"sidewalks\": true,\\n        \"spacing\": \"custom\",\\n        \"x0\": 0\\n    },\\n    \"net\": {\\n        \"additional_params\": {\\n            \"grid_array\": {\\n                \"cars_bot\": 1,\\n                \"cars_left\": 0,\\n                \"cars_right\": 1,\\n                \"cars_top\": 1,\\n                \"col_num\": 1,\\n                \"inner_length\": 50,\\n                \"row_num\": 1\\n            },\\n            \"horizontal_lanes\": 1,\\n            \"randomize_routes\": true,\\n            \"speed_limit\": 35,\\n            \"vertical_lanes\": 1\\n        },\\n        \"inflows\": {\\n            \"_InFlows__flows\": []\\n        },\\n        \"osm_path\": null,\\n        \"template\": null\\n    },\\n    \"network\": \"flow.networks.bayesian_1.Bayesian1Network\",\\n    \"ped\": {\\n        \"_PedestrianParams__pedestrians\": {},\\n        \"ids\": [\\n            \"ped_0\"\\n        ],\\n        \"num_pedestrians\": 1,\\n        \"params\": {\\n            \"ped_0\": {\\n                \"arrivalPos\": \"43\",\\n                \"depart\": \"0.00\",\\n                \"departPos\": \"5\",\\n                \"from\": \"(1.1)--(2.1)\",\\n                \"id\": \"ped_0\",\\n                \"to\": \"(2.1)--(1.1)\"\\n            }\\n        }\\n    },\\n    \"sim\": {\\n        \"color_vehicles\": true,\\n        \"emission_path\": null,\\n        \"lateral_resolution\": null,\\n        \"no_step_log\": true,\\n        \"num_clients\": 1,\\n        \"overtake_right\": false,\\n        \"port\": null,\\n        \"print_warnings\": true,\\n        \"pxpm\": 2,\\n        \"render\": false,\\n        \"restart_instance\": true,\\n        \"rllib_training\": false,\\n        \"save_render\": false,\\n        \"seed\": null,\\n        \"show_radius\": false,\\n        \"sight_radius\": 25,\\n        \"sim_step\": 0.1,\\n        \"teleport_time\": -1\\n    },\\n    \"simulator\": \"traci\",\\n    \"veh\": [\\n        {\\n            \"acceleration_controller\": [\\n                \"SimCarFollowingController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 2.6,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 7.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 25\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 2,\\n            \"routing_controller\": [\\n                \"GridRouter\",\\n                {}\\n            ],\\n            \"veh_id\": \"human\"\\n        },\\n        {\\n            \"acceleration_controller\": [\\n                \"RLController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 2.6,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 4.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 25\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 1,\\n            \"routing_controller\": [\\n                \"GridRouter\",\\n                {}\\n            ],\\n            \"veh_id\": \"rl\"\\n        }\\n    ]\\n}', 'run': 'contrib/MADDPG'}, 'env': 'Bayesian1Env-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.0001, 'monitor': False, 'log_level': 'WARN', 'callbacks': {'on_episode_start': '<function on_episode_start at 0x7f72ad2de9d8>', 'on_episode_step': '<function on_episode_step at 0x7f72ad2dea60>', 'on_episode_end': '<function on_episode_end at 0x7f72ad2deae8>', 'on_sample_end': None, 'on_train_result': None, 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {}, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'policy_0': (None, Box(25,), Box(1,), {'agent_id': 0, 'use_local_critic': True, 'obs_space_dict': {0: Box(25,), 1: Box(25,), 2: Box(25,)}, 'act_space_dict': {0: Box(1,), 1: Box(1,), 2: Box(1,)}}), 'policy_1': (None, Box(25,), Box(1,), {'agent_id': 1, 'use_local_critic': True, 'obs_space_dict': {0: Box(25,), 1: Box(25,), 2: Box(25,)}, 'act_space_dict': {0: Box(1,), 1: Box(1,), 2: Box(1,)}}), 'policy_2': (None, Box(25,), Box(1,), {'agent_id': 2, 'use_local_critic': True, 'obs_space_dict': {0: Box(25,), 1: Box(25,), 2: Box(25,)}, 'act_space_dict': {0: Box(1,), 1: Box(1,), 2: Box(1,)}})}, 'policy_mapping_fn': <function setup_exps_QMIX.<locals>.<lambda> at 0x7f3b2c1bb510>, 'policies_to_train': None}, 'agent_id': None, 'use_local_critic': False, 'use_state_preprocessor': False, 'actor_hiddens': [64, 64], 'actor_hidden_activation': 'relu', 'critic_hiddens': [64, 64], 'critic_hidden_activation': 'relu', 'n_step': 1, 'good_policy': 'maddpg', 'adv_policy': 'maddpg', 'buffer_size': 1000000, 'critic_lr': 0.01, 'actor_lr': 0.01, 'target_network_update_freq': 0, 'tau': 0.01, 'actor_feature_reg': 0.001, 'grad_norm_clipping': 0.5, 'learning_starts': 25600}, '_result_logger': <ray.tune.logger.UnifiedLogger object at 0x7f3b48de1fd0>, '_logdir': '/home/thankyou-always/ray_results/MADDPG_Bayesian1Env-v0_2020-04-07_20-38-13rz7m6er3', '_iteration': 100, '_time_total': 98.60757613182068, '_timesteps_total': 10000, '_episodes_total': 22, '_time_since_restore': 0.0, '_timesteps_since_restore': 0, '_iterations_since_restore': 0, '_restored': True, 'env_creator': <function make_create_env.<locals>.create_env at 0x7f3b2c1bbae8>, 'raw_user_config': {'actor_feature_reg': 0.001, 'actor_hidden_activation': 'relu', 'actor_hiddens': [64, 64], 'actor_lr': 0.01, 'adv_policy': 'maddpg', 'agent_id': None, 'batch_mode': 'truncate_episodes', 'buffer_size': 1000000, 'callbacks': {'on_episode_end': '<function on_episode_end at 0x7f72ad2deae8>', 'on_episode_start': '<function on_episode_start at 0x7f72ad2de9d8>', 'on_episode_step': '<function on_episode_step at 0x7f72ad2dea60>'}, 'clip_actions': True, 'clip_rewards': None, 'collect_metrics_timeout': 180, 'compress_observations': False, 'critic_hidden_activation': 'relu', 'critic_hiddens': [64, 64], 'critic_lr': 0.01, 'custom_resources_per_worker': {}, 'eager': False, 'eager_tracing': False, 'env': 'Bayesian1Env-v0', 'env_config': {'flow_params': '{\\n    \"env\": {\\n        \"additional_params\": {\\n            \"max_accel\": 2.6,\\n            \"max_decel\": 4.5,\\n            \"max_num_objects\": 3,\\n            \"qmix\": true,\\n            \"search_radius\": 50,\\n            \"target_velocity\": 25\\n        },\\n        \"clip_actions\": true,\\n        \"evaluate\": false,\\n        \"horizon\": 500,\\n        \"sims_per_step\": 1,\\n        \"warmup_steps\": 0\\n    },\\n    \"env_name\": \"flow.envs.multiagent.bayesian_1_env.Bayesian1Env\",\\n    \"exp_tag\": \"bayesian_1_env\",\\n    \"initial\": {\\n        \"additional_params\": {},\\n        \"bunching\": 0,\\n        \"edges_distribution\": \"all\",\\n        \"lanes_distribution\": Infinity,\\n        \"min_gap\": 0,\\n        \"perturbation\": 0.0,\\n        \"shuffle\": false,\\n        \"sidewalks\": true,\\n        \"spacing\": \"custom\",\\n        \"x0\": 0\\n    },\\n    \"net\": {\\n        \"additional_params\": {\\n            \"grid_array\": {\\n                \"cars_bot\": 1,\\n                \"cars_left\": 0,\\n                \"cars_right\": 1,\\n                \"cars_top\": 1,\\n                \"col_num\": 1,\\n                \"inner_length\": 50,\\n                \"row_num\": 1\\n            },\\n            \"horizontal_lanes\": 1,\\n            \"randomize_routes\": true,\\n            \"speed_limit\": 35,\\n            \"vertical_lanes\": 1\\n        },\\n        \"inflows\": {\\n            \"_InFlows__flows\": []\\n        },\\n        \"osm_path\": null,\\n        \"template\": null\\n    },\\n    \"network\": \"flow.networks.bayesian_1.Bayesian1Network\",\\n    \"ped\": {\\n        \"_PedestrianParams__pedestrians\": {},\\n        \"ids\": [\\n            \"ped_0\"\\n        ],\\n        \"num_pedestrians\": 1,\\n        \"params\": {\\n            \"ped_0\": {\\n                \"arrivalPos\": \"43\",\\n                \"depart\": \"0.00\",\\n                \"departPos\": \"5\",\\n                \"from\": \"(1.1)--(2.1)\",\\n                \"id\": \"ped_0\",\\n                \"to\": \"(2.1)--(1.1)\"\\n            }\\n        }\\n    },\\n    \"sim\": {\\n        \"color_vehicles\": true,\\n        \"emission_path\": null,\\n        \"lateral_resolution\": null,\\n        \"no_step_log\": true,\\n        \"num_clients\": 1,\\n        \"overtake_right\": false,\\n        \"port\": null,\\n        \"print_warnings\": true,\\n        \"pxpm\": 2,\\n        \"render\": false,\\n        \"restart_instance\": true,\\n        \"rllib_training\": false,\\n        \"save_render\": false,\\n        \"seed\": null,\\n        \"show_radius\": false,\\n        \"sight_radius\": 25,\\n        \"sim_step\": 0.1,\\n        \"teleport_time\": -1\\n    },\\n    \"simulator\": \"traci\",\\n    \"veh\": [\\n        {\\n            \"acceleration_controller\": [\\n                \"SimCarFollowingController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 2.6,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 7.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 25\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 2,\\n            \"routing_controller\": [\\n                \"GridRouter\",\\n                {}\\n            ],\\n            \"veh_id\": \"human\"\\n        },\\n        {\\n            \"acceleration_controller\": [\\n                \"RLController\",\\n                {}\\n            ],\\n            \"car_following_params\": {\\n                \"controller_params\": {\\n                    \"accel\": 2.6,\\n                    \"carFollowModel\": \"IDM\",\\n                    \"decel\": 4.5,\\n                    \"impatience\": 0.5,\\n                    \"maxSpeed\": 30,\\n                    \"minGap\": 2.5,\\n                    \"sigma\": 0.5,\\n                    \"speedDev\": 0.1,\\n                    \"speedFactor\": 1.0,\\n                    \"tau\": 1.0\\n                },\\n                \"speed_mode\": 25\\n            },\\n            \"initial_speed\": 0,\\n            \"lane_change_controller\": [\\n                \"SimLaneChangeController\",\\n                {}\\n            ],\\n            \"lane_change_params\": {\\n                \"controller_params\": {\\n                    \"laneChangeModel\": \"LC2013\",\\n                    \"lcCooperative\": \"1.0\",\\n                    \"lcKeepRight\": \"1.0\",\\n                    \"lcSpeedGain\": \"1.0\",\\n                    \"lcStrategic\": \"1.0\"\\n                },\\n                \"lane_change_mode\": 512\\n            },\\n            \"num_vehicles\": 1,\\n            \"routing_controller\": [\\n                \"GridRouter\",\\n                {}\\n            ],\\n            \"veh_id\": \"rl\"\\n        }\\n    ]\\n}', 'run': 'contrib/MADDPG'}, 'evaluation_config': {}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'gamma': 0.999, 'good_policy': 'maddpg', 'grad_norm_clipping': 0.5, 'horizon': 500, 'ignore_worker_failures': False, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'learning_starts': 25600, 'local_tf_session_args': {'inter_op_parallelism_threads': 8, 'intra_op_parallelism_threads': 8}, 'log_level': 'WARN', 'log_sys_usage': True, 'lr': 0.0001, 'memory': 0, 'memory_per_worker': 0, 'metrics_smoothing_episodes': 100, 'min_iter_time_s': 0, 'model': {'conv_activation': 'relu', 'conv_filters': None, 'custom_action_dist': None, 'custom_model': None, 'custom_options': {}, 'custom_preprocessor': None, 'dim': 84, 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'framestack': True, 'free_log_std': False, 'grayscale': False, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'max_seq_len': 20, 'no_final_linear': False, 'state_shape': None, 'use_lstm': False, 'vf_share_layers': True, 'zero_mean': True}, 'monitor': False, 'multiagent': {'policies': {'policy_0': (None, Box(25,), Box(1,), {'agent_id': 0, 'use_local_critic': True, 'obs_space_dict': {0: Box(25,), 1: Box(25,), 2: Box(25,)}, 'act_space_dict': {0: Box(1,), 1: Box(1,), 2: Box(1,)}}), 'policy_1': (None, Box(25,), Box(1,), {'agent_id': 1, 'use_local_critic': True, 'obs_space_dict': {0: Box(25,), 1: Box(25,), 2: Box(25,)}, 'act_space_dict': {0: Box(1,), 1: Box(1,), 2: Box(1,)}}), 'policy_2': (None, Box(25,), Box(1,), {'agent_id': 2, 'use_local_critic': True, 'obs_space_dict': {0: Box(25,), 1: Box(25,), 2: Box(25,)}, 'act_space_dict': {0: Box(1,), 1: Box(1,), 2: Box(1,)}})}, 'policy_mapping_fn': <function setup_exps_QMIX.<locals>.<lambda> at 0x7f3b2c1bb510>}, 'n_step': 1, 'no_done_at_end': True, 'no_eager_on_workers': False, 'num_cpus_for_driver': 1, 'num_cpus_per_worker': 1, 'num_envs_per_worker': 1, 'num_gpus': 0, 'num_gpus_per_worker': 0, 'num_workers': 0, 'object_store_memory': 0, 'object_store_memory_per_worker': 0, 'observation_filter': 'NoFilter', 'optimizer': {}, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'postprocess_inputs': False, 'preprocessor_pref': 'deepmind', 'remote_env_batch_wait_ms': 0, 'remote_worker_envs': False, 'sample_async': False, 'sample_batch_size': 100, 'seed': None, 'shuffle_buffer_size': 0, 'soft_horizon': False, 'synchronize_filters': True, 'target_network_update_freq': 0, 'tau': 0.01, 'tf_session_args': {'allow_soft_placement': True, 'device_count': {'CPU': 1}, 'gpu_options': {'allow_growth': True}, 'inter_op_parallelism_threads': 2, 'intra_op_parallelism_threads': 2, 'log_device_placement': False}, 'timesteps_per_iteration': 0, 'train_batch_size': 1024, 'use_local_critic': False, 'use_state_preprocessor': False}, 'state': {'last_target_update_ts': 10000, 'num_target_updates': 100}, 'workers': <ray.rllib.evaluation.worker_set.WorkerSet object at 0x7f3b48e05400>, 'optimizer': <ray.rllib.optimizers.sync_replay_optimizer.SyncReplayOptimizer object at 0x7f3a144d2668>, '_local_ip': '10.225.142.127', '_monitor': <UtilMonitor(Thread-5, started daemon 139885910406912)>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> agent.get_policy()\n",
      "ipdb> agent.get_policy\n",
      "<bound method Trainer.get_policy of <ray.rllib.agents.trainer_template.MADDPG object at 0x7f3b48de1400>>\n",
      "ipdb> inspect.signature(agent.get_policy)\n",
      "<Signature (policy_id='default_policy')>\n",
      "ipdb> agent.get_policy('policy_0')\n",
      "<ray.rllib.contrib.maddpg.maddpg_policy.MADDPGTFPolicy object at 0x7f3b48e05860>\n",
      "ipdb> dir(agent.get_policy('policy_0'))\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_action_logp', '_action_prob', '_apply_op', '_batch_divisibility_req', '_build_actor_network', '_build_apply_gradients', '_build_compute_actions', '_build_compute_gradients', '_build_critic_network', '_build_learn_on_batch', '_build_signature_def', '_debug_vars', '_extra_input_signature_def', '_extra_output_signature_def', '_get_grad_and_stats_fetches', '_get_is_training_placeholder', '_get_loss_inputs_dict', '_grads', '_grads_and_vars', '_initialize_loss', '_is_training', '_loss', '_loss_input_dict', '_loss_inputs', '_max_seq_len', '_obs_input', '_optimizer', '_prev_action_input', '_prev_reward_input', '_sampler', '_seq_lens', '_sess', '_state_inputs', '_state_outputs', '_stats_fetches', '_update_ops', '_variables', 'action_space', 'apply_gradients', 'build_apply_op', 'compute_actions', 'compute_gradients', 'compute_single_action', 'config', 'copy', 'export_checkpoint', 'export_model', 'extra_compute_action_feed_dict', 'extra_compute_action_fetches', 'extra_compute_grad_feed_dict', 'extra_compute_grad_fetches', 'get_done_from_info', 'get_initial_state', 'get_placeholder', 'get_session', 'get_state', 'get_weights', 'global_step', 'gradients', 'gvs', 'learn_on_batch', 'loss_initialized', 'losses', 'model', 'new_obs_ph', 'observation_space', 'on_global_var_update', 'optimizer', 'optimizers', 'postprocess_trajectory', 'sess', 'set_state', 'set_weights', 'target_act_sampler', 'tau', 'update_target', 'update_target_vars', 'update_vars', 'variables', 'vars', 'vars_ph']\n",
      "ipdb> tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
      "*** NameError: name 'tf' is not defined\n",
      "ipdb> import tensorflow as tf\n",
      "ipdb> tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
      "*** AttributeError: module 'tensorflow' has no attribute 'get_collection'\n",
      "ipdb> globals()\n",
      "{'__name__': '__main__', '__doc__': \"Use Nick's PPO trained policy to perform inference on whether there is a pedestrian or not\", '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', '\"\"\"Use Nick\\'s PPO trained policy to perform inference on whether there is a pedestrian or not\"\"\"\\n\\nimport argparse\\nfrom datetime import datetime\\nimport gym\\nimport os\\nimport sys\\nimport time\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport PyQt5\\n\\nimport ray\\ntry:\\n    from ray.rllib.agents.agent import get_agent_class\\nexcept ImportError:\\n    from ray.rllib.agents.registry import get_agent_class\\nfrom ray.tune.registry import register_env\\n\\nfrom flow.core.util import emission_to_csv\\nfrom flow.utils.registry import make_create_env\\nfrom flow.utils.rllib import get_flow_params\\nfrom flow.utils.rllib import get_rllib_config\\nfrom flow.utils.rllib import get_rllib_pkl\\n\\nfrom examples.rllib.multiagent_exps.test_predictor.pedestrian_policy_1 import create_env, create_agent\\nfrom examples.rllib.multiagent_exps.bayesian_1_env import make_flow_params as bayesian_0_flow_params\\n\\nEXAMPLE_USAGE = \"\"\"\\nexample usage:\\n    python ./visualizer_rllib.py /ray_results/experiment_dir/result_dir 1\\nHere the arguments are:\\n1 - the path to the simulation results\\n2 - the number of the checkpoint\\n\"\"\"', 'def run_env(env, agent, config, flow_params):\\n    \"\"\"Run the simulation and control the rl car using the trained policy. \\n    \\n    observation[4:10] = ped_param\\n    \\n    The six binary grids are at indices 4 to 9 inclusive\\n    \"\"\"\\n    # set up relevant policy and env\\n    if config.get(\\'multiagent\\', {}).get(\\'policies\\', None):\\n        multiagent = True\\n        rets = {}\\n        # map the agent id to its policy\\n        policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\']\\n        \\n#         policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\'].func\\n        for key in config[\\'multiagent\\'][\\'policies\\'].keys():\\n            rets[key] = []\\n    else:\\n        multiagent = False\\n        rets = []\\n\\n    if config[\\'model\\'][\\'use_lstm\\']:\\n        use_lstm = True\\n        if multiagent:\\n            state_init = {}\\n            # map the agent id to its policy\\n#             policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\'].func\\n\\n            policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\']\\n            size = config[\\'model\\'][\\'lstm_cell_size\\']\\n            for key in config[\\'multiagent\\'][\\'policies\\'].keys():\\n                state_init[key] = [np.zeros(size, np.float32),\\n                                   np.zeros(size, np.float32)]\\n        else:\\n            state_init = [\\n                np.zeros(config[\\'model\\'][\\'lstm_cell_size\\'], np.float32),\\n                np.zeros(config[\\'model\\'][\\'lstm_cell_size\\'], np.float32)\\n            ]\\n    else:\\n        use_lstm = False\\n\\n    env.restart_simulation(\\n        sim_params=flow_params[\\'sim\\'], render=flow_params[\\'sim\\'].render)\\n\\n    \\n    # Define variables to collect probability data. Each variable is dict\\n    # 2^6 keys (or, 2^5 values for p(o_i = b_i)) and values will be a list of numbers\\n    # TODO(KL) HARD CODED is_ped_visible is the 5th item in the state vector\\n\\n    ped_idx_lst = [4, 5, 6, 7, 8, 9]\\n    ped_front = ped_idx_lst[0]\\n    ped_back = ped_idx_lst[-1]\\n    \\n    binary_observations = True\\n    \\n    \\n    # dict for f(a | o_i = b_i), where the key is bitstring s e.g.s = \"010011\", \\n    # where s[i] corresponds to the value of b_i. cond_pdf_a_on_joint_ped[\"010011\"] = list()\\n    \\n    # 1\\n    # IF binary_observations == True:\\n    cond_pdf_action_on_joint_ped_dct = make_dct_of_lsts(num_digits=len(ped_idx_lst), vals_per_dig=2)\\n    \\n    # 2\\n    # updated Pr(o_1 = b_1, ..., o_6 = b_6) for i = 1, ..., 6 and b_i = 0, 1\\n    updated_joint_prior_prob_ped_lst = initial_prior_probs(num_digits=6, vals_per_dig=2)\\n    # fixed Pr(o_1 = b_1, ..., o_6 = b_6) for i = 1, ..., 6 and b_i = 0, 1\\n    fixed_joint_prior_prob_ped_lst = initial_prior_probs(num_digits=6, vals_per_dig=2)\\n    \\n    # 3\\n    # updated joint cond_densities lst f(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_densities_updated_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n    # fixed joint cond_densities lst f(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_densities_fixed_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n\\n    # 4\\n    # updated joint cond_probs lst p(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_probs_updated_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n    # fixed joint cond_probs lst p(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_probs_fixed_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n    \\n    # 5\\n    # updated single cond_probs lst p(o_i = b_i | a)\\n    single_cond_probs_updated_priors_lst = make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2)\\n    # fixed single cond_probs lst p(o_i = b_i | a)\\n    single_cond_probs_fixed_priors_lst = make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2)\\n    \\n    # 6\\n    # this will be the only variable that\\'s a dict of lists\\n    visible_pedestrian_dct = {i : [] for i in range(1,7)}\\n\\n    # each element in this list corresponds to a key \\n    len_6_bitstring_lst = make_permutations(num_digits=6, vals_per_dig=2)\\n    for i in range(args.num_rollouts):\\n        state = env.reset()\\n        # divide by 5 to get \"time\" in the simulation\\n        for _ in range(1000):\\n            vehicles = env.unwrapped.k.vehicle\\n            pedestrian = env.unwrapped.k.pedestrian\\n\\n            if multiagent:\\n                action, logits = {}, {}\\n                for agent_id in state.keys():\\n                    if \\'QMIX\\' == \\'QMIX\\':\\n                        agent_id += 1\\n                    if use_lstm:\\n                        action[agent_id], state_init[agent_id], logits = \\\\\\n                            agent.compute_action(state[agent_id], state=state_init[agent_id], policy_id=policy_map_fn(agent_id))\\n                    else:\\n                        s_all = state[agent_id]\\n                        # get ped visibility state array of length 6 from the rl car\\'s POV\\n                        s_ped = s_all[ped_idx_lst]\\n                        \\n                        # update the visible_pedestrian dict\\n                        for idx, val in enumerate(s_ped, 1):\\n                            visible_pedestrian_dct[idx] = visible_pedestrian_dct[idx] + [val]\\n                        print(len(state[agent_id]))\\n                        # compute the actual action taken by the rl car\\n                        action[agent_id], _, logit_actual = agent.compute_action(state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)    \\n                        action_ = action[agent_id][0]\\n                        \\n                        # store variable for the sum of densities conditioned on the actual action\\n                        cond_density_sum_updated = 0\\n                        cond_density_sum_fixed = 0\\n\\n                        # compute joint conditional densities for all possible permutations of ped visibility (ignore which permutation corresponds to the actual permutation for now)\\n                        for obs_comb in len_6_bitstring_lst:\\n                            s_all_modified = np.copy(s_all)\\n                            s_all_modified[ped_front : ped_back + 1] = list(obs_comb)\\n                            print(f\\'{s_all_modified[ped_front : ped_back + 1]} is list of length 6, {list(obs_comb)} has length 6\\')\\n                            _, _, logit = agent.compute_action(state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)\\n                            # where in the docs did we figure out the logits were ln of sigma??\\n                            import ipdb;ipdb.set_trace()\\n                            mu, ln_sigma = logit[\\'behaviour_logits\\']\\n                            sigma = np.exp(ln_sigma)\\n                            \\n                            # 1\\n                            # compute the conditional a_given_obs pdfs, i.e. f(a | o_1 = b_1, ..., o_6 = b_6)\\n                            cond_a_given_joint_o_pdf = accel_pdf(mu, sigma, action_)\\n                            # append new result to lists\\n                            cond_pdf_action_on_joint_ped_dct[obs_comb] = cond_pdf_action_on_joint_ped_dct[obs_comb] + [cond_a_given_joint_o_pdf]\\n                            \\n                            # 2\\n                            # Get the updated and fixed priors\\n                            updated_prior = updated_joint_prior_prob_ped_lst[obs_comb][-1]\\n                            fixed_prior = fixed_joint_prior_prob_ped_lst[obs_comb][-1]\\n\\n                            # 3\\n                            # compute the joint conditional obs_given_a pdfs, i.e. f(o_1 = b_1, ..., o_6 = b_6 | a)\\n                            cond_joint_o_given_a_pdf_updated = cond_a_given_joint_o_pdf * updated_prior\\n                            cond_joint_o_given_a_pdf_fixed = cond_a_given_joint_o_pdf * fixed_prior\\n                            # append new result to lists\\n                            joint_cond_densities_updated_priors_dct[obs_comb] = joint_cond_densities_updated_priors_dct[obs_comb] + [cond_joint_o_given_a_pdf_updated]\\n                            joint_cond_densities_fixed_priors_dct[obs_comb] = joint_cond_densities_updated_priors_dct[obs_comb] + [cond_joint_o_given_a_pdf_fixed]\\n\\n                            cond_density_sum_updated += cond_joint_o_given_a_pdf_updated\\n                            cond_density_sum_fixed += cond_joint_o_given_a_pdf_fixed\\n                            \\n                        # 4\\n                        # compute Pr(o_1 = b_1, ... o_6 = b_6 | a)\\n                        for obs_comb_ in len_6_bitstring_lst:\\n                            cond_joint_o_given_a_prob_updated = joint_cond_densities_updated_priors_dct[obs_comb_][-1] / cond_density_sum_updated\\n                            cond_joint_o_given_a_prob_fixed = joint_cond_densities_fixed_priors_dct[obs_comb_][-1] / cond_density_sum_fixed\\n                            # append to lists\\n                            joint_cond_probs_updated_priors_dct[obs_comb_] = joint_cond_probs_updated_priors_dct[obs_comb_] + [cond_joint_o_given_a_prob_updated]\\n                            joint_cond_probs_fixed_priors_dct[obs_comb_] = joint_cond_probs_fixed_priors_dct[obs_comb_] + [cond_joint_o_given_a_prob_fixed]\\n                        \\n                            # 2\\n                            # Update the updated and fixed priors lists\\n                            updated_joint_prior_prob_ped_lst[obs_comb_] = updated_joint_prior_prob_ped_lst[obs_comb_] + [cond_joint_o_given_a_prob_updated]\\n                            fixed_joint_prior_prob_ped_lst[obs_comb_] = fixed_joint_prior_prob_ped_lst[obs_comb_] + [cond_joint_o_given_a_prob_fixed]\\n                        \\n                        # 5\\n                        # compute Pr(o_i = b_i | a) for all i = 1, ..., 6\\n                        for grid_idx in range(1, 7):\\n                            for val in range(0, 2):\\n                                # compute the marginalization (4) - need to get the relevant ped observation combinations\\n                                single_cond_o_given_a_prob_updated = 0\\n                                single_cond_o_given_a_prob_fixed = 0\\n\\n                                for key in get_ped_possiblities(grid_idx, val):\\n                                    single_cond_o_given_a_prob_updated += joint_cond_probs_updated_priors_dct[key]\\n                                    single_cond_o_given_a_prob_fixed += joint_cond_probs_fixed_priors_dct[key]\\n                                \\n                                single_cond_prob_str = single_cond_prob_to_str(grid_idx, val)\\n                                # append to list\\n                                single_cond_probs_updated_priors_lst[single_cond_prob_str] = single_cond_probs_updated_priors_lst[single_cond_prob_str] + [single_cond_o_given_a_prob_updated]\\n                                single_cond_probs_fixed_priors_lst[single_cond_prob_str] = single_cond_probs_fixed_priors_lst[single_cond_prob_str] + [single_cond_o_given_a_prob_fixed]\\n                                \\n\\n            else:\\n                action = agent.compute_action(state)\\n            state, reward, done, _ = env.step(action)\\n\\n            if multiagent and done[\\'__all__\\']:\\n                print(_,1)\\n                break\\n            if not multiagent and done:\\n                print(_,1)\\n\\n                break    \\n            state, reward, done, _ = env.step(action)   \\n\\n        visible_ped_lsts = [visible_pedestrian_dct[i] for i in range(1, 7)]\\n        legends = [f\\'ped in grid {i}\\' for i in range(1, 7)]\\n\\n        for grid_idx in range(1, 7):\\n            for val in range(0, 2):\\n                single_cond_prob_str = single_cond_prob_to_str(grid_idx, val)\\n                a = single_cond_probs_updated_priors_lst[single_cond_prob_str]\\n                b = single_cond_probs_fixed_priors_lst[single_cond_prob_str]\\n\\n                plot_2_lines(a, b)\\n#         plot_lines(visible_ped_lsts, legends)\\n#         plot_2_lines(probs_ped_given_action_updated_priors, probs_no_ped_given_action_updated_priors, [\\'Pr(ped | action) using updated priors\\', \\'Pr(no_ped | action) using updated priors\\'], viewable_ped=visible_pedestrian[4])\\n#         plot_2_lines(probs_ped_given_action_fixed_priors, probs_no_ped_given_action_fixed_priors, [\\'Pr(ped | action) using fixed priors of Pr(ped) = 0.5\\', \\'Pr(no_ped | action) using fixed priors of Pr(ped) = 0.5\\'], viewable_ped=visible_pedestrian[5])\\n#         plot_2_lines(probs_action_given_ped, probs_action_given_no_ped, [\\'Pr(action | ped)\\', \\'Pr(action | no_ped)\\'], viewable_ped=visible_pedestrian[6])\\n\\n        ', 'def single_cond_prob_to_str(grid_idx, val, num_indices = 6):\\n    \"\"\"Generate the string representing the probability:\\n    \\n    Pr(o_i = val)\\n    \\n    ex:\\n    For Pr(o_2 = 1), we\\'d have the string \\'21\\'\\n    NB we\\'re 1-indexing here\\n    \"\"\"\\n    assert grid_idx >= 1 and grid_idx <= num_indices\\n    return str(grid_idx) + str(val)\\n\\ndef get_ped_possiblities(grid_idx, val, output_len=6):\\n    \"\"\"Give the single probability we want, compute the list of all bitstrings we need to perform the marginalization.\\n    \\n    ex:\\n    3:0 means we want p(o_3 = 0 | a)\\n    Therefore, we can get the list of all possible length 5 bitstrings, and stitch \\'0\\' in the correct place.\\n    \\n    \\n    @Return\\n    NB 1-indexing here\\n    bitstring of length 6\\n    \"\"\"\\n    assert grid_idx >= 1 and grid_idx <= output_len\\n    res_lst = make_permutations(output_len - 1, 2)\\n    for perm in res_lst:\\n\\n        print(str(perm[:grid_idx - 1:] + str(val) + perm[grid_idx - 1:]))\\n\\ndef initial_prior_probs(num_digits=6, vals_per_dig=2):\\n    \"\"\"Returns a dict with values of all permutations of bitstrings of length num_digits. \\n    Each digit can take a value from 0 to (vals_per_dig - 1)\"\"\"\\n    uniform_prob = 1 / (vals_per_dig ** num_digits)\\n    res = make_dct_of_lsts(num_digits, vals_per_dig)\\n    for key in res.keys():\\n        res[key] = res[key] + [uniform_prob]\\n    return res\\n\\ndef make_dct_of_lsts(num_digits=6, vals_per_dig=2):\\n    \"\"\"Return a dict with keys of bitstrings and values as empty lists. \\n    Hardcoded for binary vals per var.\"\"\"\\n    res = {}\\n    lst_of_bitstrings = make_permutations(num_digits, vals_per_dig)\\n        \\n    return {str_ : [] for str_ in lst_of_bitstrings}\\n\\ndef make_permutations(num_digits, vals_per_dig=2):\\n    \"\"\"Make all permutations for a bit string of length num_digits\\n    and vals_per_dig values per digit. Hardcoded for work for binary vals per var\"\"\"\\n    if num_digits == 1:\\n        return [str(i) for i in range(vals_per_dig)]\\n    else:\\n        small_perms = make_permutations(num_digits - 1, vals_per_dig)\\n        # hardcoded for work for binary vals per var\\n        return [\\'0\\' + bit_str for bit_str in small_perms] + [\\'1\\' + bit_str for bit_str in small_perms]\\n    \\ndef make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2):\\n    \"\"\"@Params\\n    num_variables = number of states that we care about\\n    \\n    @Returns\\n    dict of lists. Keys have the format: \\'o_{i}={val}\\', where val is either \\'0\\' or \\'1\\'\\n    \"\"\"\\n    res = {}\\n    for i in range(1, num_variables + 1):\\n        for val in range(vals_per_var):\\n            res[f\\'o_{i} = {val}\\'] = []\\n    return res', 'get_ped_possiblities(2, 5, 6)', 'def pr_ped_given_action(action, mu_ped, s_ped, mu_no_ped, s_no_ped, prior, fixed_prior=True):\\n    \"\"\"\\n    @Params\\n    mu_ped, s_ped: mean, sd pair from the policy receiving an input state where there is a visible pedestrian \\n    mu_no_ped, s_no_ped: mean, sd pair from the policy receiving an input state where there is no visible pedestrian \\n\\n    action: the vehicle\\'s acceleration as dictated by the policy\\n    prior: Pr(ped)\\n    fixed_prior: Boolean telling us whether to \\'update\\' the prior Pr(ped) using Pr(ped | action) or not\\n\\n    @Returns\\n    \\n    probs, a dict containing:\\n    1. Pr(action | ped)\\n    2. Pr(action | no_ped)\\n    3. Pr(ped | action)\\n    4. Pr(no_ped | action)\\n    5. Pr(ped) for the next computation of Pr(ped|action)\\n    \\n    3, 4, 5 are calculated subject to the fixed_prior parameter\\n    \"\"\"\\n    probs = {}\\n    \\n    # Compute 1, 2: Pr(action | ped), Pr(action | no_ped)\\n    unnormed_pr_action_given_ped = accel_pdf(mu_ped, s_ped, action)\\n    unnormed_pr_action_given_no_ped = accel_pdf(mu_no_ped, s_no_ped, action)\\n    \\n    pr_a_given_ped = unnormed_pr_action_given_ped / (unnormed_pr_action_given_ped + unnormed_pr_action_given_no_ped)\\n    pr_a_given_no_ped = 1 - pr_a_given_ped\\n    \\n    probs[\"pr_a_given_ped\"] = pr_a_given_ped\\n    probs[\"pr_a_given_no_ped\"] = pr_a_given_no_ped\\n    \\n    # Compute 3, 4: Pr(ped | action), Pr(no_ped | action)\\n    # Apply Bayes\\' rule\\n    pr_ped_given_action = (pr_a_given_ped * prior) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\\n    pr_no_ped_given_action = (pr_a_given_no_ped * (1 - prior)) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\\n    probs[\"pr_ped_given_action\"] = pr_ped_given_action\\n    probs[\"pr_no_ped_given_action\"] = pr_no_ped_given_action\\n                    \\n    if fixed_prior:\\n        probs[\"prior\"] = prior\\n    else:\\n        probs[\"prior\"] = probs[\"pr_ped_given_action\"]\\n    return probs\\n    \\n\\ndef accel_pdf(mu, sigma, actual):\\n    \"\"\"Return pdf evaluated at actual acceleration\"\"\"\\n    coeff = 1 / np.sqrt(2 * np.pi * (sigma**2))\\n    exp = -0.5 * ((actual - mu) / sigma)**2\\n    return coeff * np.exp(exp)\\n\\ndef run_transfer(args):\\n    # run transfer on the bayesian 1 env first\\n    bayesian_0_params = bayesian_0_flow_params(args, pedestrians=True, render=True)\\n#     import ipdb; ipdb.set_trace()\\n    env, env_name = create_env(args, bayesian_0_params)\\n    agent, config = create_agent(args, flow_params=bayesian_0_params)\\n    run_env(env, agent, config, bayesian_0_params)\\n\\ndef plot_2_lines(y1, y2, legend, viewable_ped=False):\\n    x = np.arange(len(y1))\\n    plt.plot(x, y1)\\n    plt.plot(x, y2)\\n    if viewable_ped:\\n        plt.plot(x, viewable_ped)\\n    plt.legend(legend, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\\n   \\n    plt.draw()\\n    plt.pause(0.001)\\n    \\ndef plot_lines(y_val_lsts, legends):\\n    assert len(y_val_lsts) == len(legends)\\n    x = np.arange(len(y_val_lsts[0]))\\n    for y_vals in y_val_lsts:\\n        plt.plot(x, y_vals)\\n    plt.legend(legends, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\\n    plt.draw()\\n    plt.pause(0.001)', 'def create_parser():\\n    \"\"\"Create the parser to capture CLI arguments.\"\"\"\\n    parser = argparse.ArgumentParser(\\n        formatter_class=argparse.RawDescriptionHelpFormatter,\\n        description=\\'[Flow] Evaluates a reinforcement learning agent \\'\\n                    \\'given a checkpoint.\\',\\n        epilog=EXAMPLE_USAGE)\\n\\n    # required input parameters\\n    parser.add_argument(\\n        \\'result_dir\\', type=str, help=\\'Directory containing results\\')\\n    parser.add_argument(\\'checkpoint_num\\', type=str, help=\\'Checkpoint number.\\')\\n\\n    # optional input parameters\\n    parser.add_argument(\\n        \\'--run\\',\\n        type=str,\\n        help=\\'The algorithm or model to train. This may refer to \\'\\n             \\'the name of a built-on algorithm (e.g. RLLib\\\\\\'s DQN \\'\\n             \\'or PPO), or a user-defined trainable function or \\'\\n             \\'class registered in the tune registry. \\'\\n             \\'Required for results trained with flow-0.2.0 and before.\\')\\n    parser.add_argument(\\n        \\'--num_rollouts\\',\\n        type=int,\\n        default=1,\\n        help=\\'The number of rollouts to visualize.\\')\\n    parser.add_argument(\\n        \\'--gen_emission\\',\\n        action=\\'store_true\\',\\n        help=\\'Specifies whether to generate an emission file from the \\'\\n             \\'simulation\\')\\n    parser.add_argument(\\n        \\'--evaluate\\',\\n        action=\\'store_true\\',\\n        help=\\'Specifies whether to use the \\\\\\'evaluate\\\\\\' reward \\'\\n             \\'for the environment.\\')\\n    parser.add_argument(\\n        \\'--render_mode\\',\\n        type=str,\\n        default=\\'sumo_gui\\',\\n        help=\\'Pick the render mode. Options include sumo_web3d, \\'\\n             \\'rgbd and sumo_gui\\')\\n    parser.add_argument(\\n        \\'--save_render\\',\\n        action=\\'store_true\\',\\n        help=\\'Saves a rendered video to a file. NOTE: Overrides render_mode \\'\\n             \\'with pyglet rendering.\\')\\n    parser.add_argument(\\n        \\'--horizon\\',\\n        type=int,\\n        help=\\'Specifies the horizon.\\')\\n    \\n    parser.add_argument(\\'--grid_search\\', action=\\'store_true\\', default=False,\\n                        help=\\'If true, a grid search is run\\')\\n    parser.add_argument(\\'--run_mode\\', type=str, default=\\'local\\',\\n                        help=\"Experiment run mode (local | cluster)\")\\n    parser.add_argument(\\'--algo\\', type=str, default=\\'QMIX\\',\\n                        help=\"RL method to use (PPO, TD3, QMIX)\")\\n    parser.add_argument(\"--pedestrians\",\\n                        help=\"use pedestrians, sidewalks, and crossings in the simulation\",\\n                        action=\"store_true\")\\n    \\n    return parser', 'parser = create_parser()\\nargs = parser.parse_args([\"./contrib_MADDPG_Bayesian1Env-v0_76a54d9c_2020-04-02_14-39-42gtq5jssz\", \"100\"])\\nray.shutdown()\\nray.init(num_cpus=1)\\nrun_transfer(args)'], '_oh': {}, '_dh': ['/home/thankyou-always/TODO/research/bayesian_reasoning_traffic/bayesian_inference'], 'In': ['', '\"\"\"Use Nick\\'s PPO trained policy to perform inference on whether there is a pedestrian or not\"\"\"\\n\\nimport argparse\\nfrom datetime import datetime\\nimport gym\\nimport os\\nimport sys\\nimport time\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport PyQt5\\n\\nimport ray\\ntry:\\n    from ray.rllib.agents.agent import get_agent_class\\nexcept ImportError:\\n    from ray.rllib.agents.registry import get_agent_class\\nfrom ray.tune.registry import register_env\\n\\nfrom flow.core.util import emission_to_csv\\nfrom flow.utils.registry import make_create_env\\nfrom flow.utils.rllib import get_flow_params\\nfrom flow.utils.rllib import get_rllib_config\\nfrom flow.utils.rllib import get_rllib_pkl\\n\\nfrom examples.rllib.multiagent_exps.test_predictor.pedestrian_policy_1 import create_env, create_agent\\nfrom examples.rllib.multiagent_exps.bayesian_1_env import make_flow_params as bayesian_0_flow_params\\n\\nEXAMPLE_USAGE = \"\"\"\\nexample usage:\\n    python ./visualizer_rllib.py /ray_results/experiment_dir/result_dir 1\\nHere the arguments are:\\n1 - the path to the simulation results\\n2 - the number of the checkpoint\\n\"\"\"', 'def run_env(env, agent, config, flow_params):\\n    \"\"\"Run the simulation and control the rl car using the trained policy. \\n    \\n    observation[4:10] = ped_param\\n    \\n    The six binary grids are at indices 4 to 9 inclusive\\n    \"\"\"\\n    # set up relevant policy and env\\n    if config.get(\\'multiagent\\', {}).get(\\'policies\\', None):\\n        multiagent = True\\n        rets = {}\\n        # map the agent id to its policy\\n        policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\']\\n        \\n#         policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\'].func\\n        for key in config[\\'multiagent\\'][\\'policies\\'].keys():\\n            rets[key] = []\\n    else:\\n        multiagent = False\\n        rets = []\\n\\n    if config[\\'model\\'][\\'use_lstm\\']:\\n        use_lstm = True\\n        if multiagent:\\n            state_init = {}\\n            # map the agent id to its policy\\n#             policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\'].func\\n\\n            policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\']\\n            size = config[\\'model\\'][\\'lstm_cell_size\\']\\n            for key in config[\\'multiagent\\'][\\'policies\\'].keys():\\n                state_init[key] = [np.zeros(size, np.float32),\\n                                   np.zeros(size, np.float32)]\\n        else:\\n            state_init = [\\n                np.zeros(config[\\'model\\'][\\'lstm_cell_size\\'], np.float32),\\n                np.zeros(config[\\'model\\'][\\'lstm_cell_size\\'], np.float32)\\n            ]\\n    else:\\n        use_lstm = False\\n\\n    env.restart_simulation(\\n        sim_params=flow_params[\\'sim\\'], render=flow_params[\\'sim\\'].render)\\n\\n    \\n    # Define variables to collect probability data. Each variable is dict\\n    # 2^6 keys (or, 2^5 values for p(o_i = b_i)) and values will be a list of numbers\\n    # TODO(KL) HARD CODED is_ped_visible is the 5th item in the state vector\\n\\n    ped_idx_lst = [4, 5, 6, 7, 8, 9]\\n    ped_front = ped_idx_lst[0]\\n    ped_back = ped_idx_lst[-1]\\n    \\n    binary_observations = True\\n    \\n    \\n    # dict for f(a | o_i = b_i), where the key is bitstring s e.g.s = \"010011\", \\n    # where s[i] corresponds to the value of b_i. cond_pdf_a_on_joint_ped[\"010011\"] = list()\\n    \\n    # 1\\n    # IF binary_observations == True:\\n    cond_pdf_action_on_joint_ped_dct = make_dct_of_lsts(num_digits=len(ped_idx_lst), vals_per_dig=2)\\n    \\n    # 2\\n    # updated Pr(o_1 = b_1, ..., o_6 = b_6) for i = 1, ..., 6 and b_i = 0, 1\\n    updated_joint_prior_prob_ped_lst = initial_prior_probs(num_digits=6, vals_per_dig=2)\\n    # fixed Pr(o_1 = b_1, ..., o_6 = b_6) for i = 1, ..., 6 and b_i = 0, 1\\n    fixed_joint_prior_prob_ped_lst = initial_prior_probs(num_digits=6, vals_per_dig=2)\\n    \\n    # 3\\n    # updated joint cond_densities lst f(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_densities_updated_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n    # fixed joint cond_densities lst f(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_densities_fixed_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n\\n    # 4\\n    # updated joint cond_probs lst p(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_probs_updated_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n    # fixed joint cond_probs lst p(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_probs_fixed_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n    \\n    # 5\\n    # updated single cond_probs lst p(o_i = b_i | a)\\n    single_cond_probs_updated_priors_lst = make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2)\\n    # fixed single cond_probs lst p(o_i = b_i | a)\\n    single_cond_probs_fixed_priors_lst = make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2)\\n    \\n    # 6\\n    # this will be the only variable that\\'s a dict of lists\\n    visible_pedestrian_dct = {i : [] for i in range(1,7)}\\n\\n    # each element in this list corresponds to a key \\n    len_6_bitstring_lst = make_permutations(num_digits=6, vals_per_dig=2)\\n    for i in range(args.num_rollouts):\\n        state = env.reset()\\n        # divide by 5 to get \"time\" in the simulation\\n        for _ in range(1000):\\n            vehicles = env.unwrapped.k.vehicle\\n            pedestrian = env.unwrapped.k.pedestrian\\n\\n            if multiagent:\\n                action, logits = {}, {}\\n                for agent_id in state.keys():\\n                    if \\'QMIX\\' == \\'QMIX\\':\\n                        agent_id += 1\\n                    if use_lstm:\\n                        action[agent_id], state_init[agent_id], logits = \\\\\\n                            agent.compute_action(state[agent_id], state=state_init[agent_id], policy_id=policy_map_fn(agent_id))\\n                    else:\\n                        s_all = state[agent_id]\\n                        # get ped visibility state array of length 6 from the rl car\\'s POV\\n                        s_ped = s_all[ped_idx_lst]\\n                        \\n                        # update the visible_pedestrian dict\\n                        for idx, val in enumerate(s_ped, 1):\\n                            visible_pedestrian_dct[idx] = visible_pedestrian_dct[idx] + [val]\\n                        print(len(state[agent_id]))\\n                        # compute the actual action taken by the rl car\\n                        action[agent_id], _, logit_actual = agent.compute_action(state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)    \\n                        action_ = action[agent_id][0]\\n                        \\n                        # store variable for the sum of densities conditioned on the actual action\\n                        cond_density_sum_updated = 0\\n                        cond_density_sum_fixed = 0\\n\\n                        # compute joint conditional densities for all possible permutations of ped visibility (ignore which permutation corresponds to the actual permutation for now)\\n                        for obs_comb in len_6_bitstring_lst:\\n                            s_all_modified = np.copy(s_all)\\n                            s_all_modified[ped_front : ped_back + 1] = list(obs_comb)\\n                            print(f\\'{s_all_modified[ped_front : ped_back + 1]} is list of length 6, {list(obs_comb)} has length 6\\')\\n                            _, _, logit = agent.compute_action(state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)\\n                            # where in the docs did we figure out the logits were ln of sigma??\\n                            import ipdb;ipdb.set_trace()\\n                            mu, ln_sigma = logit[\\'behaviour_logits\\']\\n                            sigma = np.exp(ln_sigma)\\n                            \\n                            # 1\\n                            # compute the conditional a_given_obs pdfs, i.e. f(a | o_1 = b_1, ..., o_6 = b_6)\\n                            cond_a_given_joint_o_pdf = accel_pdf(mu, sigma, action_)\\n                            # append new result to lists\\n                            cond_pdf_action_on_joint_ped_dct[obs_comb] = cond_pdf_action_on_joint_ped_dct[obs_comb] + [cond_a_given_joint_o_pdf]\\n                            \\n                            # 2\\n                            # Get the updated and fixed priors\\n                            updated_prior = updated_joint_prior_prob_ped_lst[obs_comb][-1]\\n                            fixed_prior = fixed_joint_prior_prob_ped_lst[obs_comb][-1]\\n\\n                            # 3\\n                            # compute the joint conditional obs_given_a pdfs, i.e. f(o_1 = b_1, ..., o_6 = b_6 | a)\\n                            cond_joint_o_given_a_pdf_updated = cond_a_given_joint_o_pdf * updated_prior\\n                            cond_joint_o_given_a_pdf_fixed = cond_a_given_joint_o_pdf * fixed_prior\\n                            # append new result to lists\\n                            joint_cond_densities_updated_priors_dct[obs_comb] = joint_cond_densities_updated_priors_dct[obs_comb] + [cond_joint_o_given_a_pdf_updated]\\n                            joint_cond_densities_fixed_priors_dct[obs_comb] = joint_cond_densities_updated_priors_dct[obs_comb] + [cond_joint_o_given_a_pdf_fixed]\\n\\n                            cond_density_sum_updated += cond_joint_o_given_a_pdf_updated\\n                            cond_density_sum_fixed += cond_joint_o_given_a_pdf_fixed\\n                            \\n                        # 4\\n                        # compute Pr(o_1 = b_1, ... o_6 = b_6 | a)\\n                        for obs_comb_ in len_6_bitstring_lst:\\n                            cond_joint_o_given_a_prob_updated = joint_cond_densities_updated_priors_dct[obs_comb_][-1] / cond_density_sum_updated\\n                            cond_joint_o_given_a_prob_fixed = joint_cond_densities_fixed_priors_dct[obs_comb_][-1] / cond_density_sum_fixed\\n                            # append to lists\\n                            joint_cond_probs_updated_priors_dct[obs_comb_] = joint_cond_probs_updated_priors_dct[obs_comb_] + [cond_joint_o_given_a_prob_updated]\\n                            joint_cond_probs_fixed_priors_dct[obs_comb_] = joint_cond_probs_fixed_priors_dct[obs_comb_] + [cond_joint_o_given_a_prob_fixed]\\n                        \\n                            # 2\\n                            # Update the updated and fixed priors lists\\n                            updated_joint_prior_prob_ped_lst[obs_comb_] = updated_joint_prior_prob_ped_lst[obs_comb_] + [cond_joint_o_given_a_prob_updated]\\n                            fixed_joint_prior_prob_ped_lst[obs_comb_] = fixed_joint_prior_prob_ped_lst[obs_comb_] + [cond_joint_o_given_a_prob_fixed]\\n                        \\n                        # 5\\n                        # compute Pr(o_i = b_i | a) for all i = 1, ..., 6\\n                        for grid_idx in range(1, 7):\\n                            for val in range(0, 2):\\n                                # compute the marginalization (4) - need to get the relevant ped observation combinations\\n                                single_cond_o_given_a_prob_updated = 0\\n                                single_cond_o_given_a_prob_fixed = 0\\n\\n                                for key in get_ped_possiblities(grid_idx, val):\\n                                    single_cond_o_given_a_prob_updated += joint_cond_probs_updated_priors_dct[key]\\n                                    single_cond_o_given_a_prob_fixed += joint_cond_probs_fixed_priors_dct[key]\\n                                \\n                                single_cond_prob_str = single_cond_prob_to_str(grid_idx, val)\\n                                # append to list\\n                                single_cond_probs_updated_priors_lst[single_cond_prob_str] = single_cond_probs_updated_priors_lst[single_cond_prob_str] + [single_cond_o_given_a_prob_updated]\\n                                single_cond_probs_fixed_priors_lst[single_cond_prob_str] = single_cond_probs_fixed_priors_lst[single_cond_prob_str] + [single_cond_o_given_a_prob_fixed]\\n                                \\n\\n            else:\\n                action = agent.compute_action(state)\\n            state, reward, done, _ = env.step(action)\\n\\n            if multiagent and done[\\'__all__\\']:\\n                print(_,1)\\n                break\\n            if not multiagent and done:\\n                print(_,1)\\n\\n                break    \\n            state, reward, done, _ = env.step(action)   \\n\\n        visible_ped_lsts = [visible_pedestrian_dct[i] for i in range(1, 7)]\\n        legends = [f\\'ped in grid {i}\\' for i in range(1, 7)]\\n\\n        for grid_idx in range(1, 7):\\n            for val in range(0, 2):\\n                single_cond_prob_str = single_cond_prob_to_str(grid_idx, val)\\n                a = single_cond_probs_updated_priors_lst[single_cond_prob_str]\\n                b = single_cond_probs_fixed_priors_lst[single_cond_prob_str]\\n\\n                plot_2_lines(a, b)\\n#         plot_lines(visible_ped_lsts, legends)\\n#         plot_2_lines(probs_ped_given_action_updated_priors, probs_no_ped_given_action_updated_priors, [\\'Pr(ped | action) using updated priors\\', \\'Pr(no_ped | action) using updated priors\\'], viewable_ped=visible_pedestrian[4])\\n#         plot_2_lines(probs_ped_given_action_fixed_priors, probs_no_ped_given_action_fixed_priors, [\\'Pr(ped | action) using fixed priors of Pr(ped) = 0.5\\', \\'Pr(no_ped | action) using fixed priors of Pr(ped) = 0.5\\'], viewable_ped=visible_pedestrian[5])\\n#         plot_2_lines(probs_action_given_ped, probs_action_given_no_ped, [\\'Pr(action | ped)\\', \\'Pr(action | no_ped)\\'], viewable_ped=visible_pedestrian[6])\\n\\n        ', 'def single_cond_prob_to_str(grid_idx, val, num_indices = 6):\\n    \"\"\"Generate the string representing the probability:\\n    \\n    Pr(o_i = val)\\n    \\n    ex:\\n    For Pr(o_2 = 1), we\\'d have the string \\'21\\'\\n    NB we\\'re 1-indexing here\\n    \"\"\"\\n    assert grid_idx >= 1 and grid_idx <= num_indices\\n    return str(grid_idx) + str(val)\\n\\ndef get_ped_possiblities(grid_idx, val, output_len=6):\\n    \"\"\"Give the single probability we want, compute the list of all bitstrings we need to perform the marginalization.\\n    \\n    ex:\\n    3:0 means we want p(o_3 = 0 | a)\\n    Therefore, we can get the list of all possible length 5 bitstrings, and stitch \\'0\\' in the correct place.\\n    \\n    \\n    @Return\\n    NB 1-indexing here\\n    bitstring of length 6\\n    \"\"\"\\n    assert grid_idx >= 1 and grid_idx <= output_len\\n    res_lst = make_permutations(output_len - 1, 2)\\n    for perm in res_lst:\\n\\n        print(str(perm[:grid_idx - 1:] + str(val) + perm[grid_idx - 1:]))\\n\\ndef initial_prior_probs(num_digits=6, vals_per_dig=2):\\n    \"\"\"Returns a dict with values of all permutations of bitstrings of length num_digits. \\n    Each digit can take a value from 0 to (vals_per_dig - 1)\"\"\"\\n    uniform_prob = 1 / (vals_per_dig ** num_digits)\\n    res = make_dct_of_lsts(num_digits, vals_per_dig)\\n    for key in res.keys():\\n        res[key] = res[key] + [uniform_prob]\\n    return res\\n\\ndef make_dct_of_lsts(num_digits=6, vals_per_dig=2):\\n    \"\"\"Return a dict with keys of bitstrings and values as empty lists. \\n    Hardcoded for binary vals per var.\"\"\"\\n    res = {}\\n    lst_of_bitstrings = make_permutations(num_digits, vals_per_dig)\\n        \\n    return {str_ : [] for str_ in lst_of_bitstrings}\\n\\ndef make_permutations(num_digits, vals_per_dig=2):\\n    \"\"\"Make all permutations for a bit string of length num_digits\\n    and vals_per_dig values per digit. Hardcoded for work for binary vals per var\"\"\"\\n    if num_digits == 1:\\n        return [str(i) for i in range(vals_per_dig)]\\n    else:\\n        small_perms = make_permutations(num_digits - 1, vals_per_dig)\\n        # hardcoded for work for binary vals per var\\n        return [\\'0\\' + bit_str for bit_str in small_perms] + [\\'1\\' + bit_str for bit_str in small_perms]\\n    \\ndef make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2):\\n    \"\"\"@Params\\n    num_variables = number of states that we care about\\n    \\n    @Returns\\n    dict of lists. Keys have the format: \\'o_{i}={val}\\', where val is either \\'0\\' or \\'1\\'\\n    \"\"\"\\n    res = {}\\n    for i in range(1, num_variables + 1):\\n        for val in range(vals_per_var):\\n            res[f\\'o_{i} = {val}\\'] = []\\n    return res', 'get_ped_possiblities(2, 5, 6)', 'def pr_ped_given_action(action, mu_ped, s_ped, mu_no_ped, s_no_ped, prior, fixed_prior=True):\\n    \"\"\"\\n    @Params\\n    mu_ped, s_ped: mean, sd pair from the policy receiving an input state where there is a visible pedestrian \\n    mu_no_ped, s_no_ped: mean, sd pair from the policy receiving an input state where there is no visible pedestrian \\n\\n    action: the vehicle\\'s acceleration as dictated by the policy\\n    prior: Pr(ped)\\n    fixed_prior: Boolean telling us whether to \\'update\\' the prior Pr(ped) using Pr(ped | action) or not\\n\\n    @Returns\\n    \\n    probs, a dict containing:\\n    1. Pr(action | ped)\\n    2. Pr(action | no_ped)\\n    3. Pr(ped | action)\\n    4. Pr(no_ped | action)\\n    5. Pr(ped) for the next computation of Pr(ped|action)\\n    \\n    3, 4, 5 are calculated subject to the fixed_prior parameter\\n    \"\"\"\\n    probs = {}\\n    \\n    # Compute 1, 2: Pr(action | ped), Pr(action | no_ped)\\n    unnormed_pr_action_given_ped = accel_pdf(mu_ped, s_ped, action)\\n    unnormed_pr_action_given_no_ped = accel_pdf(mu_no_ped, s_no_ped, action)\\n    \\n    pr_a_given_ped = unnormed_pr_action_given_ped / (unnormed_pr_action_given_ped + unnormed_pr_action_given_no_ped)\\n    pr_a_given_no_ped = 1 - pr_a_given_ped\\n    \\n    probs[\"pr_a_given_ped\"] = pr_a_given_ped\\n    probs[\"pr_a_given_no_ped\"] = pr_a_given_no_ped\\n    \\n    # Compute 3, 4: Pr(ped | action), Pr(no_ped | action)\\n    # Apply Bayes\\' rule\\n    pr_ped_given_action = (pr_a_given_ped * prior) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\\n    pr_no_ped_given_action = (pr_a_given_no_ped * (1 - prior)) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\\n    probs[\"pr_ped_given_action\"] = pr_ped_given_action\\n    probs[\"pr_no_ped_given_action\"] = pr_no_ped_given_action\\n                    \\n    if fixed_prior:\\n        probs[\"prior\"] = prior\\n    else:\\n        probs[\"prior\"] = probs[\"pr_ped_given_action\"]\\n    return probs\\n    \\n\\ndef accel_pdf(mu, sigma, actual):\\n    \"\"\"Return pdf evaluated at actual acceleration\"\"\"\\n    coeff = 1 / np.sqrt(2 * np.pi * (sigma**2))\\n    exp = -0.5 * ((actual - mu) / sigma)**2\\n    return coeff * np.exp(exp)\\n\\ndef run_transfer(args):\\n    # run transfer on the bayesian 1 env first\\n    bayesian_0_params = bayesian_0_flow_params(args, pedestrians=True, render=True)\\n#     import ipdb; ipdb.set_trace()\\n    env, env_name = create_env(args, bayesian_0_params)\\n    agent, config = create_agent(args, flow_params=bayesian_0_params)\\n    run_env(env, agent, config, bayesian_0_params)\\n\\ndef plot_2_lines(y1, y2, legend, viewable_ped=False):\\n    x = np.arange(len(y1))\\n    plt.plot(x, y1)\\n    plt.plot(x, y2)\\n    if viewable_ped:\\n        plt.plot(x, viewable_ped)\\n    plt.legend(legend, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\\n   \\n    plt.draw()\\n    plt.pause(0.001)\\n    \\ndef plot_lines(y_val_lsts, legends):\\n    assert len(y_val_lsts) == len(legends)\\n    x = np.arange(len(y_val_lsts[0]))\\n    for y_vals in y_val_lsts:\\n        plt.plot(x, y_vals)\\n    plt.legend(legends, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\\n    plt.draw()\\n    plt.pause(0.001)', 'def create_parser():\\n    \"\"\"Create the parser to capture CLI arguments.\"\"\"\\n    parser = argparse.ArgumentParser(\\n        formatter_class=argparse.RawDescriptionHelpFormatter,\\n        description=\\'[Flow] Evaluates a reinforcement learning agent \\'\\n                    \\'given a checkpoint.\\',\\n        epilog=EXAMPLE_USAGE)\\n\\n    # required input parameters\\n    parser.add_argument(\\n        \\'result_dir\\', type=str, help=\\'Directory containing results\\')\\n    parser.add_argument(\\'checkpoint_num\\', type=str, help=\\'Checkpoint number.\\')\\n\\n    # optional input parameters\\n    parser.add_argument(\\n        \\'--run\\',\\n        type=str,\\n        help=\\'The algorithm or model to train. This may refer to \\'\\n             \\'the name of a built-on algorithm (e.g. RLLib\\\\\\'s DQN \\'\\n             \\'or PPO), or a user-defined trainable function or \\'\\n             \\'class registered in the tune registry. \\'\\n             \\'Required for results trained with flow-0.2.0 and before.\\')\\n    parser.add_argument(\\n        \\'--num_rollouts\\',\\n        type=int,\\n        default=1,\\n        help=\\'The number of rollouts to visualize.\\')\\n    parser.add_argument(\\n        \\'--gen_emission\\',\\n        action=\\'store_true\\',\\n        help=\\'Specifies whether to generate an emission file from the \\'\\n             \\'simulation\\')\\n    parser.add_argument(\\n        \\'--evaluate\\',\\n        action=\\'store_true\\',\\n        help=\\'Specifies whether to use the \\\\\\'evaluate\\\\\\' reward \\'\\n             \\'for the environment.\\')\\n    parser.add_argument(\\n        \\'--render_mode\\',\\n        type=str,\\n        default=\\'sumo_gui\\',\\n        help=\\'Pick the render mode. Options include sumo_web3d, \\'\\n             \\'rgbd and sumo_gui\\')\\n    parser.add_argument(\\n        \\'--save_render\\',\\n        action=\\'store_true\\',\\n        help=\\'Saves a rendered video to a file. NOTE: Overrides render_mode \\'\\n             \\'with pyglet rendering.\\')\\n    parser.add_argument(\\n        \\'--horizon\\',\\n        type=int,\\n        help=\\'Specifies the horizon.\\')\\n    \\n    parser.add_argument(\\'--grid_search\\', action=\\'store_true\\', default=False,\\n                        help=\\'If true, a grid search is run\\')\\n    parser.add_argument(\\'--run_mode\\', type=str, default=\\'local\\',\\n                        help=\"Experiment run mode (local | cluster)\")\\n    parser.add_argument(\\'--algo\\', type=str, default=\\'QMIX\\',\\n                        help=\"RL method to use (PPO, TD3, QMIX)\")\\n    parser.add_argument(\"--pedestrians\",\\n                        help=\"use pedestrians, sidewalks, and crossings in the simulation\",\\n                        action=\"store_true\")\\n    \\n    return parser', 'parser = create_parser()\\nargs = parser.parse_args([\"./contrib_MADDPG_Bayesian1Env-v0_76a54d9c_2020-04-02_14-39-42gtq5jssz\", \"100\"])\\nray.shutdown()\\nray.init(num_cpus=1)\\nrun_transfer(args)'], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f3c329b99e8>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f3c32993898>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x7f3c32993898>, '_': '', '__': '', '___': '', '_i': 'def create_parser():\\n    \"\"\"Create the parser to capture CLI arguments.\"\"\"\\n    parser = argparse.ArgumentParser(\\n        formatter_class=argparse.RawDescriptionHelpFormatter,\\n        description=\\'[Flow] Evaluates a reinforcement learning agent \\'\\n                    \\'given a checkpoint.\\',\\n        epilog=EXAMPLE_USAGE)\\n\\n    # required input parameters\\n    parser.add_argument(\\n        \\'result_dir\\', type=str, help=\\'Directory containing results\\')\\n    parser.add_argument(\\'checkpoint_num\\', type=str, help=\\'Checkpoint number.\\')\\n\\n    # optional input parameters\\n    parser.add_argument(\\n        \\'--run\\',\\n        type=str,\\n        help=\\'The algorithm or model to train. This may refer to \\'\\n             \\'the name of a built-on algorithm (e.g. RLLib\\\\\\'s DQN \\'\\n             \\'or PPO), or a user-defined trainable function or \\'\\n             \\'class registered in the tune registry. \\'\\n             \\'Required for results trained with flow-0.2.0 and before.\\')\\n    parser.add_argument(\\n        \\'--num_rollouts\\',\\n        type=int,\\n        default=1,\\n        help=\\'The number of rollouts to visualize.\\')\\n    parser.add_argument(\\n        \\'--gen_emission\\',\\n        action=\\'store_true\\',\\n        help=\\'Specifies whether to generate an emission file from the \\'\\n             \\'simulation\\')\\n    parser.add_argument(\\n        \\'--evaluate\\',\\n        action=\\'store_true\\',\\n        help=\\'Specifies whether to use the \\\\\\'evaluate\\\\\\' reward \\'\\n             \\'for the environment.\\')\\n    parser.add_argument(\\n        \\'--render_mode\\',\\n        type=str,\\n        default=\\'sumo_gui\\',\\n        help=\\'Pick the render mode. Options include sumo_web3d, \\'\\n             \\'rgbd and sumo_gui\\')\\n    parser.add_argument(\\n        \\'--save_render\\',\\n        action=\\'store_true\\',\\n        help=\\'Saves a rendered video to a file. NOTE: Overrides render_mode \\'\\n             \\'with pyglet rendering.\\')\\n    parser.add_argument(\\n        \\'--horizon\\',\\n        type=int,\\n        help=\\'Specifies the horizon.\\')\\n    \\n    parser.add_argument(\\'--grid_search\\', action=\\'store_true\\', default=False,\\n                        help=\\'If true, a grid search is run\\')\\n    parser.add_argument(\\'--run_mode\\', type=str, default=\\'local\\',\\n                        help=\"Experiment run mode (local | cluster)\")\\n    parser.add_argument(\\'--algo\\', type=str, default=\\'QMIX\\',\\n                        help=\"RL method to use (PPO, TD3, QMIX)\")\\n    parser.add_argument(\"--pedestrians\",\\n                        help=\"use pedestrians, sidewalks, and crossings in the simulation\",\\n                        action=\"store_true\")\\n    \\n    return parser', '_ii': 'def pr_ped_given_action(action, mu_ped, s_ped, mu_no_ped, s_no_ped, prior, fixed_prior=True):\\n    \"\"\"\\n    @Params\\n    mu_ped, s_ped: mean, sd pair from the policy receiving an input state where there is a visible pedestrian \\n    mu_no_ped, s_no_ped: mean, sd pair from the policy receiving an input state where there is no visible pedestrian \\n\\n    action: the vehicle\\'s acceleration as dictated by the policy\\n    prior: Pr(ped)\\n    fixed_prior: Boolean telling us whether to \\'update\\' the prior Pr(ped) using Pr(ped | action) or not\\n\\n    @Returns\\n    \\n    probs, a dict containing:\\n    1. Pr(action | ped)\\n    2. Pr(action | no_ped)\\n    3. Pr(ped | action)\\n    4. Pr(no_ped | action)\\n    5. Pr(ped) for the next computation of Pr(ped|action)\\n    \\n    3, 4, 5 are calculated subject to the fixed_prior parameter\\n    \"\"\"\\n    probs = {}\\n    \\n    # Compute 1, 2: Pr(action | ped), Pr(action | no_ped)\\n    unnormed_pr_action_given_ped = accel_pdf(mu_ped, s_ped, action)\\n    unnormed_pr_action_given_no_ped = accel_pdf(mu_no_ped, s_no_ped, action)\\n    \\n    pr_a_given_ped = unnormed_pr_action_given_ped / (unnormed_pr_action_given_ped + unnormed_pr_action_given_no_ped)\\n    pr_a_given_no_ped = 1 - pr_a_given_ped\\n    \\n    probs[\"pr_a_given_ped\"] = pr_a_given_ped\\n    probs[\"pr_a_given_no_ped\"] = pr_a_given_no_ped\\n    \\n    # Compute 3, 4: Pr(ped | action), Pr(no_ped | action)\\n    # Apply Bayes\\' rule\\n    pr_ped_given_action = (pr_a_given_ped * prior) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\\n    pr_no_ped_given_action = (pr_a_given_no_ped * (1 - prior)) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\\n    probs[\"pr_ped_given_action\"] = pr_ped_given_action\\n    probs[\"pr_no_ped_given_action\"] = pr_no_ped_given_action\\n                    \\n    if fixed_prior:\\n        probs[\"prior\"] = prior\\n    else:\\n        probs[\"prior\"] = probs[\"pr_ped_given_action\"]\\n    return probs\\n    \\n\\ndef accel_pdf(mu, sigma, actual):\\n    \"\"\"Return pdf evaluated at actual acceleration\"\"\"\\n    coeff = 1 / np.sqrt(2 * np.pi * (sigma**2))\\n    exp = -0.5 * ((actual - mu) / sigma)**2\\n    return coeff * np.exp(exp)\\n\\ndef run_transfer(args):\\n    # run transfer on the bayesian 1 env first\\n    bayesian_0_params = bayesian_0_flow_params(args, pedestrians=True, render=True)\\n#     import ipdb; ipdb.set_trace()\\n    env, env_name = create_env(args, bayesian_0_params)\\n    agent, config = create_agent(args, flow_params=bayesian_0_params)\\n    run_env(env, agent, config, bayesian_0_params)\\n\\ndef plot_2_lines(y1, y2, legend, viewable_ped=False):\\n    x = np.arange(len(y1))\\n    plt.plot(x, y1)\\n    plt.plot(x, y2)\\n    if viewable_ped:\\n        plt.plot(x, viewable_ped)\\n    plt.legend(legend, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\\n   \\n    plt.draw()\\n    plt.pause(0.001)\\n    \\ndef plot_lines(y_val_lsts, legends):\\n    assert len(y_val_lsts) == len(legends)\\n    x = np.arange(len(y_val_lsts[0]))\\n    for y_vals in y_val_lsts:\\n        plt.plot(x, y_vals)\\n    plt.legend(legends, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\\n    plt.draw()\\n    plt.pause(0.001)', '_iii': 'get_ped_possiblities(2, 5, 6)', '_i1': '\"\"\"Use Nick\\'s PPO trained policy to perform inference on whether there is a pedestrian or not\"\"\"\\n\\nimport argparse\\nfrom datetime import datetime\\nimport gym\\nimport os\\nimport sys\\nimport time\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport PyQt5\\n\\nimport ray\\ntry:\\n    from ray.rllib.agents.agent import get_agent_class\\nexcept ImportError:\\n    from ray.rllib.agents.registry import get_agent_class\\nfrom ray.tune.registry import register_env\\n\\nfrom flow.core.util import emission_to_csv\\nfrom flow.utils.registry import make_create_env\\nfrom flow.utils.rllib import get_flow_params\\nfrom flow.utils.rllib import get_rllib_config\\nfrom flow.utils.rllib import get_rllib_pkl\\n\\nfrom examples.rllib.multiagent_exps.test_predictor.pedestrian_policy_1 import create_env, create_agent\\nfrom examples.rllib.multiagent_exps.bayesian_1_env import make_flow_params as bayesian_0_flow_params\\n\\nEXAMPLE_USAGE = \"\"\"\\nexample usage:\\n    python ./visualizer_rllib.py /ray_results/experiment_dir/result_dir 1\\nHere the arguments are:\\n1 - the path to the simulation results\\n2 - the number of the checkpoint\\n\"\"\"', 'argparse': <module 'argparse' from '/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/argparse.py'>, 'datetime': <class 'datetime.datetime'>, 'gym': <module 'gym' from '/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/gym/__init__.py'>, 'os': <module 'os' from '/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/os.py'>, 'sys': <module 'sys' (built-in)>, 'time': <module 'time' (built-in)>, 'plt': <module 'matplotlib.pyplot' from '/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/matplotlib/pyplot.py'>, 'np': <module 'numpy' from '/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/numpy/__init__.py'>, 'PyQt5': <module 'PyQt5' from '/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/PyQt5/__init__.py'>, 'ray': <module 'ray' from '/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/ray/__init__.py'>, 'get_agent_class': <function get_agent_class at 0x7f3b2fd7c620>, 'register_env': <function register_env at 0x7f3c032fa730>, 'emission_to_csv': <function emission_to_csv at 0x7f3b2c82d158>, 'make_create_env': <function make_create_env at 0x7f3b2c82d400>, 'get_flow_params': <function get_flow_params at 0x7f3b2c68fe18>, 'get_rllib_config': <function get_rllib_config at 0x7f3b2c28b488>, 'get_rllib_pkl': <function get_rllib_pkl at 0x7f3b2c21ed08>, 'create_env': <function create_env at 0x7f3b2c21ef28>, 'create_agent': <function create_agent at 0x7f3b2c2250d0>, 'bayesian_0_flow_params': <function make_flow_params at 0x7f3b2c225488>, 'EXAMPLE_USAGE': '\\nexample usage:\\n    python ./visualizer_rllib.py /ray_results/experiment_dir/result_dir 1\\nHere the arguments are:\\n1 - the path to the simulation results\\n2 - the number of the checkpoint\\n', '_i2': 'def run_env(env, agent, config, flow_params):\\n    \"\"\"Run the simulation and control the rl car using the trained policy. \\n    \\n    observation[4:10] = ped_param\\n    \\n    The six binary grids are at indices 4 to 9 inclusive\\n    \"\"\"\\n    # set up relevant policy and env\\n    if config.get(\\'multiagent\\', {}).get(\\'policies\\', None):\\n        multiagent = True\\n        rets = {}\\n        # map the agent id to its policy\\n        policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\']\\n        \\n#         policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\'].func\\n        for key in config[\\'multiagent\\'][\\'policies\\'].keys():\\n            rets[key] = []\\n    else:\\n        multiagent = False\\n        rets = []\\n\\n    if config[\\'model\\'][\\'use_lstm\\']:\\n        use_lstm = True\\n        if multiagent:\\n            state_init = {}\\n            # map the agent id to its policy\\n#             policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\'].func\\n\\n            policy_map_fn = config[\\'multiagent\\'][\\'policy_mapping_fn\\']\\n            size = config[\\'model\\'][\\'lstm_cell_size\\']\\n            for key in config[\\'multiagent\\'][\\'policies\\'].keys():\\n                state_init[key] = [np.zeros(size, np.float32),\\n                                   np.zeros(size, np.float32)]\\n        else:\\n            state_init = [\\n                np.zeros(config[\\'model\\'][\\'lstm_cell_size\\'], np.float32),\\n                np.zeros(config[\\'model\\'][\\'lstm_cell_size\\'], np.float32)\\n            ]\\n    else:\\n        use_lstm = False\\n\\n    env.restart_simulation(\\n        sim_params=flow_params[\\'sim\\'], render=flow_params[\\'sim\\'].render)\\n\\n    \\n    # Define variables to collect probability data. Each variable is dict\\n    # 2^6 keys (or, 2^5 values for p(o_i = b_i)) and values will be a list of numbers\\n    # TODO(KL) HARD CODED is_ped_visible is the 5th item in the state vector\\n\\n    ped_idx_lst = [4, 5, 6, 7, 8, 9]\\n    ped_front = ped_idx_lst[0]\\n    ped_back = ped_idx_lst[-1]\\n    \\n    binary_observations = True\\n    \\n    \\n    # dict for f(a | o_i = b_i), where the key is bitstring s e.g.s = \"010011\", \\n    # where s[i] corresponds to the value of b_i. cond_pdf_a_on_joint_ped[\"010011\"] = list()\\n    \\n    # 1\\n    # IF binary_observations == True:\\n    cond_pdf_action_on_joint_ped_dct = make_dct_of_lsts(num_digits=len(ped_idx_lst), vals_per_dig=2)\\n    \\n    # 2\\n    # updated Pr(o_1 = b_1, ..., o_6 = b_6) for i = 1, ..., 6 and b_i = 0, 1\\n    updated_joint_prior_prob_ped_lst = initial_prior_probs(num_digits=6, vals_per_dig=2)\\n    # fixed Pr(o_1 = b_1, ..., o_6 = b_6) for i = 1, ..., 6 and b_i = 0, 1\\n    fixed_joint_prior_prob_ped_lst = initial_prior_probs(num_digits=6, vals_per_dig=2)\\n    \\n    # 3\\n    # updated joint cond_densities lst f(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_densities_updated_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n    # fixed joint cond_densities lst f(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_densities_fixed_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n\\n    # 4\\n    # updated joint cond_probs lst p(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_probs_updated_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n    # fixed joint cond_probs lst p(o_1 = b_1, ..., o_6 = b_6 | a)\\n    joint_cond_probs_fixed_priors_dct = make_dct_of_lsts(num_digits=6, vals_per_dig=2)\\n    \\n    # 5\\n    # updated single cond_probs lst p(o_i = b_i | a)\\n    single_cond_probs_updated_priors_lst = make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2)\\n    # fixed single cond_probs lst p(o_i = b_i | a)\\n    single_cond_probs_fixed_priors_lst = make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2)\\n    \\n    # 6\\n    # this will be the only variable that\\'s a dict of lists\\n    visible_pedestrian_dct = {i : [] for i in range(1,7)}\\n\\n    # each element in this list corresponds to a key \\n    len_6_bitstring_lst = make_permutations(num_digits=6, vals_per_dig=2)\\n    for i in range(args.num_rollouts):\\n        state = env.reset()\\n        # divide by 5 to get \"time\" in the simulation\\n        for _ in range(1000):\\n            vehicles = env.unwrapped.k.vehicle\\n            pedestrian = env.unwrapped.k.pedestrian\\n\\n            if multiagent:\\n                action, logits = {}, {}\\n                for agent_id in state.keys():\\n                    if \\'QMIX\\' == \\'QMIX\\':\\n                        agent_id += 1\\n                    if use_lstm:\\n                        action[agent_id], state_init[agent_id], logits = \\\\\\n                            agent.compute_action(state[agent_id], state=state_init[agent_id], policy_id=policy_map_fn(agent_id))\\n                    else:\\n                        s_all = state[agent_id]\\n                        # get ped visibility state array of length 6 from the rl car\\'s POV\\n                        s_ped = s_all[ped_idx_lst]\\n                        \\n                        # update the visible_pedestrian dict\\n                        for idx, val in enumerate(s_ped, 1):\\n                            visible_pedestrian_dct[idx] = visible_pedestrian_dct[idx] + [val]\\n                        print(len(state[agent_id]))\\n                        # compute the actual action taken by the rl car\\n                        action[agent_id], _, logit_actual = agent.compute_action(state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)    \\n                        action_ = action[agent_id][0]\\n                        \\n                        # store variable for the sum of densities conditioned on the actual action\\n                        cond_density_sum_updated = 0\\n                        cond_density_sum_fixed = 0\\n\\n                        # compute joint conditional densities for all possible permutations of ped visibility (ignore which permutation corresponds to the actual permutation for now)\\n                        for obs_comb in len_6_bitstring_lst:\\n                            s_all_modified = np.copy(s_all)\\n                            s_all_modified[ped_front : ped_back + 1] = list(obs_comb)\\n                            print(f\\'{s_all_modified[ped_front : ped_back + 1]} is list of length 6, {list(obs_comb)} has length 6\\')\\n                            _, _, logit = agent.compute_action(state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)\\n                            # where in the docs did we figure out the logits were ln of sigma??\\n                            import ipdb;ipdb.set_trace()\\n                            mu, ln_sigma = logit[\\'behaviour_logits\\']\\n                            sigma = np.exp(ln_sigma)\\n                            \\n                            # 1\\n                            # compute the conditional a_given_obs pdfs, i.e. f(a | o_1 = b_1, ..., o_6 = b_6)\\n                            cond_a_given_joint_o_pdf = accel_pdf(mu, sigma, action_)\\n                            # append new result to lists\\n                            cond_pdf_action_on_joint_ped_dct[obs_comb] = cond_pdf_action_on_joint_ped_dct[obs_comb] + [cond_a_given_joint_o_pdf]\\n                            \\n                            # 2\\n                            # Get the updated and fixed priors\\n                            updated_prior = updated_joint_prior_prob_ped_lst[obs_comb][-1]\\n                            fixed_prior = fixed_joint_prior_prob_ped_lst[obs_comb][-1]\\n\\n                            # 3\\n                            # compute the joint conditional obs_given_a pdfs, i.e. f(o_1 = b_1, ..., o_6 = b_6 | a)\\n                            cond_joint_o_given_a_pdf_updated = cond_a_given_joint_o_pdf * updated_prior\\n                            cond_joint_o_given_a_pdf_fixed = cond_a_given_joint_o_pdf * fixed_prior\\n                            # append new result to lists\\n                            joint_cond_densities_updated_priors_dct[obs_comb] = joint_cond_densities_updated_priors_dct[obs_comb] + [cond_joint_o_given_a_pdf_updated]\\n                            joint_cond_densities_fixed_priors_dct[obs_comb] = joint_cond_densities_updated_priors_dct[obs_comb] + [cond_joint_o_given_a_pdf_fixed]\\n\\n                            cond_density_sum_updated += cond_joint_o_given_a_pdf_updated\\n                            cond_density_sum_fixed += cond_joint_o_given_a_pdf_fixed\\n                            \\n                        # 4\\n                        # compute Pr(o_1 = b_1, ... o_6 = b_6 | a)\\n                        for obs_comb_ in len_6_bitstring_lst:\\n                            cond_joint_o_given_a_prob_updated = joint_cond_densities_updated_priors_dct[obs_comb_][-1] / cond_density_sum_updated\\n                            cond_joint_o_given_a_prob_fixed = joint_cond_densities_fixed_priors_dct[obs_comb_][-1] / cond_density_sum_fixed\\n                            # append to lists\\n                            joint_cond_probs_updated_priors_dct[obs_comb_] = joint_cond_probs_updated_priors_dct[obs_comb_] + [cond_joint_o_given_a_prob_updated]\\n                            joint_cond_probs_fixed_priors_dct[obs_comb_] = joint_cond_probs_fixed_priors_dct[obs_comb_] + [cond_joint_o_given_a_prob_fixed]\\n                        \\n                            # 2\\n                            # Update the updated and fixed priors lists\\n                            updated_joint_prior_prob_ped_lst[obs_comb_] = updated_joint_prior_prob_ped_lst[obs_comb_] + [cond_joint_o_given_a_prob_updated]\\n                            fixed_joint_prior_prob_ped_lst[obs_comb_] = fixed_joint_prior_prob_ped_lst[obs_comb_] + [cond_joint_o_given_a_prob_fixed]\\n                        \\n                        # 5\\n                        # compute Pr(o_i = b_i | a) for all i = 1, ..., 6\\n                        for grid_idx in range(1, 7):\\n                            for val in range(0, 2):\\n                                # compute the marginalization (4) - need to get the relevant ped observation combinations\\n                                single_cond_o_given_a_prob_updated = 0\\n                                single_cond_o_given_a_prob_fixed = 0\\n\\n                                for key in get_ped_possiblities(grid_idx, val):\\n                                    single_cond_o_given_a_prob_updated += joint_cond_probs_updated_priors_dct[key]\\n                                    single_cond_o_given_a_prob_fixed += joint_cond_probs_fixed_priors_dct[key]\\n                                \\n                                single_cond_prob_str = single_cond_prob_to_str(grid_idx, val)\\n                                # append to list\\n                                single_cond_probs_updated_priors_lst[single_cond_prob_str] = single_cond_probs_updated_priors_lst[single_cond_prob_str] + [single_cond_o_given_a_prob_updated]\\n                                single_cond_probs_fixed_priors_lst[single_cond_prob_str] = single_cond_probs_fixed_priors_lst[single_cond_prob_str] + [single_cond_o_given_a_prob_fixed]\\n                                \\n\\n            else:\\n                action = agent.compute_action(state)\\n            state, reward, done, _ = env.step(action)\\n\\n            if multiagent and done[\\'__all__\\']:\\n                print(_,1)\\n                break\\n            if not multiagent and done:\\n                print(_,1)\\n\\n                break    \\n            state, reward, done, _ = env.step(action)   \\n\\n        visible_ped_lsts = [visible_pedestrian_dct[i] for i in range(1, 7)]\\n        legends = [f\\'ped in grid {i}\\' for i in range(1, 7)]\\n\\n        for grid_idx in range(1, 7):\\n            for val in range(0, 2):\\n                single_cond_prob_str = single_cond_prob_to_str(grid_idx, val)\\n                a = single_cond_probs_updated_priors_lst[single_cond_prob_str]\\n                b = single_cond_probs_fixed_priors_lst[single_cond_prob_str]\\n\\n                plot_2_lines(a, b)\\n#         plot_lines(visible_ped_lsts, legends)\\n#         plot_2_lines(probs_ped_given_action_updated_priors, probs_no_ped_given_action_updated_priors, [\\'Pr(ped | action) using updated priors\\', \\'Pr(no_ped | action) using updated priors\\'], viewable_ped=visible_pedestrian[4])\\n#         plot_2_lines(probs_ped_given_action_fixed_priors, probs_no_ped_given_action_fixed_priors, [\\'Pr(ped | action) using fixed priors of Pr(ped) = 0.5\\', \\'Pr(no_ped | action) using fixed priors of Pr(ped) = 0.5\\'], viewable_ped=visible_pedestrian[5])\\n#         plot_2_lines(probs_action_given_ped, probs_action_given_no_ped, [\\'Pr(action | ped)\\', \\'Pr(action | no_ped)\\'], viewable_ped=visible_pedestrian[6])\\n\\n        ', 'run_env': <function run_env at 0x7f3b2c2efd08>, '_i3': 'def single_cond_prob_to_str(grid_idx, val, num_indices = 6):\\n    \"\"\"Generate the string representing the probability:\\n    \\n    Pr(o_i = val)\\n    \\n    ex:\\n    For Pr(o_2 = 1), we\\'d have the string \\'21\\'\\n    NB we\\'re 1-indexing here\\n    \"\"\"\\n    assert grid_idx >= 1 and grid_idx <= num_indices\\n    return str(grid_idx) + str(val)\\n\\ndef get_ped_possiblities(grid_idx, val, output_len=6):\\n    \"\"\"Give the single probability we want, compute the list of all bitstrings we need to perform the marginalization.\\n    \\n    ex:\\n    3:0 means we want p(o_3 = 0 | a)\\n    Therefore, we can get the list of all possible length 5 bitstrings, and stitch \\'0\\' in the correct place.\\n    \\n    \\n    @Return\\n    NB 1-indexing here\\n    bitstring of length 6\\n    \"\"\"\\n    assert grid_idx >= 1 and grid_idx <= output_len\\n    res_lst = make_permutations(output_len - 1, 2)\\n    for perm in res_lst:\\n\\n        print(str(perm[:grid_idx - 1:] + str(val) + perm[grid_idx - 1:]))\\n\\ndef initial_prior_probs(num_digits=6, vals_per_dig=2):\\n    \"\"\"Returns a dict with values of all permutations of bitstrings of length num_digits. \\n    Each digit can take a value from 0 to (vals_per_dig - 1)\"\"\"\\n    uniform_prob = 1 / (vals_per_dig ** num_digits)\\n    res = make_dct_of_lsts(num_digits, vals_per_dig)\\n    for key in res.keys():\\n        res[key] = res[key] + [uniform_prob]\\n    return res\\n\\ndef make_dct_of_lsts(num_digits=6, vals_per_dig=2):\\n    \"\"\"Return a dict with keys of bitstrings and values as empty lists. \\n    Hardcoded for binary vals per var.\"\"\"\\n    res = {}\\n    lst_of_bitstrings = make_permutations(num_digits, vals_per_dig)\\n        \\n    return {str_ : [] for str_ in lst_of_bitstrings}\\n\\ndef make_permutations(num_digits, vals_per_dig=2):\\n    \"\"\"Make all permutations for a bit string of length num_digits\\n    and vals_per_dig values per digit. Hardcoded for work for binary vals per var\"\"\"\\n    if num_digits == 1:\\n        return [str(i) for i in range(vals_per_dig)]\\n    else:\\n        small_perms = make_permutations(num_digits - 1, vals_per_dig)\\n        # hardcoded for work for binary vals per var\\n        return [\\'0\\' + bit_str for bit_str in small_perms] + [\\'1\\' + bit_str for bit_str in small_perms]\\n    \\ndef make_single_cond_prob_dct_of_lsts(num_variables=6, vals_per_var=2):\\n    \"\"\"@Params\\n    num_variables = number of states that we care about\\n    \\n    @Returns\\n    dict of lists. Keys have the format: \\'o_{i}={val}\\', where val is either \\'0\\' or \\'1\\'\\n    \"\"\"\\n    res = {}\\n    for i in range(1, num_variables + 1):\\n        for val in range(vals_per_var):\\n            res[f\\'o_{i} = {val}\\'] = []\\n    return res', 'single_cond_prob_to_str': <function single_cond_prob_to_str at 0x7f3c301518c8>, 'get_ped_possiblities': <function get_ped_possiblities at 0x7f3b2c2540d0>, 'initial_prior_probs': <function initial_prior_probs at 0x7f3b2c254158>, 'make_dct_of_lsts': <function make_dct_of_lsts at 0x7f3b2c2541e0>, 'make_permutations': <function make_permutations at 0x7f3b2c254268>, 'make_single_cond_prob_dct_of_lsts': <function make_single_cond_prob_dct_of_lsts at 0x7f3b2c2542f0>, '_i4': 'get_ped_possiblities(2, 5, 6)', '_i5': 'def pr_ped_given_action(action, mu_ped, s_ped, mu_no_ped, s_no_ped, prior, fixed_prior=True):\\n    \"\"\"\\n    @Params\\n    mu_ped, s_ped: mean, sd pair from the policy receiving an input state where there is a visible pedestrian \\n    mu_no_ped, s_no_ped: mean, sd pair from the policy receiving an input state where there is no visible pedestrian \\n\\n    action: the vehicle\\'s acceleration as dictated by the policy\\n    prior: Pr(ped)\\n    fixed_prior: Boolean telling us whether to \\'update\\' the prior Pr(ped) using Pr(ped | action) or not\\n\\n    @Returns\\n    \\n    probs, a dict containing:\\n    1. Pr(action | ped)\\n    2. Pr(action | no_ped)\\n    3. Pr(ped | action)\\n    4. Pr(no_ped | action)\\n    5. Pr(ped) for the next computation of Pr(ped|action)\\n    \\n    3, 4, 5 are calculated subject to the fixed_prior parameter\\n    \"\"\"\\n    probs = {}\\n    \\n    # Compute 1, 2: Pr(action | ped), Pr(action | no_ped)\\n    unnormed_pr_action_given_ped = accel_pdf(mu_ped, s_ped, action)\\n    unnormed_pr_action_given_no_ped = accel_pdf(mu_no_ped, s_no_ped, action)\\n    \\n    pr_a_given_ped = unnormed_pr_action_given_ped / (unnormed_pr_action_given_ped + unnormed_pr_action_given_no_ped)\\n    pr_a_given_no_ped = 1 - pr_a_given_ped\\n    \\n    probs[\"pr_a_given_ped\"] = pr_a_given_ped\\n    probs[\"pr_a_given_no_ped\"] = pr_a_given_no_ped\\n    \\n    # Compute 3, 4: Pr(ped | action), Pr(no_ped | action)\\n    # Apply Bayes\\' rule\\n    pr_ped_given_action = (pr_a_given_ped * prior) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\\n    pr_no_ped_given_action = (pr_a_given_no_ped * (1 - prior)) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\\n    probs[\"pr_ped_given_action\"] = pr_ped_given_action\\n    probs[\"pr_no_ped_given_action\"] = pr_no_ped_given_action\\n                    \\n    if fixed_prior:\\n        probs[\"prior\"] = prior\\n    else:\\n        probs[\"prior\"] = probs[\"pr_ped_given_action\"]\\n    return probs\\n    \\n\\ndef accel_pdf(mu, sigma, actual):\\n    \"\"\"Return pdf evaluated at actual acceleration\"\"\"\\n    coeff = 1 / np.sqrt(2 * np.pi * (sigma**2))\\n    exp = -0.5 * ((actual - mu) / sigma)**2\\n    return coeff * np.exp(exp)\\n\\ndef run_transfer(args):\\n    # run transfer on the bayesian 1 env first\\n    bayesian_0_params = bayesian_0_flow_params(args, pedestrians=True, render=True)\\n#     import ipdb; ipdb.set_trace()\\n    env, env_name = create_env(args, bayesian_0_params)\\n    agent, config = create_agent(args, flow_params=bayesian_0_params)\\n    run_env(env, agent, config, bayesian_0_params)\\n\\ndef plot_2_lines(y1, y2, legend, viewable_ped=False):\\n    x = np.arange(len(y1))\\n    plt.plot(x, y1)\\n    plt.plot(x, y2)\\n    if viewable_ped:\\n        plt.plot(x, viewable_ped)\\n    plt.legend(legend, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\\n   \\n    plt.draw()\\n    plt.pause(0.001)\\n    \\ndef plot_lines(y_val_lsts, legends):\\n    assert len(y_val_lsts) == len(legends)\\n    x = np.arange(len(y_val_lsts[0]))\\n    for y_vals in y_val_lsts:\\n        plt.plot(x, y_vals)\\n    plt.legend(legends, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\\n    plt.draw()\\n    plt.pause(0.001)', 'pr_ped_given_action': <function pr_ped_given_action at 0x7f3b2c254e18>, 'accel_pdf': <function accel_pdf at 0x7f3b2c254ae8>, 'run_transfer': <function run_transfer at 0x7f3b2c254a60>, 'plot_2_lines': <function plot_2_lines at 0x7f3b2c2549d8>, 'plot_lines': <function plot_lines at 0x7f3b2c254950>, '_i6': 'def create_parser():\\n    \"\"\"Create the parser to capture CLI arguments.\"\"\"\\n    parser = argparse.ArgumentParser(\\n        formatter_class=argparse.RawDescriptionHelpFormatter,\\n        description=\\'[Flow] Evaluates a reinforcement learning agent \\'\\n                    \\'given a checkpoint.\\',\\n        epilog=EXAMPLE_USAGE)\\n\\n    # required input parameters\\n    parser.add_argument(\\n        \\'result_dir\\', type=str, help=\\'Directory containing results\\')\\n    parser.add_argument(\\'checkpoint_num\\', type=str, help=\\'Checkpoint number.\\')\\n\\n    # optional input parameters\\n    parser.add_argument(\\n        \\'--run\\',\\n        type=str,\\n        help=\\'The algorithm or model to train. This may refer to \\'\\n             \\'the name of a built-on algorithm (e.g. RLLib\\\\\\'s DQN \\'\\n             \\'or PPO), or a user-defined trainable function or \\'\\n             \\'class registered in the tune registry. \\'\\n             \\'Required for results trained with flow-0.2.0 and before.\\')\\n    parser.add_argument(\\n        \\'--num_rollouts\\',\\n        type=int,\\n        default=1,\\n        help=\\'The number of rollouts to visualize.\\')\\n    parser.add_argument(\\n        \\'--gen_emission\\',\\n        action=\\'store_true\\',\\n        help=\\'Specifies whether to generate an emission file from the \\'\\n             \\'simulation\\')\\n    parser.add_argument(\\n        \\'--evaluate\\',\\n        action=\\'store_true\\',\\n        help=\\'Specifies whether to use the \\\\\\'evaluate\\\\\\' reward \\'\\n             \\'for the environment.\\')\\n    parser.add_argument(\\n        \\'--render_mode\\',\\n        type=str,\\n        default=\\'sumo_gui\\',\\n        help=\\'Pick the render mode. Options include sumo_web3d, \\'\\n             \\'rgbd and sumo_gui\\')\\n    parser.add_argument(\\n        \\'--save_render\\',\\n        action=\\'store_true\\',\\n        help=\\'Saves a rendered video to a file. NOTE: Overrides render_mode \\'\\n             \\'with pyglet rendering.\\')\\n    parser.add_argument(\\n        \\'--horizon\\',\\n        type=int,\\n        help=\\'Specifies the horizon.\\')\\n    \\n    parser.add_argument(\\'--grid_search\\', action=\\'store_true\\', default=False,\\n                        help=\\'If true, a grid search is run\\')\\n    parser.add_argument(\\'--run_mode\\', type=str, default=\\'local\\',\\n                        help=\"Experiment run mode (local | cluster)\")\\n    parser.add_argument(\\'--algo\\', type=str, default=\\'QMIX\\',\\n                        help=\"RL method to use (PPO, TD3, QMIX)\")\\n    parser.add_argument(\"--pedestrians\",\\n                        help=\"use pedestrians, sidewalks, and crossings in the simulation\",\\n                        action=\"store_true\")\\n    \\n    return parser', 'create_parser': <function create_parser at 0x7f3b2c2547b8>, '_i7': 'parser = create_parser()\\nargs = parser.parse_args([\"./contrib_MADDPG_Bayesian1Env-v0_76a54d9c_2020-04-02_14-39-42gtq5jssz\", \"100\"])\\nray.shutdown()\\nray.init(num_cpus=1)\\nrun_transfer(args)', 'parser': ArgumentParser(prog='ipykernel_launcher.py', usage=None, description='[Flow] Evaluates a reinforcement learning agent given a checkpoint.', formatter_class=<class 'argparse.RawDescriptionHelpFormatter'>, conflict_handler='error', add_help=True), 'args': Namespace(algo='QMIX', checkpoint_num='100', evaluate=False, gen_emission=False, grid_search=False, horizon=None, num_rollouts=1, pedestrians=False, render_mode='sumo_gui', result_dir='./contrib_MADDPG_Bayesian1Env-v0_76a54d9c_2020-04-02_14-39-42gtq5jssz', run=None, run_mode='local', save_render=False)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> tf.multiply.\n",
      "*** SyntaxError: invalid syntax\n",
      "ipdb> tf.multiply\n",
      "<function multiply at 0x7f3b5f886f28>\n",
      "ipdb> tf.get_collection\n",
      "*** AttributeError: module 'tensorflow' has no attribute 'get_collection'\n",
      "ipdb> tf.__version__\n",
      "'2.1.0'\n"
     ]
    }
   ],
   "source": [
    "parser = create_parser()\n",
    "args = parser.parse_args([\"./contrib_MADDPG_Bayesian1Env-v0_76a54d9c_2020-04-02_14-39-42gtq5jssz\", \"100\"])\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=1)\n",
    "run_transfer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_ped_given_action(.6538469, 0.14252673, 2.4491935, 0.14200129, 2.5765653, 0.5100277175584501, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([11,22, 3, 4, 5, 6, 7])\n",
    "c = [1, 2, 3]\n",
    "b[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, val in enumerate(b, 1):\n",
    "    print(idx, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flow] *",
   "language": "python",
   "name": "conda-env-flow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
