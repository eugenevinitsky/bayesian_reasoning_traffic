{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Probability extractor\"\"\"\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PyQt5\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.core.util import emission_to_csv\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import get_flow_params\n",
    "from flow.utils.rllib import get_rllib_config\n",
    "from flow.utils.rllib import get_rllib_pkl\n",
    "\n",
    "from examples.rllib.multiagent_exps.test_predictor.pedestrian_policy_1 import create_env, create_agent\n",
    "from examples.rllib.multiagent_exps.bayesian_0_training_script import make_flow_params as bayesian_1_flow_params\n",
    "\n",
    "EXAMPLE_USAGE = \"\"\"\n",
    "example usage:\n",
    "    python ./visualizer_rllib.py /ray_results/experiment_dir/result_dir 1\n",
    "Here the arguments are:\n",
    "1 - the path to the simulation results\n",
    "2 - the number of the checkpoint\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env, agent, config, flow_params):\n",
    "\n",
    "    # set up relevant policy and env\n",
    "    if config.get('multiagent', {}).get('policies', None):\n",
    "        multiagent = True\n",
    "        rets = {}\n",
    "        # map the agent id to its policy\n",
    "        policy_map_fn = config['multiagent']['policy_mapping_fn'].func\n",
    "        for key in config['multiagent']['policies'].keys():\n",
    "            rets[key] = []\n",
    "    else:\n",
    "        multiagent = False\n",
    "        rets = []\n",
    "\n",
    "    if config['model']['use_lstm']:\n",
    "        use_lstm = True\n",
    "        if multiagent:\n",
    "            state_init = {}\n",
    "            # map the agent id to its policy\n",
    "            policy_map_fn = config['multiagent']['policy_mapping_fn'].func\n",
    "            size = config['model']['lstm_cell_size']\n",
    "            for key in config['multiagent']['policies'].keys():\n",
    "                state_init[key] = [np.zeros(size, np.float32),\n",
    "                                   np.zeros(size, np.float32)]\n",
    "        else:\n",
    "            state_init = [\n",
    "                np.zeros(config['model']['lstm_cell_size'], np.float32),\n",
    "                np.zeros(config['model']['lstm_cell_size'], np.float32)\n",
    "            ]\n",
    "    else:\n",
    "        use_lstm = False\n",
    "\n",
    "    env.restart_simulation(\n",
    "        sim_params=flow_params['sim'], render=flow_params['sim'].render)\n",
    "\n",
    "    \n",
    "    # Define variables to collect probability data\n",
    "    # TODO(KL) HARD CODED is_ped_visible is the 5th item in the state vector\n",
    "\n",
    "    ped_idx = 4\n",
    "    probs_action_given_ped = []\n",
    "    probs_action_given_no_ped = []\n",
    "\n",
    "    # updated priors list\n",
    "    probs_ped_given_action_updated_priors = []\n",
    "    probs_no_ped_given_action_updated_priors = []\n",
    "\n",
    "    # fixed priors list\n",
    "    probs_ped_given_action_fixed_priors = []\n",
    "    probs_no_ped_given_action_fixed_priors = []\n",
    "\n",
    "    # updated Pr(ped), Pr(no_ped)        \n",
    "    updated_prior_prob_ped = 0.5\n",
    "    updated_prior_prob_no_ped = 0.5\n",
    "\n",
    "    # fixed Pr(ped), Pr(no_ped)   \n",
    "    fixed_prior_prob_ped = 0.5\n",
    "    fixed_prior_prob_no_ped = 0.5\n",
    "\n",
    "    visible_pedestrian = []\n",
    "\n",
    "    for i in range(args.num_rollouts):\n",
    "        state = env.reset()\n",
    "        # divide by 5 to get \"time\" in the simulation\n",
    "        for _ in range(300):\n",
    "            vehicles = env.unwrapped.k.vehicle\n",
    "            pedestrian = env.unwrapped.k.pedestrian\n",
    "\n",
    "            if multiagent:\n",
    "                action, logits = {}, {}\n",
    "                for agent_id in state.keys():\n",
    "                    if use_lstm:\n",
    "                        action[agent_id], state_init[agent_id], logits = \\\n",
    "                            agent.compute_action(state[agent_id], \n",
    "                                                    state=state_init[agent_id],\n",
    "                                                    policy_id=policy_map_fn(agent_id))\n",
    "                    else:\n",
    "                        curr_ped = state[agent_id][ped_idx]\n",
    "                        visible_pedestrian.append(curr_ped)\n",
    "\n",
    "                        flipped_ped = 1 if curr_ped == 0 else 0\n",
    "                        \n",
    "                        ped_flipped_state = np.copy(state[agent_id])\n",
    "                        ped_flipped_state[ped_idx] = flipped_ped\n",
    "\n",
    "                        action[agent_id], _, logit_actual = agent.compute_action(\n",
    "                            state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)\n",
    "                            \n",
    "                        _, _, logit_flipped = agent.compute_action(\n",
    "                            ped_flipped_state, policy_id=policy_map_fn(agent_id), full_fetch=True)\n",
    "\n",
    "                        mu_ped, ln_sigma_ped = logit_actual['behaviour_logits']\n",
    "                        mu_no_ped, ln_sigma_no_ped = logit_flipped['behaviour_logits']\n",
    "\n",
    "                        sigma_ped = np.exp(ln_sigma_ped)\n",
    "                        sigma_no_ped = np.exp(ln_sigma_no_ped)\n",
    "\n",
    "                        action_ = action[agent_id][0]\n",
    "\n",
    "                        # dict of probs for updated and fixed priors\n",
    "                        probs_fixed = pr_ped_given_action(action_, mu_ped, sigma_ped, mu_no_ped, sigma_no_ped, fixed_prior_prob_ped, fixed_prior=True)\n",
    "                        probs_updated = pr_ped_given_action(action_, mu_ped, sigma_ped, mu_no_ped, sigma_no_ped, updated_prior_prob_ped, fixed_prior=False)\n",
    "\n",
    "                        probs_action_given_ped.append(probs_fixed[\"pr_a_given_ped\"])\n",
    "                        probs_action_given_no_ped.append(probs_fixed[\"pr_a_given_no_ped\"])\n",
    "                        \n",
    "                        # Fixed priors: Pr(ped | action), Pr(no_ped | action)\n",
    "                        probs_ped_given_action_fixed_priors.append(probs_fixed[\"pr_ped_given_action\"])\n",
    "                        probs_no_ped_given_action_fixed_priors.append(probs_fixed[\"pr_no_ped_given_action\"])\n",
    "                        fixed_prior_prob_ped = probs_fixed[\"prior\"]\n",
    "\n",
    "                        # Updated priors: Pr(ped | action), Pr(no_ped | action)\n",
    "                        probs_ped_given_action_updated_priors.append(probs_updated[\"pr_ped_given_action\"])\n",
    "                        probs_no_ped_given_action_updated_priors.append(probs_updated[\"pr_no_ped_given_action\"])\n",
    "                        updated_prior_prob_ped = probs_updated[\"prior\"] \n",
    "\n",
    "                        # set a lower bound for priors\n",
    "                        updated_prior_prob_ped = probs_updated[\"prior\"] if probs_updated[\"prior\"] < 0.99 else 0.99\n",
    "#                         %matplotlib qt\n",
    "#                         vehicles.get_viewable_objects('rl_0', \\\n",
    "#                                     pedestrians=pedestrian, visualize=True)\n",
    "                        \n",
    "\n",
    "\n",
    "            else:\n",
    "                action = agent.compute_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if multiagent and done['__all__']:\n",
    "                break\n",
    "            if not multiagent and done:\n",
    "                break    \n",
    "            state, reward, done, _ = env.step(action)   \n",
    "        %matplotlib inline    \n",
    "\n",
    "        plot_2_lines(probs_ped_given_action_updated_priors, probs_no_ped_given_action_updated_priors, ['Pr(ped | action) using updated priors', 'Pr(no_ped | action) using updated priors'], viewable_ped=visible_pedestrian)\n",
    "        plot_2_lines(probs_ped_given_action_fixed_priors, probs_no_ped_given_action_fixed_priors, ['Pr(ped | action) using fixed priors of Pr(ped) = 0.5', 'Pr(no_ped | action) using fixed priors of Pr(ped) = 0.5'], viewable_ped=visible_pedestrian)\n",
    "        plot_2_lines(probs_action_given_ped, probs_action_given_no_ped, ['Pr(action | ped)', 'Pr(action | no_ped)'], viewable_ped=visible_pedestrian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_ped_given_action(action, mu_ped, s_ped, mu_no_ped, s_no_ped, prior, fixed_prior=True):\n",
    "    \"\"\"\n",
    "    @Params\n",
    "    mu_ped, s_ped: mean, sd pair from the policy receiving an input state where there is a visible pedestrian \n",
    "    mu_no_ped, s_no_ped: mean, sd pair from the policy receiving an input state where there is no visible pedestrian \n",
    "\n",
    "    action: the vehicle's acceleration as dictated by the policy\n",
    "    prior: Pr(ped)\n",
    "    fixed_prior: Boolean telling us whether to 'update' the prior Pr(ped) using Pr(ped | action) or not\n",
    "\n",
    "    @Returns\n",
    "    \n",
    "    probs, a dict containing:\n",
    "    1. Pr(action | ped)\n",
    "    2. Pr(action | no_ped)\n",
    "    3. Pr(ped | action)\n",
    "    4. Pr(no_ped | action)\n",
    "    5. Pr(ped) for the next computation of Pr(ped|action)\n",
    "    \n",
    "    3, 4, 5 are calculated subject to the fixed_prior parameter\n",
    "    \"\"\"\n",
    "    probs = {}\n",
    "    \n",
    "    # Compute 1, 2: Pr(action | ped), Pr(action | no_ped)\n",
    "    unnormed_pr_action_given_ped = accel_pdf(mu_ped, s_ped, action)\n",
    "    unnormed_pr_action_given_no_ped = accel_pdf(mu_no_ped, s_no_ped, action)\n",
    "    \n",
    "    pr_a_given_ped = unnormed_pr_action_given_ped / (unnormed_pr_action_given_ped + unnormed_pr_action_given_no_ped)\n",
    "    pr_a_given_no_ped = 1 - pr_a_given_ped\n",
    "    \n",
    "    probs[\"pr_a_given_ped\"] = pr_a_given_ped\n",
    "    probs[\"pr_a_given_no_ped\"] = pr_a_given_no_ped\n",
    "    \n",
    "    # Compute 3, 4: Pr(ped | action), Pr(no_ped | action)\n",
    "    # Apply Bayes' rule\n",
    "    pr_ped_given_action = (pr_a_given_ped * prior) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\n",
    "    pr_no_ped_given_action = (pr_a_given_no_ped * (1 - prior)) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\n",
    "    probs[\"pr_ped_given_action\"] = pr_ped_given_action\n",
    "    probs[\"pr_no_ped_given_action\"] = pr_no_ped_given_action\n",
    "                    \n",
    "    if fixed_prior:\n",
    "        probs[\"prior\"] = prior\n",
    "    else:\n",
    "        probs[\"prior\"] = probs[\"pr_ped_given_action\"]\n",
    "    return probs\n",
    "    \n",
    "\n",
    "def accel_pdf(mu, sigma, actual):\n",
    "    \"\"\"Return pdf evaluated at actual acceleration\"\"\"\n",
    "    coeff = 1 / np.sqrt(2 * np.pi * (sigma**2))\n",
    "    exp = -0.5 * ((actual - mu) / sigma)**2\n",
    "    return coeff * np.exp(exp)\n",
    "\n",
    "def run_transfer(args):\n",
    "    # run transfer on the bayesian 1 env first\n",
    "    bayesian_1_params = bayesian_1_flow_params(pedestrians=True, render=True)\n",
    "    env, env_name = create_env(args, bayesian_1_params)\n",
    "    agent, config = create_agent(args, flow_params=bayesian_1_params)\n",
    "    run_env(env, agent, config, bayesian_1_params)\n",
    "\n",
    "def plot_2_lines(y1, y2, legend, viewable_ped=False):\n",
    "    x = np.arange(len(y1))\n",
    "    plt.plot(x, y1)\n",
    "    plt.plot(x, y2)\n",
    "    if viewable_ped:\n",
    "        plt.plot(x, viewable_ped)\n",
    "    plt.legend(legend, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\n",
    "   \n",
    "    plt.draw()\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser():\n",
    "    \"\"\"Create the parser to capture CLI arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        description='[Flow] Evaluates a reinforcement learning agent '\n",
    "                    'given a checkpoint.',\n",
    "        epilog=EXAMPLE_USAGE)\n",
    "\n",
    "    # required input parameters\n",
    "    parser.add_argument(\n",
    "        'result_dir', type=str, help='Directory containing results')\n",
    "    parser.add_argument('checkpoint_num', type=str, help='Checkpoint number.')\n",
    "\n",
    "    # optional input parameters\n",
    "    parser.add_argument(\n",
    "        '--run',\n",
    "        type=str,\n",
    "        help='The algorithm or model to train. This may refer to '\n",
    "             'the name of a built-on algorithm (e.g. RLLib\\'s DQN '\n",
    "             'or PPO), or a user-defined trainable function or '\n",
    "             'class registered in the tune registry. '\n",
    "             'Required for results trained with flow-0.2.0 and before.')\n",
    "    parser.add_argument(\n",
    "        '--num_rollouts',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='The number of rollouts to visualize.')\n",
    "    parser.add_argument(\n",
    "        '--gen_emission',\n",
    "        action='store_true',\n",
    "        help='Specifies whether to generate an emission file from the '\n",
    "             'simulation')\n",
    "    parser.add_argument(\n",
    "        '--evaluate',\n",
    "        action='store_true',\n",
    "        help='Specifies whether to use the \\'evaluate\\' reward '\n",
    "             'for the environment.')\n",
    "    parser.add_argument(\n",
    "        '--render_mode',\n",
    "        type=str,\n",
    "        default='sumo_gui',\n",
    "        help='Pick the render mode. Options include sumo_web3d, '\n",
    "             'rgbd and sumo_gui')\n",
    "    parser.add_argument(\n",
    "        '--save_render',\n",
    "        action='store_true',\n",
    "        help='Saves a rendered video to a file. NOTE: Overrides render_mode '\n",
    "             'with pyglet rendering.')\n",
    "    parser.add_argument(\n",
    "        '--horizon',\n",
    "        type=int,\n",
    "        help='Specifies the horizon.')\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-27 23:17:56,507\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-03-27_23-17-56_507156_29827/logs.\n",
      "2020-03-27 23:17:56,623\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:25211 to respond...\n",
      "2020-03-27 23:17:56,737\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:48626 to respond...\n",
      "2020-03-27 23:17:56,743\tINFO services.py:809 -- Starting Redis shard with 3.33 GB max memory.\n",
      "2020-03-27 23:17:56,794\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-03-27_23-17-56_507156_29827/logs.\n",
      "2020-03-27 23:17:56,796\tWARNING services.py:1330 -- WARNING: The default object store size of 4.99 GB will use more than 50% of the available memory on this node (7.51 GB). Consider setting the object store memory manually to a smaller size to avoid memory contention with other applications.\n",
      "2020-03-27 23:17:56,797\tINFO services.py:1475 -- Starting the Plasma object store with 4.99 GB memory using /dev/shm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: With render mode sumo_gui, an extra instance of the SUMO GUI will display before the GUI for visualizing the result. Click the green Play arrow to continue.\n",
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "True\n",
      "NOTE: With render mode sumo_gui, an extra instance of the SUMO GUI will display before the GUI for visualizing the result. Click the green Play arrow to continue.\n",
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "Error making env  Cannot re-register id: Bayesian1Env-v0\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-27 23:17:59,125\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n",
      "2020-03-27 23:17:59,136\tWARNING ppo.py:143 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "Error making env  Cannot re-register id: Bayesian1Env-v0\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-27 23:18:00,253\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "2020-03-27 23:18:00,372\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\n",
      "{ 'action_prob': <tf.Tensor 'av/action_prob:0' shape=(?,) dtype=float32>,\n",
      "  'actions': <tf.Tensor 'av/actions:0' shape=(?, 1) dtype=float32>,\n",
      "  'advantages': <tf.Tensor 'av/advantages:0' shape=(?,) dtype=float32>,\n",
      "  'behaviour_logits': <tf.Tensor 'av/behaviour_logits:0' shape=(?, 2) dtype=float32>,\n",
      "  'dones': <tf.Tensor 'av/dones:0' shape=(?,) dtype=bool>,\n",
      "  'new_obs': <tf.Tensor 'av/new_obs:0' shape=(?, 17) dtype=float32>,\n",
      "  'obs': <tf.Tensor 'av/observation:0' shape=(?, 17) dtype=float32>,\n",
      "  'prev_actions': <tf.Tensor 'av/action:0' shape=(?, 1) dtype=float32>,\n",
      "  'prev_rewards': <tf.Tensor 'av/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "  'rewards': <tf.Tensor 'av/rewards:0' shape=(?,) dtype=float32>,\n",
      "  'value_targets': <tf.Tensor 'av/value_targets:0' shape=(?,) dtype=float32>,\n",
      "  'vf_preds': <tf.Tensor 'av/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\n",
      "/home/thankyou-always/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "2020-03-27 23:18:01,100\tINFO rollout_worker.py:742 -- Built policy map: {'av': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f89284e3f60>}\n",
      "2020-03-27 23:18:01,101\tINFO rollout_worker.py:743 -- Built preprocessor map: {'av': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f89284f2470>}\n",
      "2020-03-27 23:18:01,101\tINFO rollout_worker.py:356 -- Built filter map: {'av': <ray.rllib.utils.filter.NoFilter object at 0x7f89284e3668>}\n",
      "2020-03-27 23:18:01,104\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Observation outside expected value range', Box(17,), array([ 0.,  0.,  1., 20.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0def7b0bff82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrun_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d0b16e930f28>\u001b[0m in \u001b[0;36mrun_transfer\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbayesian_1_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbayesian_1_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mrun_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbayesian_1_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_2_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviewable_ped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4a99b96eaddf>\u001b[0m in \u001b[0;36mrun_env\u001b[0;34m(env, agent, config, flow_params)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                         action[agent_id], _, logit_actual = agent.compute_action(\n\u001b[0;32m---> 87\u001b[0;31m                             state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                         _, _, logit_flipped = agent.compute_action(\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mcompute_action\u001b[0;34m(self, observation, state, prev_action, prev_reward, info, policy_id, full_fetch)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         preprocessed = self.workers.local_worker().preprocessors[\n\u001b[0;32m--> 563\u001b[0;31m             policy_id].transform(observation)\n\u001b[0m\u001b[1;32m    564\u001b[0m         filtered_obs = self.workers.local_worker().filters[policy_id](\n\u001b[1;32m    565\u001b[0m             preprocessed, update=False)\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/preprocessors.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPreprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/preprocessors.py\u001b[0m in \u001b[0;36mcheck_shape\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     raise ValueError(\n\u001b[1;32m     60\u001b[0m                         \u001b[0;34m\"Observation outside expected value range\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                         self._obs_space, observation)\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: ('Observation outside expected value range', Box(17,), array([ 0.,  0.,  1., 20.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,\n        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]))"
     ]
    }
   ],
   "source": [
    "parser = create_parser()\n",
    "args = parser.parse_args([\"./Bayesian1Env_with_pedestrians/\", \"100\"])\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=1)\n",
    "run_transfer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(\"policy_0.pkl\", 'wb') as f:\n",
    "    dill.dump(run_transfer, f)\n",
    "    dill.dump(args, f)\n",
    "    dill.dump(bayesian_1_flow_params, f)\n",
    "    dill.dump(create_env, f)\n",
    "    dill.dump(create_agent, f)\n",
    "    dill.dump(run_env, f)\n",
    "    dill.dump(pr_ped_given_action, f)\n",
    "    dill.dump(accel_pdf, f)\n",
    "    dill.dump(plot_2_lines, f)\n",
    "    with open('./requirements.txt', 'r') as req:\n",
    "        x = req.readlines()\n",
    "        dill.dump(x, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
