{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Probability extractor\"\"\"\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PyQt5\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.core.util import emission_to_csv\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import get_flow_params\n",
    "from flow.utils.rllib import get_rllib_config\n",
    "from flow.utils.rllib import get_rllib_pkl\n",
    "\n",
    "from examples.rllib.multiagent_exps.test_predictor.pedestrian_policy_1 import create_env, create_agent\n",
    "from examples.rllib.multiagent_exps.bayesian_0_no_grid_env import make_flow_params as bayesian_1_flow_params\n",
    "\n",
    "EXAMPLE_USAGE = \"\"\"\n",
    "example usage:\n",
    "    python ./visualizer_rllib.py /ray_results/experiment_dir/result_dir 1\n",
    "Here the arguments are:\n",
    "1 - the path to the simulation results\n",
    "2 - the number of the checkpoint\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env, agent, config, flow_params):\n",
    "\n",
    "    # set up relevant policy and env\n",
    "    if config.get('multiagent', {}).get('policies', None):\n",
    "        multiagent = True\n",
    "        rets = {}\n",
    "        # map the agent id to its policy\n",
    "        policy_map_fn = config['multiagent']['policy_mapping_fn']\n",
    "        for key in config['multiagent']['policies'].keys():\n",
    "            rets[key] = []\n",
    "    else:\n",
    "        multiagent = False\n",
    "        rets = []\n",
    "\n",
    "    if config['model']['use_lstm']:\n",
    "        use_lstm = True\n",
    "        if multiagent:\n",
    "            state_init = {}\n",
    "            # map the agent id to its policy\n",
    "            policy_map_fn = config['multiagent']['policy_mapping_fn']\n",
    "            size = config['model']['lstm_cell_size']\n",
    "            for key in config['multiagent']['policies'].keys():\n",
    "                state_init[key] = [np.zeros(size, np.float32),\n",
    "                                   np.zeros(size, np.float32)]\n",
    "        else:\n",
    "            state_init = [\n",
    "                np.zeros(config['model']['lstm_cell_size'], np.float32),\n",
    "                np.zeros(config['model']['lstm_cell_size'], np.float32)\n",
    "            ]\n",
    "    else:\n",
    "        use_lstm = False\n",
    "\n",
    "    env.restart_simulation(\n",
    "        sim_params=flow_params['sim'], render=flow_params['sim'].render)\n",
    "\n",
    "    \n",
    "    # Define variables to collect probability data\n",
    "    # TODO(KL) HARD CODED is_ped_visible is the 6th item in the state vector\n",
    "    ped_idx = 5\n",
    "    probs_action_given_ped = []\n",
    "    probs_action_given_no_ped = []\n",
    "\n",
    "    # updated priors list\n",
    "    probs_ped_given_action_updated_priors = []\n",
    "    probs_no_ped_given_action_updated_priors = []\n",
    "\n",
    "    # fixed priors list\n",
    "    probs_ped_given_action_fixed_priors = []\n",
    "    probs_no_ped_given_action_fixed_priors = []\n",
    "\n",
    "    # updated Pr(ped), Pr(no_ped)        \n",
    "    updated_prior_prob_ped = 0.5\n",
    "    updated_prior_prob_no_ped = 0.5\n",
    "\n",
    "    # fixed Pr(ped), Pr(no_ped)   \n",
    "    fixed_prior_prob_ped = 0.5\n",
    "    fixed_prior_prob_no_ped = 0.5\n",
    "\n",
    "    visible_pedestrian = []\n",
    "\n",
    "    for i in range(args.num_rollouts):\n",
    "        state = env.reset()\n",
    "        # divide by 5 to get \"time\" in the simulation\n",
    "        for _ in range(600):\n",
    "            vehicles = env.unwrapped.k.vehicle\n",
    "            pedestrian = env.unwrapped.k.pedestrian\n",
    "\n",
    "            if multiagent:\n",
    "                action, logits = {}, {}\n",
    "                for agent_id in state.keys():\n",
    "                    if use_lstm:\n",
    "                        action[agent_id], state_init[agent_id], logits = \\\n",
    "                            agent.compute_action(state[agent_id], \n",
    "                                                    state=state_init[agent_id],\n",
    "                                                    policy_id=policy_map_fn(agent_id))\n",
    "                    else:\n",
    "                        # track the 'ground-truth' of whether car actually sees ped\n",
    "                        curr_ped_state = state[agent_id][ped_idx]\n",
    "                        visible_pedestrian.append(curr_ped_state)\n",
    "                        \n",
    "                        ped_present_state = np.copy(state[agent_id])\n",
    "                        ped_present_state[5] = 1\n",
    "                        ped_not_present_state = np.copy(state[agent_id])\n",
    "                        ped_not_present_state[5] = 0\n",
    "\n",
    "                        action[agent_id], _, _ = agent.compute_action(\n",
    "                            state[agent_id], policy_id=policy_map_fn(agent_id), full_fetch=True)\n",
    "                        \n",
    "                        _, _, ped_present_logits = agent.compute_action(\n",
    "                            ped_present_state, policy_id=policy_map_fn(agent_id), full_fetch=True)\n",
    "                        \n",
    "                        _, _, ped_not_present_logits = agent.compute_action(\n",
    "                            ped_not_present_state, policy_id=policy_map_fn(agent_id), full_fetch=True)\n",
    "\n",
    "                        mu_ped, ln_sigma_ped = ped_present_logits['behaviour_logits']\n",
    "                        mu_no_ped, ln_sigma_no_ped = ped_not_present_logits['behaviour_logits']\n",
    "\n",
    "                        sigma_ped = np.exp(ln_sigma_ped)\n",
    "                        sigma_no_ped = np.exp(ln_sigma_no_ped)\n",
    "\n",
    "                        action_ = action[agent_id][0]\n",
    "\n",
    "                        # dict of probs for updated and fixed priors\n",
    "                        probs_fixed = pr_ped_given_action(action_, mu_ped, sigma_ped, mu_no_ped, sigma_no_ped, fixed_prior_prob_ped, fixed_prior=True)\n",
    "                        probs_updated = pr_ped_given_action(action_, mu_ped, sigma_ped, mu_no_ped, sigma_no_ped, updated_prior_prob_ped, fixed_prior=False)\n",
    "\n",
    "                        probs_action_given_ped.append(probs_fixed[f'pr_a_given_ped'])\n",
    "                        probs_action_given_no_ped.append(probs_fixed[f'pr_a_given_no_ped'])\n",
    "                        \n",
    "                        # Fixed priors: Pr(ped | action), Pr(no_ped | action)\n",
    "                        probs_ped_given_action_fixed_priors.append(probs_fixed[\"pr_ped_given_action\"])\n",
    "                        probs_no_ped_given_action_fixed_priors.append(probs_fixed[\"pr_no_ped_given_action\"])\n",
    "                        fixed_prior_prob_ped = probs_fixed[\"prior\"]\n",
    "\n",
    "                        # Updated priors: Pr(ped | action), Pr(no_ped | action)\n",
    "                        probs_ped_given_action_updated_priors.append(probs_updated[\"pr_ped_given_action\"])\n",
    "                        probs_no_ped_given_action_updated_priors.append(probs_updated[\"pr_no_ped_given_action\"])\n",
    "                        updated_prior_prob_ped = probs_updated[\"prior\"] \n",
    "\n",
    "                        # set a lower bound for priors\n",
    "                        updated_prior_prob_ped = probs_updated[\"prior\"] if probs_updated[\"prior\"] < 0.99 else 0.99\n",
    "#                         %matplotlib qt\n",
    "#                         vehicles.get_viewable_objects('rl_0', \\\n",
    "#                                     pedestrians=pedestrian, visualize=True)\n",
    "                        \n",
    "\n",
    "\n",
    "            else:\n",
    "                action = agent.compute_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if multiagent and done['__all__']:\n",
    "                break\n",
    "            if not multiagent and done:\n",
    "                break    \n",
    "            state, reward, done, _ = env.step(action)   \n",
    "        %matplotlib inline    \n",
    "        fig, axes = plt.subplots(3, 1, figsize=(8, 12))\n",
    "        fig.tight_layout()\n",
    "        ax1, ax2, ax3 = axes\n",
    "        plot_2_lines(ax1, probs_ped_given_action_updated_priors, probs_no_ped_given_action_updated_priors, ['Pr(ped | action) using updated priors', 'Pr(no_ped | action) using updated priors'], False)\n",
    "        plot_2_lines(ax2, probs_ped_given_action_fixed_priors, probs_no_ped_given_action_fixed_priors, ['Pr(ped | action) using fixed priors of Pr(ped) = 0.5', 'Pr(no_ped | action) using fixed priors of Pr(ped) = 0.5'], False)\n",
    "        plot_2_lines(ax3, probs_action_given_ped, probs_action_given_no_ped, ['Pr(action | ped)', 'Pr(action | no_ped)'], viewable_ped=visible_pedestrian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_ped_given_action(action, mu_ped, s_ped, mu_no_ped, s_no_ped, prior, fixed_prior=True):\n",
    "    \"\"\"\n",
    "    @Params\n",
    "    mu_ped, s_ped: mean, sd pair from the policy receiving an input state where there is a visible pedestrian \n",
    "    mu_no_ped, s_no_ped: mean, sd pair from the policy receiving an input state where there is no visible pedestrian \n",
    "\n",
    "    action: the vehicle's acceleration as dictated by the policy\n",
    "    prior: Pr(ped)\n",
    "    fixed_prior: Boolean telling us whether to 'update' the prior Pr(ped) using Pr(ped | action) or not\n",
    "\n",
    "    @Returns\n",
    "    \n",
    "    probs, a dict containing:\n",
    "    1. Pr(action | ped)\n",
    "    2. Pr(action | no_ped)\n",
    "    3. Pr(ped | action)\n",
    "    4. Pr(no_ped | action)\n",
    "    5. Pr(ped) for the next computation of Pr(ped|action)\n",
    "    \n",
    "    3, 4, 5 are calculated subject to the fixed_prior parameter\n",
    "    \"\"\"\n",
    "    probs = {}\n",
    "    \n",
    "    # Compute 1, 2: Pr(action | ped), Pr(action | no_ped)\n",
    "    unnormed_pr_action_given_ped = accel_pdf(mu_ped, s_ped, action)\n",
    "    unnormed_pr_action_given_no_ped = accel_pdf(mu_no_ped, s_no_ped, action)\n",
    "    \n",
    "    pr_a_given_ped = unnormed_pr_action_given_ped / (unnormed_pr_action_given_ped + unnormed_pr_action_given_no_ped)\n",
    "    pr_a_given_no_ped = 1 - pr_a_given_ped\n",
    "    \n",
    "    probs[\"pr_a_given_ped\"] = pr_a_given_ped\n",
    "    probs[\"pr_a_given_no_ped\"] = pr_a_given_no_ped\n",
    "    \n",
    "    # Compute 3, 4: Pr(ped | action), Pr(no_ped | action)\n",
    "    # Apply Bayes' rule\n",
    "    pr_ped_given_action = (pr_a_given_ped * prior) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\n",
    "    pr_no_ped_given_action = (pr_a_given_no_ped * (1 - prior)) / ((pr_a_given_ped * prior)  + (pr_a_given_no_ped * (1 - prior)))\n",
    "    probs[\"pr_ped_given_action\"] = pr_ped_given_action\n",
    "    probs[\"pr_no_ped_given_action\"] = pr_no_ped_given_action\n",
    "                    \n",
    "    if fixed_prior:\n",
    "        probs[\"prior\"] = prior\n",
    "    else:\n",
    "        probs[\"prior\"] = probs[\"pr_ped_given_action\"]\n",
    "    return probs\n",
    "    \n",
    "\n",
    "def accel_pdf(mu, sigma, actual):\n",
    "    \"\"\"Return pdf evaluated at actual acceleration\"\"\"\n",
    "    coeff = 1 / np.sqrt(2 * np.pi * (sigma**2))\n",
    "    exp = -0.5 * ((actual - mu) / sigma)**2\n",
    "    return coeff * np.exp(exp)\n",
    "\n",
    "def run_transfer(args):\n",
    "    # run transfer on the bayesian 1 env first\n",
    "    bayesian_1_params = bayesian_1_flow_params(args, pedestrians=True, render=True)\n",
    "    env, env_name = create_env(args, bayesian_1_params)\n",
    "    agent, config = create_agent(args, flow_params=bayesian_1_params)\n",
    "    run_env(env, agent, config, bayesian_1_params)\n",
    "\n",
    "def plot_2_lines(axis, y1, y2, legend, viewable_ped=False):\n",
    "    x = np.arange(len(y1))\n",
    "    axis.plot(x, y1)\n",
    "    axis.plot(x, y2)\n",
    "    if viewable_ped:\n",
    "        axis.plot(x, viewable_ped, 'r')\n",
    "    axis.legend(legend, bbox_to_anchor=(0.5, 1.05), loc=3, borderaxespad=0.)\n",
    "#     axis.draw()\n",
    "#     axis.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser():\n",
    "    \"\"\"Create the parser to capture CLI arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        description='[Flow] Evaluates a reinforcement learning agent '\n",
    "                    'given a checkpoint.',\n",
    "        epilog=EXAMPLE_USAGE)\n",
    "\n",
    "    # required input parameters\n",
    "    parser.add_argument(\n",
    "        'result_dir', type=str, help='Directory containing results')\n",
    "    parser.add_argument('checkpoint_num', type=str, help='Checkpoint number.')\n",
    "\n",
    "    # optional input parameters\n",
    "    parser.add_argument(\n",
    "        '--run',\n",
    "        type=str,\n",
    "        help='The algorithm or model to train. This may refer to '\n",
    "             'the name of a built-on algorithm (e.g. RLLib\\'s DQN '\n",
    "             'or PPO), or a user-defined trainable function or '\n",
    "             'class registered in the tune registry. '\n",
    "             'Required for results trained with flow-0.2.0 and before.')\n",
    "    parser.add_argument(\n",
    "        '--num_rollouts',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='The number of rollouts to visualize.')\n",
    "    parser.add_argument(\n",
    "        '--gen_emission',\n",
    "        action='store_true',\n",
    "        help='Specifies whether to generate an emission file from the '\n",
    "             'simulation')\n",
    "    parser.add_argument(\n",
    "        '--evaluate',\n",
    "        action='store_true',\n",
    "        help='Specifies whether to use the \\'evaluate\\' reward '\n",
    "             'for the environment.')\n",
    "    parser.add_argument(\n",
    "        '--render_mode',\n",
    "        type=str,\n",
    "        default='sumo_gui',\n",
    "        help='Pick the render mode. Options include sumo_web3d, '\n",
    "             'rgbd and sumo_gui')\n",
    "    parser.add_argument(\n",
    "        '--save_render',\n",
    "        action='store_true',\n",
    "        help='Saves a rendered video to a file. NOTE: Overrides render_mode '\n",
    "             'with pyglet rendering.')\n",
    "    parser.add_argument(\n",
    "        '--horizon',\n",
    "        type=int,\n",
    "        help='Specifies the horizon.')\n",
    "    parser.add_argument(\n",
    "        '--algo', \n",
    "        type=str, default='QMIX',\n",
    "        help=\"RL method to use (PPO, TD3, QMIX)\")\n",
    "    parser.add_argument(\n",
    "        \"--pedestrians\",\n",
    "        help=\"use pedestrians, sidewalks, and crossings in the simulation\",\n",
    "        action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "        \"--render\",\n",
    "        help=\"render SUMO simulation\",\n",
    "        action=\"store_true\")\n",
    "    \n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-07 15:08:16,987\tWARNING services.py:597 -- setpgrp failed, processes may not be cleaned up properly: [Errno 1] Operation not permitted.\n",
      "2020-05-07 15:08:16,988\tINFO resource_spec.py:216 -- Starting Ray with 4.3 GiB memory available for workers and up to 2.16 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: With render mode sumo_gui, an extra instance of the SUMO GUI will display before the GUI for visualizing the result. Click the green Play arrow to continue.\n",
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "Error making env  Cannot re-register id: Bayesian0NoGridEnv-v0\n",
      "True\n",
      "NOTE: With render mode sumo_gui, an extra instance of the SUMO GUI will display before the GUI for visualizing the result. Click the green Play arrow to continue.\n",
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "Error making env  Cannot re-register id: Bayesian0NoGridEnv-v0\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-07 15:08:19,641\tWARNING ppo.py:168 -- Using the simple minibatch optimizer. This will significantly reduce performance, consider simple_optimizer=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.1)--(1.1) (1.1)--(1.2) 1 1\n",
      "Error making env  Cannot re-register id: Bayesian0NoGridEnv-v0\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-07 15:08:21,375\tWARNING util.py:45 -- Install gputil for GPU system monitoring.\n",
      "2020-05-07 15:08:21,402\tINFO trainable.py:346 -- Restored from checkpoint: ./PPO_bay0_horizon_50/checkpoint_250/checkpoint-250\n",
      "2020-05-07 15:08:21,403\tINFO trainable.py:353 -- Current state after restoring: {'_iteration': 250, '_timesteps_total': 2802922, '_time_total': 9881.619344949722, '_episodes_total': 5938}\n"
     ]
    }
   ],
   "source": [
    "parser = create_parser()\n",
    "args = parser.parse_args([\"./PPO_bay0_horizon_50/\", \"250\"])\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=1)\n",
    "run_transfer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(\"policy_0.pkl\", 'wb') as f:\n",
    "    dill.dump(run_transfer, f)\n",
    "    dill.dump(args, f)\n",
    "    dill.dump(bayesian_1_flow_params, f)\n",
    "    dill.dump(create_env, f)\n",
    "    dill.dump(create_agent, f)\n",
    "    dill.dump(run_env, f)\n",
    "    dill.dump(pr_ped_given_action, f)\n",
    "    dill.dump(accel_pdf, f)\n",
    "    dill.dump(plot_2_lines, f)\n",
    "    with open('./requirements.txt', 'r') as req:\n",
    "        x = req.readlines()\n",
    "        dill.dump(x, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
